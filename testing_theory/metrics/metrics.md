# **Метрики тестирования**

## **Junior Level*

Метрики тестирования — это количественные показатели, которые помогают измерить и оценить различные аспекты процесса
тестирования и качества продукта. Они отвечают на вопросы: "Насколько хорошо мы тестируем?", "Каково качество нашего
кода?", "Эффективны ли наши тесты?".

Основные метрики:

- **Покрытие кода (Code Coverage):** Какой процент кода выполняется во время тестов. Измеряется в процентах по строкам,
  ветвям, функциям.
- **Количество дефектов:** Сколько багов найдено, сколько исправлено, скорость их закрытия.
- **Время выполнения тестов:** Как долго работает тестовый набор.
- **Стабильность тестов (Flakiness):** Как часто тесты падают не из-за багов в коде, а по случайным причинам (например,
  проблемы с сетью).
- **Стоимость дефекта:** Сколько стоит найти и исправить баг на разных этапах (чем раньше, тем дешевле).

Метрики помогают принимать обоснованные решения: куда направить усилия по тестированию, когда можно выпускать релиз,
какие тесты нужно улучшить.

## **Middle Level**

С технической точки зрения метрики в Python-экосистеме тестирования собираются и анализируются с помощью конкретных
инструментов и практик.

1. **Метрики покрытия кода:**
    - **Инструменты:** `coverage.py` — стандартный инструмент для измерения покрытия. Интегрируется с pytest через
      `pytest-cov`.
    - **Типы покрытия:**
        - **Line coverage:** Процент выполненных строк.
        - **Branch coverage:** Процент пройденных ветвей в условиях (if/else).
        - **Function coverage:** Процент вызванных функций.
        - **Condition coverage:** Процент комбинаций условий в сложных булевых выражениях.
    - **Интеграция в CI/CD:** Генерация отчетов в формате XML/HTML, интеграция с сервисами (Codecov, Coveralls).

2. **Метрики качества тестов:**
    - **Mutation score (Мутационное тестирование):** `mutmut` внедряет мелкие изменения (мутации) в код и проверяет,
      обнаружат ли их тесты. Процент убитых мутаций — показатель эффективности тестов.
    - **Стабильность тестов (Flakiness):** Анализ истории запусков тестов. Если тест иногда проходит, иногда падает при
      тех же условиях — он нестабилен. Инструменты: `pytest-flakefinder`, кастомные скрипты анализа Jenkins/Allure
      отчетов.
    - **Время выполнения:** `pytest` с флагом `--durations` показывает самые медленные тесты. `pytest-xdist` для
      параллельного запуска, но нужно учитывать накладные расходы.

3. **Метрики дефектов:**
    - **Плотность дефектов (Defect Density):** Количество багов на тысячу строк кода (KLOC).
    - **Эффективность тестирования (Test Effectiveness):** Процент дефектов, найденных тестами, от общего числа
      дефектов (включая найденные пользователями).
    - **Время жизни дефекта (Defect Age):** Среднее время от создания бага до его закрытия.

4. **Метрики процесса:**
    - **Скорость выполнения тестов:** Сколько тестов выполняется в минуту/час.
    - **Автоматизация:** Процент автоматизированных тестов от общего числа.
    - **Стоимость:** Затраты на инфраструктуру тестирования (вычислительные ресурсы, лицензии инструментов).

5. **Инструменты для сбора метрик:**
    - **Allure TestOps / ReportPortal:** Системы для хранения результатов тестов, анализа метрик.
    - **Prometheus + Grafana:** Для мониторинга производительности тестовой инфраструктуры и самого приложения во время
      тестов.
    - **Кастомные скрипты на Python:** Анализ логов, парсинг отчетов, вычисление метрик.

## **Senior Level**

Метрики — это приборная панель Senior QA. Без них вы «летите вслепую». Но важно отличать «метрики тщеславия» (Vanity
Metrics), которые выглядят красиво, от «метрик действий» (Actionable Metrics), которые реально влияют на качество.

Инженерия качества (Quality Engineering) опирается на закон Гудхарта: «Когда мера становится целью, она перестает быть
хорошей мерой». Научные исследования сосредоточены на поиске корреляций между метриками и реальной надежностью ПО.

### Связь покрытия кода и плотности дефектов

Одно из самых важных исследований (Malaiya et al., 2002) установило **логарифмическую связь** между покрытием тестами (
Test Coverage) и плотностью дефектов (Defect Density).

* **Результат:** 100% покрытие не гарантирует 0 багов. Однако, покрытие ниже определенного порога (обычно 70-80% для
  ветвей/branches) экспоненциально увеличивает вероятность отказа в продакшене.
* **Нюанс:** Yamashita (2016) показала, что метрики сложности кода (Cyclomatic Complexity) коррелируют с вероятностью
  багов, но имеют форму «перевернутой U» для плотности дефектов: самые сложные файлы часто имеют *меньше* багов на
  строку кода, так как их пишут и тестируют тщательнее.

***

## 2. Ключевые метрики (Senior Level)

### 1. DRE (Defect Removal Efficiency) — "Король метрик"

Это главная метрика эффективности QA-команды. Она показывает, какой процент багов вы нашли *до* релиза.

$$DRE = \frac{Bugs_{QA}}{Bugs_{QA} + Bugs_{Prod}} \times 100\%$$

* **Эталон:** Мировой стандарт для хорошего процесса — **>85%**. Отличный процесс (High Maturity) — **>95%**.
* **Пример:** QA нашли 90 багов. После релиза пользователи нашли еще 10.
  $DRE = 90 / (90 + 10) = 90\%$. Отличный результат.
* **Действие:** Если DRE падает ниже 85%, значит, ваши тесты (или тестовое окружение) не соответствуют реальности.

### 2. Code Coverage (Покрытие кода)

* **Line Coverage:** Бесполезная метрика для Senior. Можно пройти по строке, но не проверить логику.
* **Branch Coverage (Покрытие ветвлений):** Настоящий стандарт. Проверяет `True` и `False` для каждого `if`.
* **Mutation Score:** Самая "честная" метрика. Специальный тул (например, `mutmut` для Python) ломает ваш код. Если
  тесты не упали — покрытие "липовое".

### 3. Defect Density (Плотность дефектов)

Количество багов на 1000 строк кода (KLOC) или на модуль.

* **Применение:** Помогает найти "горячие точки". Если в модуле "Корзина" 15 багов на KLOC, а в "Профиле" — 2, то
  регресс "Корзины" нужно усилить в 3 раза.

### 4. Mean Time To Detect (MTTD) & Mean Time To Repair (MTTR)

Метрики скорости CI/CD.

* **MTTD:** Сколько времени проходит от коммита "багованного" кода до падения теста? (Хорошо: < 15 мин).
* **MTTR:** Сколько времени проходит от обнаружения критического бага до фикса в продакшене?

***

## 3. Техническая реализация (Python)

Как собирать эти метрики автоматически?

### Mutation Testing (Python)

Вместо того чтобы верить отчету `coverage.py` на слово, используем мутационное тестирование.

```bash
# 1. Ставим библиотеку
pip install mutmut

# 2. Запускаем
mutmut run

# 3. Смотрим результаты
mutmut results
```

**Интерпретация:**

* **Killed:** Тест упал (Хорошо! Мы поймали мутанта).
* **Survived:** Тест прошел, хотя код был сломан (Плохо! Тест "дырявый").

**Пример "дырявого" теста:**

```python
def check_age(age):
    return "Adult" if age >= 18 else "Child"


# Плохой тест (Line Coverage 100%, но Mutation Score низкий)
def test_age():
    assert check_age(20) == "Adult"
    # Этот тест не заметит, если мы заменим `>=` на `>` (Boundary Bug)
    # Мутант (age > 18) выживет!
```

### Сбор DRE (Jira/Allure)

DRE нельзя посчитать в коде, это процессный показатель.

1. В Jira помечайте баги метками `found_in_qa` и `found_in_prod`.
2. Настройте JQL-фильтр или дашборд:
   `(labels = found_in_qa) / ((labels = found_in_qa) + (labels = found_in_prod))`

### Резюме для интервью

На вопрос "Какие метрики вы используете?", Senior QA не должен перечислять всё подряд.
**Правильный ответ:**
> "Я фокусируюсь на DRE, чтобы оценивать эффективность фильтрации багов. Для оценки качества автотестов я использую не
> просто Line Coverage, а Branch Coverage и иногда Mutation Score. А для бизнеса важны метрики стабильности релизов (
> Change Failure Rate)."

[Содержание](/CONTENTS.md#содержание)