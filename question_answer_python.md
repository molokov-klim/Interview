
---

# **SQLAlchemy**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Alembic**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Pydantic**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **FastAPI/Flask/Django**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Celery**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Websockets**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Paramiko**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Prometheus/Grafana**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Профилирование кода**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Статический анализ**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Анализ покрытия**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Бенчмаркинг**

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Пирамида тестирования**

## **Junior Level*

Пирамида тестирования — это концепция, которая визуализирует оптимальное соотношение различных типов автоматизированных
тестов в проекте. Она состоит из трех основных уровней:

1. **Unit-тесты (нижний уровень, основание пирамиды):** Тестируют отдельные компоненты системы (функции, классы) в
   полной изоляции. Их должно быть больше всего — они быстрые, дешевые в поддержке и дают мгновенную обратную связь.

2. **Интеграционные тесты (средний уровень):** Проверяют взаимодействие нескольких компонентов (модулей, сервисов, баз
   данных). Их меньше, чем unit-тестов — они медленнее, сложнее в поддержке, но проверяют критически важные
   взаимодействия.

3. **UI/E2E-тесты (верхний уровень, вершина пирамиды):** Тестируют систему с точки зрения конечного пользователя,
   проверяя полные сценарии работы. Их должно быть меньше всего — они самые медленные, хрупкие и дорогие в поддержке, но
   дают уверенность в работе системы в целом.

Цель пирамиды — создать сбалансированную стратегию тестирования: много быстрых и стабильных тестов внизу, меньше
медленных и комплексных наверху. Для QA инженера понимание этой концепции помогает планировать усилия по автоматизации,
распределять ресурсы и строить эффективный процесс тестирования.

## **Middle Level**

С технической точки зрения реализация каждого уровня пирамиды в Python-экосистеме имеет свои особенности:

1. **Unit-тестирование:**
    - **Инструменты:** `pytest`, `unittest`, `nose2`. Pytest стал де-факто стандартом благодаря гибкости и богатой
      экосистеме.
    - **Изоляция:** Использование моков (`unittest.mock`) для замены зависимостей. Ключевые техники: патчинг (`patch`),
      подмены (`MagicMock`, `AsyncMock`).
    - **Покрытие кода:** Инструменты `coverage.py` и `pytest-cov` для измерения покрытия.
    - **Параметризация:** Декоратор `@pytest.mark.parametrize` для запуска одного теста с разными входными данными.
    - **Важно:** Хороший unit-тест не зависит от внешних систем (БД, файловая система, сеть).

2. **Интеграционное тестирование:**
    - **Тестирование API:** Библиотеки `requests` + `pytest` для HTTP-API. Для асинхронных API — `aiohttp` или `httpx`.
    - **Тестирование БД:** Использование тестовых баз данных (например, SQLite in-memory) или механизмов транзакций с
      откатом после каждого теста. Инструменты: `pytest-django`, `factory_boy` для генерации данных.
    - **Тестирование микросервисов:** Использование тестовых дублей (test doubles) — заглушек (stubs) и моков для
      зависимых сервисов. Контейнеризация зависимостей (Docker) для запуска реальных сервисов в тестовом окружении.
    - **Фикстуры с областью видимости:** В pytest использование `@pytest.fixture(scope="module")` или
      `@pytest.fixture(scope="session")` для создания дорогих ресурсов (например, соединение с БД), которые
      переиспользуются между тестами.

3. **UI/E2E-тестирование:**
    - **Инструменты:** `Selenium WebDriver`, `Playwright`, `Cypress` (через `pytest-playwright`).
    - **Page Object Pattern:** Организация тестового кода через абстракции страниц/компонентов для уменьшения хрупкости
      и повышения переиспользуемости.
    - **Управление состоянием:** Создание и очистка тестовых данных перед/после тестов. Использование API для
      предварительной настройки состояния системы.
    - **Параллельный запуск:** Инструменты `pytest-xdist` для параллельного выполнения тестов. Для UI-тестов важно
      изолировать сессии браузера.

4. **Для AQA:**
    - **Баланс уровней:** Практическое правило: 70% unit-тестов, 20% интеграционных, 10% E2E. Но пропорции зависят от
      проекта.
    - **CI/CD интеграция:** Размещение разных уровней тестов в разных стадиях пайплайна: unit-тесты запускаются на
      каждом коммите, интеграционные — на пулл-реквестах, E2E — на релизных кандидатах.
    - **Флаки-тесты:** UI-тесты часто нестабильны. Необходимы стратегии борьбы: retry механизмы, стабилизация ожиданий (
      explicit waits), изоляция окружения.

## **Senior Level**

Глубокий анализ пирамиды тестирования как архитектурного паттерна, его эволюции, ограничений и интеграции с современными
практиками разработки.

1. **Эволюция и критика классической пирамиды:**
    - **"Песочные часы" или "Ромб":** Современные подходы предлагают увеличивать средний уровень (
      интеграционные/сервисные тесты) для микросервисных архитектур. Вместо пирамиды — песочные часы: много unit-тестов,
      много E2E, но акцент на контрактных тестах между сервисами.
    - **Пирамида Майка Кона:** Дополнение пирамиды ручным тестированием (исследовательское, usability) и тестами
      производительности/безопасности.
    - **Критика:** В микросервисной архитектуре unit-тесты часто дают ложное чувство безопасности, так как не проверяют
      взаимодействие сервисов. Акцент смещается на контрактное тестирование (Pact) и тестирование потребителя (
      consumer-driven contracts).

2. **Архитектурные аспекты реализации каждого уровня:**
    - **Unit-тесты и чистая архитектура:** Unit-тесты должны тестировать бизнес-логику в изоляции от инфраструктуры.
      Достигается через Dependency Injection и следование принципам SOLID. Использование `Protocol` для абстракций
      позволяет создавать моки без наследования.
    - **Интеграционные тесты и транзакции:** Для тестов БД важно использовать механизмы отката транзакций. В Django —
      `@pytest.mark.django_db(transaction=True)`. В SQLAlchemy — `session.begin_nested()` для nested transactions. Для
      NoSQL БД — создание отдельной тестовой базы на каждый тестовый прогон.
    - **E2E тесты и идемпотентность:** Каждый E2E тест должен быть идемпотентным — его повторный запуск не должен
      зависеть от предыдущих запусков. Достигается через:
        - Глобальную уникальность тестовых данных (UUID, временные метки).
        - Паттерн Test Data Builder.
        - Автоматическую очистку через хуки (например, `pytest.fixture` с `autouse=True` и `yield`).

3. **Пирамида и CI/CD:**
    - **Стратификация выполнения:** Разделение тестов на "быстрые" и "медленные". Быстрые тесты запускаются на каждом
      коммите, медленные — по расписанию или по мере необходимости. В GitLab CI/CD — `rules: changes`, в GitHub
      Actions — `paths`.
    - **Канареечный деплоймент и тестирование:** E2E-тесты выполняются на канареечном окружении перед выкатом в прод.
      Использование feature flags для управления доступностью функциональности.
    - **Тестирование в продакшене:** Практики progressive delivery: A/B тестирование, мониторинг ошибок, трассировка
      запросов. Тесты в проде — это следующий уровень после пирамиды.

- [Содержание](CONTENTS.md#содержание)

---

# **Виды тестирования**

## **Junior Level*

Виды тестирования — это различные подходы и методы проверки программного обеспечения, каждый из которых решает
конкретные задачи и имеет свою область применения. Основные виды:

1. **Функциональное тестирование** — проверяет, что система работает в соответствии с требованиями (что она делает).
2. **Нефункциональное тестирование** — проверяет, как система работает (производительность, безопасность, надежность).
3. **Модульное тестирование (Unit)** — тестирование отдельных компонентов кода (функций, классов) в изоляции.
4. **Интеграционное тестирование** — проверка взаимодействия между компонентами, модулями или системами.
5. **Системное тестирование (End-to-End)** — тестирование полного рабочего потока приложения от начала до конца.
6. **Регрессионное тестирование** — проверка, что новые изменения не сломали существующую функциональность.
7. **Дымовое тестирование (Smoke)** — быстрая проверка основных функций системы после сборки.
8. **Приемочное тестирование (Acceptance)** — проверка соответствия системы бизнес-требованиям.

Для QA инженера понимание этих видов помогает выбирать правильные подходы для разных ситуаций: что тестировать
автоматически, а что вручную, как распределять ресурсы и строить стратегию тестирования.

## **Middle Level**

С технической точки зрения каждый вид тестирования в Python-экосистеме реализуется через конкретные инструменты и
практики:

1. **Функциональное тестирование:**
    - **API-тестирование:** Использование `requests`, `httpx`, `aiohttp` для HTTP-запросов. Фреймворки: `pytest` с
      плагинами `pytest-httpx`, `pytest-asyncio`.
    - **UI-тестирование:** `Selenium WebDriver`, `Playwright`, `Cypress` через Python-биндинги. Паттерн Page Object для
      структурирования кода.
    - **Тестирование бизнес-логики:** Модульные и интеграционные тесты с использованием моков (`unittest.mock`) и
      стабов.

2. **Нефункциональное тестирование:**
    - **Нагрузочное тестирование:** `locust` (кодовая нагрузка), `k6` (через subprocess), `JMeter` (через
      `jmeter-python`).
    - **Тестирование безопасности:** Статические анализаторы (`bandit`, `safety`), динамические (`OWASP ZAP` API),
      проверка зависимостей (`dependabot`, `renovate`).
    - **Тестирование доступности (a11y):** `axe-core` через `selenium` или `playwright`.

3. **Модульное тестирование (Unit):**
    - **Изоляция:** Использование `unittest.mock.patch`, `MagicMock`, `AsyncMock` для подмены зависимостей.
    - **Параметризация:** `@pytest.mark.parametrize` для тестирования с разными входными данными.
    - **Property-based тестирование:** `hypothesis` для генерации тестовых данных и проверки инвариантов.

4. **Интеграционное тестирование:**
    - **Тестирование с БД:** Использование тестовых БД (SQLite in-memory), транзакций с откатом, фикстур для данных.
    - **Тестирование микросервисов:** `docker-compose` для поднятия зависимостей, `testcontainers` для управления
      контейнерами из кода.
    - **Контрактное тестирование:** `pact-python` для проверки совместимости между потребителем и поставщиком API.

5. **Регрессионное тестирование:**
    - **Тест-сьюты:** Организация тестов по тегам (`@pytest.mark.regression`) для выборочного запуска.
    - **Анализ покрытия:** `pytest-cov` для отслеживания покрытия измененного кода.

6. **Приемочное тестирование:**
    - **BDD-подход:** `behave`, `pytest-bdd` для тестирования на основе пользовательских сценариев (Gherkin).
    - **Автоматизация сценариев:** Комбинация API и UI-тестов для проверки полных пользовательских сценариев.

7. **Тестирование в CI/CD:**
    - **Стратификация тестов:** Разделение на быстрые (unit) и медленные (UI, нагрузочные) с разными триггерами запуска.
    - **Параллельный запуск:** `pytest-xdist` для ускорения выполнения.

## **Senior Level**

# **Единая классификация видов тестирования**

Вместо хаотичного списка лучше представлять тестирование как многомерный куб. Один и тот же тест может быть одновременно
**системным**, **функциональным**, **автоматизированным** и **регрессионным**.

Ниже — структурированное разделение по ключевым измерениям (Dimensions).

***

## **1. По объекту тестирования (Что проверяем?)**

Это самое главное деление: проверяем мы бизнес-функции или качество реализации.

### **1.1 Функциональное тестирование (Functional)**

Отвечает на вопрос: **«Что система делает?»**.
Мы проверяем, решает ли программа задачи пользователя.

* **Функциональное (Functional):** Проверка бизнес-сценариев. Работает ли логин? Считается ли скидка в корзине?
* **Взаимодействия (Interoperability):** Может ли наша система общаться с другими (например, корректно ли мы шлем данные
  в 1С или платежный шлюз).

**Внутри функционального выделяют подходы:**

* **Позитивное:** «Счастливый путь» (Happy Path). Вводим корректные данные, ожидаем успех.
* **Негативное:** Вводим мусор, спецсимволы, null. Проверяем, что система не падает, а вежливо сообщает об ошибке.

### **1.2 Нефункциональное тестирование (Non-functional)**

Отвечает на вопрос: **«Как система работает?»**.
Функция может работать, но если страница грузится 30 секунд — это баг.

* **Производительности (Performance):** Общее понятие скорости и ресурсов.
    * *Нагрузочное (Load):* Как ведет себя система при **штатной** ожидаемой нагрузке.
    * *Стрессовое (Stress):* Найти точку отказа. Даем нагрузку выше максимума, пока сервер не упадет (или не
      восстановится).
    * *Стабильности/Надежности (Stability/Soak/Endurance):* Тест на выносливость. Работаем под средней нагрузкой долго (
      24+ часа), ищем утечки памяти.
    * *Объемное (Volume):* Как система работает с огромной базой данных (миллионы записей).
    * *Масштабируемости (Scalability):* Если добавить железа (CPU/RAM), вырастет ли производительность пропорционально?
* **Безопасности (Security):** Проверка на уязвимости (SQL-инъекции, XSS), разграничение прав доступа (может ли юзер
  видеть админку) и конфиденциальность.
* **Удобства использования (Usability/UX):** Насколько удобно и понятно пользователю. Это про интуитивность интерфейса,
  а не только про красоту.
* **Доступности (Accessibility/a11y):** Могут ли сайтом пользоваться люди с ограничениями (скринридеры, цветовая
  слепота).
* **Совместимости (Compatibility):**
    * *Кроссбраузерное:* Chrome, Firefox, Safari.
    * *Кроссплатформенное:* iOS vs Android, Windows vs Linux.
* **Локализации (Localization/L10n):** Проверка перевода, форматов дат, валют и направления текста (RTL).
* **Установки и конфигурирования (Installation & Configuration):** Как софт ставится, обновляется и удаляется.

***

## **2. По уровню детализации (Пирамида тестирования)**

На каком уровне архитектуры мы находимся.

1. **Модульное (Unit):** Самый низкий уровень. Проверяем отдельную функцию или класс в изоляции. Делают разработчики.
   Быстро, дешево.
2. **Интеграционное (Integration):** Проверка стыков. Как два модуля (или сервис + база данных) общаются друг с другом.
3. **Системное (System / E2E):** Проверка системы целиком, как черный ящик. Максимально близко к действиям реального
   пользователя.
4. **Приемочное (Acceptance):** Финальный этап. Заказчик (или PM) смотрит и говорит: «Да, это то, что я заказывал».

***

## **3. По знанию системы (Доступ к коду)**

* **Черный ящик (Black Box):** Мы не видим код. Знаем только вход (требования) и выход. Мы — как пользователь.
* **Белый ящик (White Box):** Мы видим код, знаем структуру БД, алгоритмы. Пишем тесты, чтобы покрыть конкретные ветки
  кода (Statement/Branch coverage).
* **Серый ящик (Grey Box):** Мы работаем как пользователь (через UI/API), но можем заглянуть в БД или логи, чтобы
  проверить, правильно ли записались данные.

***

## **4. По хронологии и изменениям (Когда запускаем?)**

* **Дымовое (Smoke):** «Включается ли вообще?». Быстрая проверка критического функционала после сборки. Если дым идет —
  дальше не тестируем.
* **Санитарное (Sanity):** Проверка **конкретной** области после исправлений. Убеждаемся, что *именно этот* баг починили
  и смежные функции работают. Узконаправленно.
* **Регрессионное (Regression):** Проверка **всей** старой функциональности после внесения изменений. Убеждаемся, что
  новый код не сломал старый.
* **Подтверждающее (Re-testing):** Просто перепроверка баг-репорта. Был баг -> разраб исправил -> мы проверили (
  Re-test).

***

## **5. По степени автоматизации**

* **Ручное (Manual):** Человек кликает мышкой. Незаменимо для UX и исследовательского тестирования.
* **Автоматизированное (Automated):** Скрипты выполняют проверки. Идеально для регресса и нагрузки.
* **Полуавтоматизированное:** Человек запускает скрипты, которые генерируют данные, но решение «Правильно/Неправильно»
  принимает сам.

***

## **6. По степени формализации**

* **Сценарное (Scripted):** Строго по тест-кейсам. Шаг влево, шаг вправо — расстрел.
* **Исследовательское (Exploratory):** Тестировщик одновременно изучает систему, придумывает тесты и выполняет их.
  Требует опыта и интуиции.
* **Ad-hoc (Интуитивное):** «Метод тыка». Бессистемное тестирование без подготовки. Иногда помогает найти самые странные
  баги.

***

# **Техническая реализация (Python Context)**

Как Senior QA Automation, вы должны знать не только *названия* видов, но и *инструменты* для них.

### **1. Функциональное тестирование**

* **API:** Основной рабочий инструмент.
    * *Libs:* `requests` (синхронно), `aiohttp`/`httpx` (асинхронно).
    * *Framework:* `pytest` — стандарт индустрии.
    * *Schema validation:* `Pydantic` или `jsonschema` (валидировать контракты ответов).
* **UI (E2E):**
    * *Tools:* `Selenium WebDriver` (классика), `Playwright` (современный, быстрый, стабильный).
    * *Pattern:* Page Object Model (POM) — обязательно для разделения локаторов и логики теста.
* **Mobile (Android/iOS):**
    * *Tool:* `Appium` (клиент на Python). Знание `ADB` и `uiautomator2`.

### **2. Нефункциональное тестирование**

* **Load (Нагрузка):**
    * `Locust`: Пишется на чистом Python. Отлично подходит для проверки API под нагрузкой.
    * `K6`: (JS/Go), но можно запускать и анализировать через Python-обвязки.
* **Security (Безопасность):**
    * Статический анализ зависимостей: `safety` (проверка requirements.txt на дыры).
    * Сканнеры: `OWASP ZAP` (можно управлять через API).

### **3. Unit & Integration (Белый ящик)**

* **Mocking:** `unittest.mock` (Mock, MagicMock, patch). Умение изолировать тест от внешнего API или БД.
* **Database:** Использование фикстур (`pytest fixtures`) для подготовки и очистки тестовых данных в БД (SQLAlchemy/Raw
  SQL).
* **Coverage:** `pytest-cov` — посмотреть, какой процент кода задет тестами.

### **4. CI/CD & Infrastructure**

* **Docker:** Запуск тестов в изолированных контейнерах (`testcontainers-python`).
* **Allure:** Генерация красивых отчетов, понятных менеджменту.
* **GitHub Actions/GitLab CI:** Настройка пайплайнов (запуск смоуков на PR, регресса на релиз).

- [Содержание](CONTENTS.md#содержание)

---

# **Техники тест-дизайна**

## **Junior Level**

Техники проектирования тестов (Test Design Techniques) — это структурированные методы создания тестовых случаев, которые
помогают эффективно и полно проверить систему. Они отвечают на вопрос: "Как придумать хорошие тесты?" и заменяют
случайный перебор данных системным подходом.

Основные техники:

1. **Эквивалентное разделение (Equivalence Partitioning):** Входные данные делятся на группы (классы эквивалентности),
   где поведение системы ожидается одинаковым. Например, для поля "возраст" группы: отрицательные числа (невалидные),
   0-17 (несовершеннолетние), 18-65 (взрослые), больше 65 (пенсионеры). Достаточно протестировать по одному значению из
   каждого класса.
2. **Анализ граничных значений (Boundary Value Analysis):** Фокусируется на тестировании значений на границах этих
   классов, так как именно там чаще всего возникают ошибки. Для диапазона 18-65 проверяются значения: 17, 18, 19 и 64,
   65, 66.
3. **Таблица принятия решений (Decision Table Testing):** Применяется для логики, зависящей от комбинаций условий.
   Создается таблица, где столбцы — это условия и действия, а строки — тестовые сценарии для всех значимых комбинаций.
4. **Тестирование состояний и переходов (State Transition Testing):** Используется для систем с конечным числом
   состояний (например, банкомат: "Ожидание карты" -> "Ввод PIN" -> "Выбор операции"). Тестируются как корректные
   переходы, так и ошибочные (например, ввод неверного PIN-кода).
5. **Тестирование сценариев использования (Use Case Testing):** Система проверяется через призму реальных
   пользовательских сценариев, описывающих, как пользователь взаимодействует с системой для достижения конкретной цели (
   например, "Оформление заказа").
6. **Попарное тестирование (Pairwise Testing):** Метод оптимизации, который позволяет покрыть все возможные пары
   значений входных параметров, сокращая количество тестовых комбинаций до приемлемого уровня.
7. **Предугадывание ошибок (Error Guessing):** Опытный тестировщик на основе знаний о системе, предыдущих дефектах и
   типичных проблемах в подобных продуктах выдвигает гипотезы о возможных ошибках и создает целевые тесты для их
   проверки.

## **Middle Level**

На среднем уровне важно не только знать техники, но и понимать, как эффективно применять их в рамках автоматизации,
управлять тестовыми данными и интегрировать подходы в процесс разработки.

### Особенности применения и автоматизации

1. **Эквивалентное разделение и анализ граничных значений:**
    * **Параметризация тестов:** В pytest с помощью декоратора `@pytest.mark.parametrize` один тест превращается в набор
      проверок для данных из разных классов эквивалентности и граничных значений.
    * **Генерация данных:** Значения можно генерировать динамически или выносить в отдельные фикстуры для повторного
      использования.
    * **Ключевой момент:** Автоматизация позволяет легко и полно покрыть все границы и классы, что сложно сделать
      вручную.

2. **Таблица принятия решений:**
    * **Data-Driven Testing (DDT):** Саму таблицу (например, в формате CSV, JSON или Excel) можно использовать как
      источник данных для тестов. Это делает логику прозрачной и легко обновляемой.
    * **Интеграция:** Инструменты вроде `pandas` упрощают загрузку и обработку табличных данных в тестах.
    * **Преимущество:** Четкое разделение тестовой логики (код) и тестовых данных (таблица).

3. **Тестирование состояний и переходов:**
    * **Паттерн "Конечный автомат" (State Machine):** Логику системы можно смоделировать в коде, что облегчает создание
      тестов, проверяющих корректность переходов.
    * **Последовательности шагов:** Тесты организуются как цепочки действий (например, через фикстуры в `pytest`),
      которые переводят систему из одного состояния в другое с последующей проверкой.
    * **Фокус:** Автоматизация помогает проверить длинные и сложные цепочки переходов, включая обработку нестандартных
      сценариев.

4. **Тестирование сценариев использования:**
    * **BDD-фреймворки:** Инструменты вроде `behave` или `pytest-bdd` позволяют описывать сценарии на языке Gherkin (
      Given-When-Then), что улучшает взаимодействие между разработчиками, тестировщиками и аналитиками.
    * **Паттерн Page Object:** Для UI-автоматизации этот паттерн идеально ложится на сценарии, инкапсулируя логику
      работы с элементами страницы и делая тесты устойчивее к изменениям в верстке.

5. **Попарное тестирование (Pairwise):**
    * **Автоматизация генерации:** Инструменты (`allpairspy`, `pict`) интегрируются в процесс подготовки тестовых
      данных, генерируя минимальный набор комбинаций для покрытия всех пар.
    * **Применение:** Особенно полезно при тестировании конфигураций (ОС x браузер x разрешение экрана) или
      функциональности с множеством независимых параметров.

6. **Предугадывание ошибок:**
    * **Систематизация:** Хотя техника основана на опыте, найденные дефекты и гипотезы можно фиксировать в виде
      автоматизированных проверок и добавлять их в регрессионную тестовую базу.
    * **Анализ рисков:** Метод тесно связан с анализом областей повышенного риска в приложении, что помогает расставлять
      приоритеты при написании автоматизированных тестов.

## **Senior Level**

# Техника: Эквивалентное разделение и Анализ граничных значений

Это не просто «правила хорошего тона», а математически обоснованные методы сокращения бесконечного числа тестов до
конечного набора. В основе лежат теория множеств и эмпирические исследования распределения ошибок в коде.

***

## 1. Научное обоснование (The Science Behind It)

С точки зрения Computer Science, программа — это математическая функция $f(x)$, которая отображает входные данные на
выходные. Тестирование — это попытка найти такие $x$, где $f(x)$ работает некорректно.

### Гипотеза однородности (Basis for Equivalence Partitioning)

Фундамент метода эквивалентных классов — **гипотеза однородности (Homogeneity Hypothesis)**. Она утверждает, что если
один тест из класса эквивалентности выявляет ошибку, то с высокой вероятностью её выявят и все остальные тесты этого
класса. И наоборот: если один тест проходит успешно, остальные тоже пройдут.

> **Суть:** Мы предполагаем, что программа обрабатывает все числа от 1 до 100 *одним и тем же куском кода* (одним путем
> в графе потока управления — Control Flow Graph).

### Гипотеза сгущения ошибок (Basis for BVA)

Анализ граничных значений опирается на **гипотезу граничных значений**. Эмпирически доказано, что вероятность
отказа $P(failure)$ не распределена равномерно. Она имеет резкие пики (спайки) в точках, где меняется логика программы.

**Почему это происходит? Причины кроются в психологии программирования:**

1. **Ошибки на единицу (Off-by-one errors):** Разработчики путают `>` и `>=`, `<` и `<=`.
2. **Инициализация циклов:** Ошибки в `for (i=0; i < N; i++)` часто приводят к пропуску последнего элемента или выходу
   за массив.
3. **Переполнение типов:** Границы часто совпадают с предельными значениями типов данных (`int`, `short`).

## 2. Результаты ключевых исследований

Научные работы последовательно подтверждают эффективность этих методов по сравнению со случайным тестированием (Random
Testing).

### 1. Reid (1997): «Empirical Analysis of EP, BVA and Random Testing»

Стюарт Рейд провел фундаментальное исследование на реальной системе авионики (20 000 строк кода Ada).

* **Результат:** BVA (граничные значения) оказалось **самым эффективным** методом, выявляя ошибки, которые пропускали
  другие методы.
* **Сравнение:** BVA находил почти в 2 раза больше дефектов, чем EP (классы эквивалентности), но требовал кратно больше
  тест-кейсов.
* **Вывод:** EP — дешевле (меньше тестов), но пропускает граничные баги. BVA — дороже, но надежнее.

### 2. Basili & Selby (1987): «Comparing the Effectiveness of Software Testing Strategies»

Классическое исследование, сравнивавшее функциональное тестирование (EP+BVA) со структурным (покрытие кода) и
рецензированием кода (Code Reading).

* **Результат:** Функциональное тестирование (Black-box) показало отличные результаты в обнаружении ошибок инициализации
  и управления, часто превосходя структурные тесты.

### 3. Современные данные (Dobslaw 2023, Hubner 2019)

В эпоху AI и автоматизации исследования показывают, что стратегии генерации тестов, которые "целятся" в границы (
Boundary-guided testing), находят на 30-50% больше мутационных ошибок, чем слепой фаззинг (fuzzing).

# Техника: Таблица принятия решений (Decision Table Testing)

В отличие от граничных значений, которые исследуют *диапазоны*, таблицы решений исследуют *логику* и *комбинаторику*.
Это метод для борьбы с комбинаторным взрывом и цикломатической сложностью.

***

## 1. Научное обоснование

Фундаментом для таблиц решений служат **Булева алгебра** и **Пропозициональная логика**. Любая программа, принимающая
решения, может быть представлена как функция от набора бинарных (или конечных) переменных.

### Проблема, которую решает метод

Человеческий мозг плохо удерживает в оперативной памяти более 3-4 условий одновременно (следствие закона
Миллера $7 \pm 2$). Когда в коде встречаются вложенные `if-else` или зависимые условия, вероятность ошибки (пропущенной
ветки) стремится к 100%.

### Математические свойства (Completeness & Consistency)

Таблицы решений опираются на два строгих математических свойства:

1. **Полнота (Completeness):** Гарантия того, что рассмотрены *все возможные* комбинации входных условий. Для $N$
   бинарных условий существует ровно $2^N$ возможных комбинаций. Если таблица содержит меньше правил, она либо неполна,
   либо сжата (collapsed).
2. **Непротиворечивость (Consistency):** Гарантия того, что одна и та же комбинация условий не ведет к взаимоисключающим
   действиям.

***

## 2. Результаты исследований

Научные работы подтверждают, что таблицы решений (DT) превосходят другие методы в выявлении логических ошибок, но могут
быть избыточны.

### 1. Subramanian (1992): «A comparison of the decision table and tree»

Исследование эффективности представления логики.

* **Результат:** Тестировщики и аналитики, использующие табличное представление (Decision Tables), совершали
  статистически значимо **меньше ошибок** при анализе сложной логики по сравнению с теми, кто использовал деревья
  решений (Decision Trees) или текстовые спецификации.
* **Вывод:** Табличная структура снижает когнитивную нагрузку и позволяет быстрее замечать пропущенные сценарии (gaps).

### 2. Сравнение с Boundary Value Analysis (Ferriday, 2007)

* **Результат:** BVA генерирует примерно в 5 раз больше тест-кейсов, чем Decision Tables, но DT находит специфический
  класс ошибок — **ошибки взаимодействия условий** (interaction faults), которые BVA пропускает полностью.
* **Эффективность:** DT выявляет "логические дыры" (ситуации, когда спецификация молчит о поведении системы), в то время
  как другие методы проверяют только написанное.

### 3. Shiffman (1997): «Representation of Clinical Practice Guidelines»

Исследование на критических системах (медицина).

* **Результат:** Применение таблиц решений к медицинским алгоритмам позволило выявить логическую неполноту (missing
  rules) и противоречия в клинических рекомендациях, которые не были замечены экспертами-людьми при обычном чтении
  текста.

# Техника: Тестирование состояний и переходов (State Transition Testing)

Этот вид тестирования радикально отличается от предыдущих. Если *Эквивалентное разделение* работает с «моментальными»
данными (stateless), то *State Transition Testing* проверяет «память» системы.

Это метод для проверки сложной бизнес-логики, где ответ системы зависит не только от того, **что** вы нажали, но и от
того, **в каком состоянии** система находилась до этого.

***

## 1. Научное обоснование (Scientific Basis)

В основе метода лежит раздел дискретной математики — **Теория конечных автоматов (Automata Theory)**.

### Формальная модель

Любую систему с состояниями можно описать как кортеж $(S, I, \delta)$, где:

* $S$ — конечное множество состояний (States).
* $I$ — множество входных сигналов (Inputs/Events).
* $\delta$ — функция перехода: $S_{current} \times I \rightarrow S_{new}$.

Это означает, что **реакция системы является функцией от её истории**. В отличие от простых функций $y=f(x)$,
здесь $y=f(x, state)$.

### Почему это необходимо?

Классические методы (EP, BVA) бессильны против ошибок последовательности.

* *Пример:* Нажатие кнопки «Оплатить» с валидной картой (EP/BVA говорят "ОК") должно приводить к успеху *только* в
  состоянии «Заказ создан», но должно вызывать ошибку в состоянии «Заказ уже оплачен». Без учета состояния мы пропустим
  этот баг.

***

## 2. Результаты исследований

Эффективность метода подтверждена десятилетиями исследований в области надежности ПО, особенно в embedded-системах и
телекоме.

### 1. Фундаментальная работа T.S. Chow (1978)

Статья «Testing Software Design Modeled by Finite-State Machines» является библией этого метода.

* **Результат:** Чоу доказал, что тестирование на основе автоматов гарантированно обнаруживает определенные классы
  ошибок, которые невозможно найти другими способами:
    * **Operation errors:** Неверный выходной результат при переходе.
    * **Transfer errors:** Переход не в то состояние.
    * **Extra/Missing states:** Лишние или недостижимые состояния.
* **Вклад:** Он ввел понятие **N-switch coverage** (покрытие последовательностей длины N), показав, что проверки
  одиночных переходов (0-switch) недостаточно для выявления сложных багов.

### 2. Offutt et al. (2003) — State-based Specification Testing

Джефф Оффат исследовал генерацию тестов из UML-диаграмм состояний.

* **Результат:** Автоматическая генерация тестов на основе состояний (State-based) находит глубокие логические ошибки,
  которые пропускают люди при написании тестов вручную, так как люди склонны проверять только «счастливые пути» (Happy
  Path) переходов.

### 3. Сравнение эффективности (Holt, 2014)

Исследование на промышленном ПО:

* **Результат:** State-Based Testing (SBT) с использованием строгих оракулов (проверок) позволяет обнаруживать дефекты
  управления потоком эффективнее, чем покрытие кода (Code Coverage). Удаление деталей из модели состояния (упрощение)
  снижает стоимость тестирования на 85%, но снижает эффективность обнаружения багов всего на ~30%, что делает метод
  рентабельным даже в упрощенном виде.

# Техника: Тестирование сценариев использования (Use Case Testing)

Если предыдущие техники (EP, BVA, Decision Table) были атомарными проверками «вход-выход», то **Use Case Testing** — это
тестирование *потоков* и *целей* пользователя. Это переход от проверки «как работает код» к проверке «как работает
бизнес-процесс».

***

## 1. Научное обоснование

В основе метода лежит **ориентированный на пользователя подход (User-Centered Design)** и **акторно-сетевая теория**.

### Концептуальная модель

Система рассматривается не как набор функций, а как "черный ящик", с которым взаимодействуют внешние сущности — **Акторы
** (пользователи, другие системы, таймеры), чтобы достичь определенной **Цели**.

* **Целеполагание:** Научно доказано, что пользователи не используют ПО ради функций (нажать кнопку). Они используют его
  для решения задач (купить билет, отправить отчет). Тестирование Use Case валидирует именно достижение целей.
* **Эвристика Парето (80/20):** Исследования показывают, что 80% времени пользователи используют 20% функционала (
  основные сценарии). Use Case Testing гарантирует, что именно эти критические 20% работают безупречно.

***

## 2. Результаты исследований

Научные данные подтверждают, что этот метод критически важен для нахождения дефектов *интеграции* и *требований*,
которые пропускают unit-тесты.

### 1. Ivar Jacobson (1992): «Object-Oriented Software Engineering»

Ивар Якобсон, создатель термина "Use Case" (изначально *Användningsfall* на шведском), доказал в своих работах, что
построение разработки и тестирования вокруг сценариев использования ("Use Case Driven") снижает количество архитектурных
ошибок на ранних стадиях.

### 2. Sophocleous et al. (2020): «Examining the Current State of System Testing»

Исследование 252 QA-инженеров и промышленных кейсов показало:

* **Результат:** Тестирование на основе реальных пользовательских сценариев (в комбинации с smoke/regression)
  статистически значимо ($p = 0.000$) снижает количество дефектов, обнаруженных конечными пользователями после
  релиза.[3]
* **Вывод:** Чем ближе тесты к реальным сценариям использования, тем выше удовлетворенность заказчика (User Acceptance).

### 3. Gutierrez et al.: «A Case Study for Generating Test Cases from Use Cases»

Исследование автоматической генерации тестов:

* **Результат:** Метод анализа сценариев (Scenario Analysis) позволяет выявить пропущенные пути в требованиях (gaps),
  которые не очевидны при просмотре списка требований списком. Тесты, сгенерированные из Use Case, имеют более высокое
  покрытие *бизнес-логики* по сравнению с тестами, основанными на структуре кода.[4]

# Техника: Попарное тестирование (Pairwise Testing)

Это метод-скальпель: он отсекает 99% тестов, сохраняя 95% эффективности. Это не магия, а прикладная комбинаторика.

***

## 1. Научное обоснование

**Проблема:** Комбинаторный взрыв. Если у вас 10 параметров по 10 значений в каждом, вам нужно $10^{10}$ тестов (10
миллиардов). Это невозможно выполнить.

**Решение (Эмпирический закон):** Ошибки в ПО крайне редко вызываются сложным взаимодействием 3-х и более параметров
одновременно.

* **Single-mode faults:** 20-30% багов вызываются одним параметром (ловится BVA/EP).
* **Double-mode faults:** 50-70% багов возникают на стыке **ДВУХ** параметров (например, "Шрифт=Arial" + "Принтер=HP").
* **Multi-mode faults:** Баги, требующие 3+ условий, составляют менее 5-10% (для некритических систем).[1]

**Математическая база:** Метод основан на **Ортогональных массивах (Orthogonal Arrays)** и **Covering Arrays**.
Ортогональный массив $L_N(S^k)$ гарантирует, что для любых двух колонок (параметров) *каждая возможная пара значений*
встречается ровно один раз (или как минимум один раз для Covering Arrays).[2][3]

Это позволяет сократить $10^{10}$ тестов до $\approx 100-200$, сохранив покрытие всех парных взаимодействий.

***

## 2. Результаты исследований

### 1. NIST (Wallace & Kuhn, 2004) — Исследование "магического числа"

Национальный институт стандартов и технологий США (NIST) проанализировал базы багов NASA (космические аппараты),
медицинских устройств и браузеров.

* **Результат:**
    * **98%** всех дефектов в медицинском ПО выявляются тестированием **пар** (2-way testing).[4]
    * В сложных системах (NASA) для выявления 100% багов требовалось тестирование взаимодействия до 6 параметров (
      6-way).
    * Кривая насыщения: 2-way ловит ~80-90% багов, 3-way ~95%, 4-way ~99%.[1]
* **Вывод:** Pairwise (2-way) — это "золотой стандарт" по соотношению цена/качество. Для критических модулей стоит
  использовать 3-way или 4-way.

### 2. Charbachi (2017) — Сравнение с ручным тестированием

* **Результат:** Pairwise-тесты находили примерно столько же багов, сколько тесты, написанные опытными инженерами
  вручную, но обеспечивали **более высокое покрытие кода (Code Coverage)** за счет неочевидных комбинаций, о которых
  люди часто не задумываются.[5]

### 3. Wood (2016) — Применение в фармакологии

Любопытный факт: принцип работает не только в IT. В биологии 80% реакций на "коктейль" из лекарств также предсказываются
попарным взаимодействием компонентов, что подтверждает универсальность закона "малых взаимодействий".[6]

# Техника: Предугадывание ошибок (Error Guessing)

Этот метод часто недооценивают, называя «интуицией», но в инженерной психологии он имеет строгое обоснование. Это не
гадание, а **применение неявного знания (Implicit Knowledge)** и распознавание паттернов.

***

## 1. Научное обоснование

### Когнитивная психология: Модель RPD

С точки зрения когнитивистики (Gary Klein), эксперт использует **модель принятия решений по распознаванию (
Recognition-Primed Decision, RPD)**. Мозг опытного инженера хранит тысячи паттернов «ситуация -> ошибка» и при виде
знакомого кода подсознательно «подсвечивает» опасные места.

* **Гипотеза кластеризации дефектов (Pareto Principle):** Принцип Парето работает и здесь: 80% ошибок содержатся в 20%
  модулей. Error Guessing — это эвристический поиск этих кластеров.
* **Таксономия дефектов (Defect Taxonomy):** Метод опирается на классификацию типичных ошибок (например, Boris Beizer’s
  Taxonomy). Ошибки не уникальны; программисты совершают одни и те же ляпы десятилетиями (деление на ноль, race
  condition, null pointer).

***

## 2. Результаты исследований

Наука подтверждает: опыт бьет формализм в поиске специфических багов, но проигрывает в полноте покрытия.

### 1. Basili & Selby (1987) — Роль экспертизы

В том же исследовании, где сравнивали EP/BVA, было замечено:

* **Результат:** Тестировщики-эксперты, использовавшие «свободный поиск» (фактически Error Guessing), находили самые
  сложные логические ошибки, которые пропускали формальные методы.
* **Нюанс:** Эффективность метода линейно зависит от опыта. Junior QA с этим методом находит близкое к нулю количество
  критических багов.[1]

### 2. Исследования Exploratory Testing (Bhatti, 2010)

Error Guessing является частью исследовательского тестирования.

* **Результат:** Исследовательский подход (основанный на догадках) позволил найти статистически значимо **больше
  дефектов** за единицу времени, чем сценарное тестирование, особенно в категориях UI и Usability.[2]

### 3. MITRE — Seven Pernicious Kingdoms

Исследование CWE (Common Weakness Enumeration)  — это, по сути, глобальная база для Error Guessing в области
безопасности. Она доказывает, что 90% уязвимостей (SQLi, XSS, Buffer Overflow) предсказуемы.[3]

- [Содержание](CONTENTS.md#содержание)

---

# **Метрики тестирования**

## **Junior Level*

Метрики тестирования — это количественные показатели, которые помогают измерить и оценить различные аспекты процесса
тестирования и качества продукта. Они отвечают на вопросы: "Насколько хорошо мы тестируем?", "Каково качество нашего
кода?", "Эффективны ли наши тесты?".

Основные метрики:

- **Покрытие кода (Code Coverage):** Какой процент кода выполняется во время тестов. Измеряется в процентах по строкам,
  ветвям, функциям.
- **Количество дефектов:** Сколько багов найдено, сколько исправлено, скорость их закрытия.
- **Время выполнения тестов:** Как долго работает тестовый набор.
- **Стабильность тестов (Flakiness):** Как часто тесты падают не из-за багов в коде, а по случайным причинам (например,
  проблемы с сетью).
- **Стоимость дефекта:** Сколько стоит найти и исправить баг на разных этапах (чем раньше, тем дешевле).

Метрики помогают принимать обоснованные решения: куда направить усилия по тестированию, когда можно выпускать релиз,
какие тесты нужно улучшить.

## **Middle Level**

С технической точки зрения метрики в Python-экосистеме тестирования собираются и анализируются с помощью конкретных
инструментов и практик.

1. **Метрики покрытия кода:**
    - **Инструменты:** `coverage.py` — стандартный инструмент для измерения покрытия. Интегрируется с pytest через
      `pytest-cov`.
    - **Типы покрытия:**
        - **Line coverage:** Процент выполненных строк.
        - **Branch coverage:** Процент пройденных ветвей в условиях (if/else).
        - **Function coverage:** Процент вызванных функций.
        - **Condition coverage:** Процент комбинаций условий в сложных булевых выражениях.
    - **Интеграция в CI/CD:** Генерация отчетов в формате XML/HTML, интеграция с сервисами (Codecov, Coveralls).

2. **Метрики качества тестов:**
    - **Mutation score (Мутационное тестирование):** `mutmut` внедряет мелкие изменения (мутации) в код и проверяет,
      обнаружат ли их тесты. Процент убитых мутаций — показатель эффективности тестов.
    - **Стабильность тестов (Flakiness):** Анализ истории запусков тестов. Если тест иногда проходит, иногда падает при
      тех же условиях — он нестабилен. Инструменты: `pytest-flakefinder`, кастомные скрипты анализа Jenkins/Allure
      отчетов.
    - **Время выполнения:** `pytest` с флагом `--durations` показывает самые медленные тесты. `pytest-xdist` для
      параллельного запуска, но нужно учитывать накладные расходы.

3. **Метрики дефектов:**
    - **Плотность дефектов (Defect Density):** Количество багов на тысячу строк кода (KLOC).
    - **Эффективность тестирования (Test Effectiveness):** Процент дефектов, найденных тестами, от общего числа
      дефектов (включая найденные пользователями).
    - **Время жизни дефекта (Defect Age):** Среднее время от создания бага до его закрытия.

4. **Метрики процесса:**
    - **Скорость выполнения тестов:** Сколько тестов выполняется в минуту/час.
    - **Автоматизация:** Процент автоматизированных тестов от общего числа.
    - **Стоимость:** Затраты на инфраструктуру тестирования (вычислительные ресурсы, лицензии инструментов).

5. **Инструменты для сбора метрик:**
    - **Allure TestOps / ReportPortal:** Системы для хранения результатов тестов, анализа метрик.
    - **Prometheus + Grafana:** Для мониторинга производительности тестовой инфраструктуры и самого приложения во время
      тестов.
    - **Кастомные скрипты на Python:** Анализ логов, парсинг отчетов, вычисление метрик.

## **Senior Level**

Метрики — это приборная панель Senior QA. Без них вы «летите вслепую». Но важно отличать «метрики тщеславия» (Vanity
Metrics), которые выглядят красиво, от «метрик действий» (Actionable Metrics), которые реально влияют на качество.

Инженерия качества (Quality Engineering) опирается на закон Гудхарта: «Когда мера становится целью, она перестает быть
хорошей мерой». Научные исследования сосредоточены на поиске корреляций между метриками и реальной надежностью ПО.

### Связь покрытия кода и плотности дефектов

Одно из самых важных исследований (Malaiya et al., 2002) установило **логарифмическую связь** между покрытием тестами (
Test Coverage) и плотностью дефектов (Defect Density).

* **Результат:** 100% покрытие не гарантирует 0 багов. Однако, покрытие ниже определенного порога (обычно 70-80% для
  ветвей/branches) экспоненциально увеличивает вероятность отказа в продакшене.
* **Нюанс:** Yamashita (2016) показала, что метрики сложности кода (Cyclomatic Complexity) коррелируют с вероятностью
  багов, но имеют форму «перевернутой U» для плотности дефектов: самые сложные файлы часто имеют *меньше* багов на
  строку кода, так как их пишут и тестируют тщательнее.

***

## 2. Ключевые метрики (Senior Level)

### 1. DRE (Defect Removal Efficiency) — "Король метрик"

Это главная метрика эффективности QA-команды. Она показывает, какой процент багов вы нашли *до* релиза.

$$DRE = \frac{Bugs_{QA}}{Bugs_{QA} + Bugs_{Prod}} \times 100\%$$

* **Эталон:** Мировой стандарт для хорошего процесса — **>85%**. Отличный процесс (High Maturity) — **>95%**.
* **Пример:** QA нашли 90 багов. После релиза пользователи нашли еще 10.
  $DRE = 90 / (90 + 10) = 90\%$. Отличный результат.
* **Действие:** Если DRE падает ниже 85%, значит, ваши тесты (или тестовое окружение) не соответствуют реальности.

### 2. Code Coverage (Покрытие кода)

* **Line Coverage:** Бесполезная метрика для Senior. Можно пройти по строке, но не проверить логику.
* **Branch Coverage (Покрытие ветвлений):** Настоящий стандарт. Проверяет `True` и `False` для каждого `if`.
* **Mutation Score:** Самая "честная" метрика. Специальный тул (например, `mutmut` для Python) ломает ваш код. Если
  тесты не упали — покрытие "липовое".

### 3. Defect Density (Плотность дефектов)

Количество багов на 1000 строк кода (KLOC) или на модуль.

* **Применение:** Помогает найти "горячие точки". Если в модуле "Корзина" 15 багов на KLOC, а в "Профиле" — 2, то
  регресс "Корзины" нужно усилить в 3 раза.

### 4. Mean Time To Detect (MTTD) & Mean Time To Repair (MTTR)

Метрики скорости CI/CD.

* **MTTD:** Сколько времени проходит от коммита "багованного" кода до падения теста? (Хорошо: < 15 мин).
* **MTTR:** Сколько времени проходит от обнаружения критического бага до фикса в продакшене?

***

## 3. Техническая реализация (Python)

Как собирать эти метрики автоматически?

### Mutation Testing (Python)

Вместо того чтобы верить отчету `coverage.py` на слово, используем мутационное тестирование.

```bash
# 1. Ставим библиотеку
pip install mutmut

# 2. Запускаем
mutmut run

# 3. Смотрим результаты
mutmut results
```

**Интерпретация:**

* **Killed:** Тест упал (Хорошо! Мы поймали мутанта).
* **Survived:** Тест прошел, хотя код был сломан (Плохо! Тест "дырявый").

**Пример "дырявого" теста:**

```python
def check_age(age):
    return "Adult" if age >= 18 else "Child"


# Плохой тест (Line Coverage 100%, но Mutation Score низкий)
def test_age():
    assert check_age(20) == "Adult"
    # Этот тест не заметит, если мы заменим `>=` на `>` (Boundary Bug)
    # Мутант (age > 18) выживет!
```

### Сбор DRE (Jira/Allure)

DRE нельзя посчитать в коде, это процессный показатель.

1. В Jira помечайте баги метками `found_in_qa` и `found_in_prod`.
2. Настройте JQL-фильтр или дашборд:
   `(labels = found_in_qa) / ((labels = found_in_qa) + (labels = found_in_prod))`

### Резюме для интервью

На вопрос "Какие метрики вы используете?", Senior QA не должен перечислять всё подряд.
**Правильный ответ:**
> "Я фокусируюсь на DRE, чтобы оценивать эффективность фильтрации багов. Для оценки качества автотестов я использую не
> просто Line Coverage, а Branch Coverage и иногда Mutation Score. А для бизнеса важны метрики стабильности релизов (
> Change Failure Rate)."

- [Содержание](CONTENTS.md#содержание)

---

# **Тест-кейсы и чек-листы

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Баг-репорты и трекеры

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестовая документация

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Методологии тестирования

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Метрики качества кода

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование в Agile/Scrum

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Нефункциональное тестирование

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Эксплораторное тестирование

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Page Object Model (POM)

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование API

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование WebSocket

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование баз данных

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование мобильных приложений

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Работа с прокси и снифферами

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование локализации и интернационализации

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Доступность (Accessibility) тестирование

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Кросс-браузерное и кросс-платформенное тестирование

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование в различных окружениях

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Роль QA в команде разработки

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Test Management системы

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Риск-ориентированное тестирование

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Автоматизация vs ручное тестирование

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Тестирование legacy систем

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Работа с требованиями и пользовательскими историями

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Коммуникация с разработчиками и продакт-менеджерами

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Приоритизация тестовых сценариев

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Оценка сроков тестирования

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Менторинг junior QA

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---

# **Техническая документация для тестов

## **Junior Level**

## **Middle Level**

## **Senior Level**

- [Содержание](CONTENTS.md#содержание)

---



Лягушка
