# Собеседование Python AQA

# Содержание

## Базовые знания

- [Типы данных](#типы-данных)
- [*args и **kwargs](#args-и-kwargs)
- [Хеш-таблица](#хеш-таблица)
- [Встроенные функции](#встроенные-функции)
- [Контекстные менеджеры (with)](#контекстные-менеджеры)
- [Генераторы и итераторы](#генераторы-и-итераторы)
- [Декораторы и замыкания](#декораторы-и-замыкания)
- [GIL (Global Interpreter Lock)](#gil-global-interpreter-lock)
- [Изменение списка во время итерации](#изменение-списка-во-время-итерации)
- [Области видимости](#области-видимости)
- [Lambda-функции](#lambda-функции)
- [Comprehensions и генераторные выражения](#comprehensions-и-генераторные-выражения)
- [copy() и deepcopy()](#copy-и-deepcopy)
- [Асинхронность](#асинхронность)
- [Многопоточность](#многопоточность)
- [Мультипроцессинг](#мультипроцессинг)
- [Dataclass](#dataclass)
- [Enum](#enum)
- [Garbage Collector (сборщик мусора)](#garbage-collector-сборщик-мусора)
- [Сложность кода](#сложность-кода)

## РАЗДЕЛ ООП

- [ООП](#ооп)
- [Абстракция (ООП)](#абстракция)
- [Инкапсуляция (ООП)](#инкапсуляция)
- [Наследование (ООП)](#наследование)
- [Полиморфизм (ООП)](#полиморфизм)
- [Diamond Problem](#diamond-problem)
- [Магические методы](#магические-методы)
- [Инвариантность и ковариантность](#инвариантность-и-ковариантность)
- [Декораторы классов и методов](#декораторы-классов-и-методов)
- [Множественное наследование и MRO](#множественное-наследование-и-mro)
- [ABC (Abstract Base Classes)](#abc)
- [Протокол (Protocol)](#протокол-protocol)
- [Паттерны проектирования](#паттерны-проектирования)
- [Композиция и агрегация](#композиция-и-агрегация)
- [Связность и связанность](#связность-и-связанность)
- [SOLID](#solid)
- [Специфика ООП в Python](#специфика-ооп-в-python)
- [Наследование и композиция](#наследование-и-композиция)
- [Метапрограммирование](#метапрограммирование)
- [Миксины](#миксины)

## Типизация

1. [typing: Optional, Union, TypeVar, Generic](#typing)
2. [Literal, TypedDict, Protocol](#literal-typeddict-protocol)
3. [Ковариантность, контравариантность](#ковариантность-контравариантность)

## Инструменты

1. [pytest](#pytest)
2. [pytest hooks](#pytest-hooks)
3. [Kubernetes](#kubernetes)

## Теория тестирования

1. [Пирамида тестирования](#пирамида-тестирования)
2. [Виды тестирования](#виды-тестирования)
3. [Метрики тестирования](#метрики-тестирования)
4. [Техники тест дизайна](#техники-тест-дизайна)
5. [Автоматизация](#автоматизация)

- [Содержание](#содержание)

---

# **Типы данных**

Типы данных в Python делятся на изменяемые (например, list, dict, set) и неизменяемые (например, int, str, tuple).
Ключевое отличие в том, что изменяемые объекты можно модифицировать после создания, а любые операции над неизменяемыми
создают новый объект. Это фундаментальное различие влияет на их поведение при передаче в функции, возможность быть
ключом словаря и работу механизмов памяти интерпретатора.

## **Junior Level**

В Python, как и в любом языке программирования, данные бывают разных видов или «типов». Это базовые кирпичики, с
которыми работает программа. Их можно разделить на две большие категории: изменяемые и неизменяемые.

Простые, или «скалярные» типы — это числа (целые, вещественные, комплексные), строки текста и логические значения (
`True`/`False`). Важное свойство строк и чисел — их **нельзя изменить после создания**. Если ты пишешь `x = "hello"`, а
потом `x = "world"`, ты не меняешь строку "hello", а создаешь новую и даешь ей то же имя `x`.

Более сложные, «коллекционные» типы — это списки, кортежи, словари и множества. Они нужны для хранения набора других
значений. Здесь ключевое различие: **список можно изменить** (добавить или удалить элемент), а **кортеж — нет**. Словарь
хранит данные в парах «ключ-значение», и его тоже можно изменять.

## **Middle Level**

1. **Классификация по мутабельности (изменяемости):**

* **Изменяемые (mutable):** `list`, `dict`, `set`, `bytearray`, пользовательские классы (по умолчанию). Содержимое
  таких объектов можно менять. При передаче в функцию передается ссылка на тот же самый объект, поэтому изменения,
  сделанные внутри функции, видны снаружи.
* **Неизменяемые (immutable):** `int`, `float`, `complex`, `str`, `bytes`, `tuple`, `frozenset`, `bool`, `NoneType`.
  Экземпляр такого типа после создания не может быть изменен. Любая операция, выглядящая как изменение, на деле
  создает новый объект. Это имеет глубокие последствия для хэшируемости (объекты этих типов обычно хэшируемы),
  использования в качестве ключей словаря и поведения при передаче в функции (передается ссылка на объект, но так
  как объект нельзя изменить, создается иллюзия передачи «по значению»).

2. **Устройство объектов:** В Python всё является объектом. Каждый объект имеет три обязательных атрибута: *
   *идентификатор** (уникальный числовой адрес в памяти, возвращаемый `id()`), **тип** (определяющий возможные операции,
   возвращаемый `type()`) и **значение**. Для неизменяемых типов идентификатор и значение жестко связаны. Для
   изменяемых — объект может менять значение, сохраняя идентификатор.

3. **Специфика типов:**

* `None` — синглтон, объект, обозначающий отсутствие значения. `id(None)` всегда одинаков.
* Булевы значения `True` и `False` — также синглтоны и являются подклассами `int`.
* `tuple` — неизменяем, но если содержит изменяемые элементы (например, списки), то эти внутренние элементы менять
  можно. Это делает кортеж «условно хэшируемым».
* `dict` начиная с Python 3.7 гарантирует сохранение порядка вставки, а с 3.6 это было особенностью реализации
  CPython.
* `set` и `frozenset` хранят только уникальные, хэшируемые элементы. Их внутренняя реализация близка к словарю, где
  есть только ключи.

Сравнение `[1,2,3] == [1,2,3]` дает `True`, но `id()` у этих списков разный.

## **Senior Level**

1. **Всё есть `PyObject`:** В исходном коде CPython (`Include/object.h`) лежит фундамент — структура `PyObject`. Это
   базовый «контейнер» для любого типа данных.

```c
typedef struct _object {
    Py_ssize_t ob_refcnt;  // Счетчик ссылок — основа механизма GC
    PyTypeObject *ob_type; // Указатель на структуру типа
    } PyObject;
```

Каждый объект в Python начинается с этих двух полей. `ob_refcnt` — счетчик ссылок для сборщика мусора. `ob_type` —
указатель на другой объект — его тип, который сам является объектом (`PyTypeObject`). Добавление новых полей
происходит путем «расширения» этой структуры. Например, `PyLongObject` (целое число) добавляет поле для хранения
цифр.

2. **Неизменяемость и интернирование (interning):** Это не просто договоренность, а оптимизация на уровне
   интерпретатора.

* **Маленькие целые числа:** Диапазон обычно от -5 до 256. Эти объекты создаются при запуске интерпретатора и
  хранятся в специальном массиве. Операция `a = 10; b = 10` приведет к тому, что `a` и `b` будут указывать на **один
  и тот же** объект в памяти (`id(a) == id(b)`). Проверка делается макросом `PyLong_FromLong`.
* **Строки (interned strings):** Если строка состоит только из символов ASCII, букв, цифр и подчеркивания, и не
  выглядит как число, она также может быть интернирована. Это особенно важно для имен переменных, атрибутов. Python
  делает это автоматически, но можно принудительно интернировать строку через `sys.intern()`. Это ускоряет сравнение
  строк (достаточно сравнить указатели, `a is b`) и экономит память в случае множества одинаковых строк (например,
  при парсинге больших XML/JSON в тестах).

3. **Мутабельность и байткод:** Рассмотрим операцию `my_list.append(x)`. Байткод инструкция `LIST_APPEND` работает
   непосредственно с внутренним C-массивом структуры `PyListObject`. Объект списка (`list`) хранит указатель (
   `**ob_item`) на этот массив указателей на `PyObject` и его текущую длину (`ob_size`). `LIST_APPEND` увеличивает
   `ob_size`, при необходимости перераспределяет память для `ob_item` (сложность amortized O(1)) и помещает в новый слот
   ссылку на `x`, увеличивая `ob_refcnt` у объекта `x`. Никакого нового `list` не создается, `id(my_list)` остается
   прежним.

4. **Словарь: краеугольный камень языка.** `dict` — не просто тип, это фундаментальная структура, используемая
   повсеместно: пространства имен модулей, атрибуты объектов, передача аргументов в функции (`**kwargs`) реализованы
   через словари. Его внутренности — это хэш-таблица (массив `PyDictKeyEntry`). Ключевая хитрость в том, как разрешаются
   коллизии (метод открытой адресации). При удалении многих элементов словарь может оставаться разреженным, что ведет к
   утечкам памяти. Для AQA это означает, что долгоживущие объекты с большими изменяющимися словарями (например, кэши в
   тестируемом приложении) требуют мониторинга памяти. Словари также резко замедляются при атаках хэш-коллизиями, что
   может быть вектором DoS-атаки — это важно для security-тестирования.

5. **`tuple` vs `list`: Не просто мутабельность.** `tuple` из-за своей неизменности аллоцируется одной непрерывной
   областью памяти. Его `ob_item` — это встроенный массив указателей фиксированного размера. `list` же имеет буфер с
   «запасом» (`allocated`), чтобы не переаллоцировать память при каждом `append`. Сравнение `is` для кортежей,
   содержащих только неизменяемые элементы, может давать `True` благодаря механизму кэширования (`tupleobject.c`):
   Python может переиспользовать только что созданный кортеж, если он пуст или состоит из одного элемента.

6. **Типизация для AQA:** На этом уровне мы понимаем, что система типов Python — динамическая, но строгая (strong).
   «Утиная типизация» реализуется через механизм поиска атрибутов в `__dict__` объекта и далее по MRO (Method Resolution
   Order). Паттерн `isinstance(obj, abc.ABC)` или `hasattr(obj, '__len__')` на байткод-уровне сводится к проверкам
   `PyObject_IsInstance` и `PyObject_HasAttr`, которые проходят по цепочке классов. Для Senior AQA критично понимать эти
   механизмы при тестировании полиморфных компонентов, мокинге и создании сложных фикстур, имитирующих определенные
   интерфейсы.

- [Содержание](#содержание)

---

# **args и kwargs**

*args и **kwargs позволяют функциям принимать произвольное количество аргументов: *args собирает позиционные аргументы в
кортеж, а **kwargs — именованные в словарь. Этот механизм также используется для распаковки коллекций при вызове
функций. Он является основой для создания гибких API, декораторов и функций-обёрток.

## **Junior Level**

`*args` и `**kwargs` — это специальные синтаксические конструкции в Python, позволяющие функциям принимать произвольное
количество аргументов.

`*args` (от слова "arguments") собирает все **позиционные аргументы**, переданные функции сверх явно объявленных, в *
*кортеж**. Это полезно, когда вы не знаете заранее, сколько аргументов может понадобиться передать.

`**kwargs` (от "keyword arguments") собирает все **именованные аргументы** (ключ=значение), которые не были явно
перечислены в параметрах функции, в **словарь**. Это часто используется для передачи конфигурационных параметров или для
создания функций-оберток.

Также символы `*` и `**` используются при **вызове** функции для распаковки коллекций в отдельные аргументы. `*`
распаковывает итерируемый объект (список, кортеж) в позиционные аргументы, а `**` распаковывает словарь в именованные
аргументы.

Это фундаментальный механизм для создания гибких API и декораторов.

## **Middle Level**

На этом уровне важно понимать **строгие правила порядка параметров** и внутреннее представление:

1. **Строгий порядок в определении функции**:

```
def f(a, b, *args, c=None, d=None, **kwargs)
```

Порядок следования:

- Позиционные параметры (a, b)
- `*args` — собирает избыточные позиционные аргументы
- Keyword-only аргументы (c, d) — могут быть переданы только по имени
- `**kwargs` — собирает избыточные именованные аргументы

После `*args` все последующие параметры становятся **keyword-only** (требуют явного указания имени). Это важная фича
для создания чистого API.

2. **Распаковка на уровне байткода**:
   Когда вы вызываете `func(*[1, 2, 3])`, происходит:

- Байткод `BUILD_LIST` создает список
- Байткод `CALL_FUNCTION_EX` с флагом `0x01` (HAVE_ARG_FLAGS) распаковывает итерируемый объект в аргументы
- Внутри функции эти аргументы доступны через `sys._getframe().f_locals`

3. **`*` и `**` — не магия, а синтаксический сахар**:
   Конструкция `def foo(*args, **kwargs)` компилируется в функцию с двумя специальными параметрами. При компиляции в
   байткод для них создаются отдельные инструкции для упаковки аргументов в кортеж и словарь.

4. **Важные нюансы для AQA**:

- При передаче словаря в `**kwargs` ключи **должны быть строками**
- Дублирование имен аргументов при распаковке приводит к `TypeError`
- `**kwargs` сохраняет порядок аргументов начиная с Python 3.6 (благодаря сохранению порядка вставки в словарях)
- Метод `__getitem__` объекта используется при распаковке через `**`, что позволяет распаковывать не только словари,
  но и любые mapping-объекты

## **Senior Level (Байткод, CPython и грабли производительности)**

1. **Уровень байткода: как работает упаковка аргументов**:

Рассмотрим функцию:

```python
def func(*args, **kwargs):
    pass
```

Байткод для вызова `func(1, 2, a=3, b=4)`:

```
LOAD_CONST               1 (1)
LOAD_CONST               2 (2)
LOAD_CONST               3 (3)
LOAD_CONST               4 (4)
LOAD_CONST               5 (('a', 'b'))  # Имена ключей
CALL_FUNCTION_KW         2           # 2 позиционных + keyword args
```

Байткод внутри `func` для доступа к аргументам:

```
# Для *args
BUILD_TUPLE              # Собирает позиционные аргументы в кортеж
STORE_FAST             0 (args)

# Для **kwargs  
BUILD_MAP                # Собирает именованные аргументы в словарь
STORE_FAST             1 (kwargs)
```

В CPython 3.10+ используется `CALL` с флагами вместо отдельных `CALL_FUNCTION*` инструкций.

2. **CPython: механизм вызова функций**:

В `Include/cpython/abstract.h` функция `PyObject_Call` принимает `PyObject *args` (кортеж) и `PyObject *kwargs` (
словарь). Вся система вызова построена вокруг этих двух структур.

Когда интерпретатор видит `*args` в вызове, он выполняет:

- `PySequence_Tuple` для преобразования итерируемого объекта в кортеж
- `PyTuple_New` для создания нового кортежа аргументов
- Конкатенацию с существующими позиционными аргументами

3. **Критические оптимизации CPython 3.11+**:

В Python 3.11 появилась **специализация байткода для вызовов функций** (PEP 659). Интерпретатор создает "
адаптивные" (adaptive) инструкции вызова, которые кэшируют:

- Форму вызова (позиционные vs именованные аргументы)
- Типы передаваемых аргументов
- Количество аргументов

Например, вызов `func(*args)` без именованных аргументов компилируется в специализированную версию `CALL`, которая
пропускает проверки на наличие `kwargs`. Это дает до 50% ускорения для частых вызовов.

**Но!** Если функция определена как `def f(**kwargs)`, а вызывается как `f(*args)`, происходит деоптимизация — сброс
кэша и возврат к обобщенному медленному пути.

4. **Frame object и доступ к аргументам**:

Локальные переменные функции (включая `args` и `kwargs`) хранятся в `frame->f_localsplus` — массиве указателей
`PyObject*`. `*args` занимает один слот (указатель на кортеж), `**kwargs` — один слот (указатель на словарь).

Доступ через `sys._getframe()` позволяет инспектировать это в runtime, что используется в продвинутых тестовых
фреймворках для анализа вызовов.

5. **`inspect.Signature` и валидация аргументов**:

Модуль `inspect` использует `__code__.co_varnames`, `__code__.co_argcount`, `__code__.co_kwonlyargcount` для
реконструкции сигнатуры. `*args` соответствует `__code__.co_flags & 0x04` (CO_VARARGS), `**kwargs` — `0x08` (
CO_VARKEYWORDS).

6. **Производительность и антипаттерны**:

- **Двойная упаковка/распаковка**: `func(*tuple(args), **dict(kwargs))` создает **новые** кортеж и словарь, копируя
  все элементы. Вместо этого нужно использовать прямое присваивание.

- **Рекурсивная распаковка в циклах**:

```python
for item in items:
    process(**item)  # Создание нового словаря для каждого вызова!
```

Лучше: `process(key1=item['key1'], key2=item['key2'])` если сигнатура известна.

- **Большие `*args`**: передача огромного списка через `*` приводит к созданию кортежа из всех элементов на стеке
  вызовов, что может вызвать `RecursionError` при глубокой рекурсии.

7. **Специфика для декораторов (как для AQA)**:

При написании декораторов для тестов:

```python
def retry(max_attempts):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)  # Внимание!

                except AssertionError:
                    if attempt == max_attempts - 1:
                        raise

        return wrapper

    return decorator
```

**Проблема**: каждый вызов `func(*args, **kwargs)` внутри `wrapper` создает **новые** кортеж и словарь. Для high-load
тестов это может стать бутылочным горлышком. Решение — использовать `functools.wraps` который кэширует signature.

8. **Интроспекция и мокирование**:

Для создания умных моков в тестах нужно понимать, как `unittest.mock` работает с `*args`/`**kwargs`:

- `mock.call_args` хранит `args` как кортеж и `kwargs` как словарь
- `mock.assert_called_with(*args, **kwargs)` использует ту же семантику распаковки
- При `side_effect = lambda *a, **k: ...` сигнатура должна точно соответствовать

9. **C-расширения и `PyArg_ParseTupleAndKeywords`**:

В нативных модулях функция `PyArg_ParseTupleAndKeywords` принимает:

```c
static PyObject* func(PyObject *self, PyObject *args, PyObject *kwargs)
{
char *keywords[] = {"param1", "param2", NULL};
// Парсинг args и kwargs
}
```

Здесь `args` и `kwargs` — те же PyObject*, что и в Python-функциях. Это знание критично для тестирования нативных
расширений.

- [Содержание](#содержание)

---

# *Хеш-таблица*

Хеш-таблица — это структура данных, обеспечивающая амортизированную сложность O(1) для операций поиска, вставки и
удаления за счёт использования хеш-функции, преобразующей ключ в индекс массива. В Python она лежит в основе словарей (
dict) и множеств (set), используя открытую адресацию для разрешения коллизий. Ключевыми особенностями являются
требование хешируемости (неизменяемости) ключей, сохранение порядка вставки и автоматическое увеличение размера при
достижении определённого коэффициента заполнения.

## **Junior Level**

Хеш-таблица — это структура данных, которая позволяет очень быстро находить, добавлять и удалять элементы. В Python
хеш-таблицы лежат в основе двух ключевых типов: **словарей (dict)** и **множеств (set)**.

Представьте себе библиотеку с книгами. Вместо того чтобы искать книгу по названию, перебирая все полки, вы вычисляете
номер полки по названию книги по определенному правилу (например, первая буква). Это правило — **хеш-функция**. Она
преобразует ключ (название книги) в число (номер полки). В идеальном случае вы сразу идете к нужной полке и находите
книгу за O(1) время.

Коллизии (когда две разные книги должны лежать на одной полке) решаются разными способами — например, на полке может
быть несколько книг, и вы тогда ищете среди них уже по полному названию.

В Python словарь — это коллекция пар «ключ-значение», где ключ должен быть **хешируемым** (неизменяемым) объектом.
Множество — это коллекция уникальных хешируемых элементов.

## **Middle Level**

В Python хеш-таблицы реализованы через открытую адресацию (open addressing) с двойным хешированием для разрешения
коллизий.

**Ключевые аспекты:**

1. **Хешируемость:** Объект хешируем, если:

- Имеет метод `__hash__`, возвращающий целое число
- Имеет метод `__eq__` для сравнения
- Удовлетворяет условию: `a == b` ⇒ `hash(a) == hash(b)`

Неизменяемые типы (int, str, tuple, frozenset) хешируемы. Изменяемые (list, dict, set) — нет, но могут стать
хешируемыми, если реализовать неизменяемую версию.

2. **Размер таблицы:** Всегда является степенью двойки. Это позволяет использовать быструю битовую маску для вычисления
   индекса: `index = hash(key) & (table_size - 1)`.

3. **Коэффициент загрузки (load factor):** При достижении ~2/3 заполнения таблица увеличивается вдвое, происходит *
   *rehashing** — пересчет позиций всех элементов. Это амортизированная операция O(n).

4. **Разрешение коллизий:** Используется **квадратичное зондирование (quadratic probing)** вида
   `index = (5*index + 1 + perturb) & mask`, где `perturb` изначально равен хешу, а затем сдвигается. Это обеспечивает
   хорошее распределение.

5. **Удаление элементов:** При удалении элемент не удаляется физически, а помечается как **dummy** (удаленный слот). Это
   необходимо для сохранения цепочек зондирования.

6. **Порядок элементов:** Начиная с Python 3.7/3.6 (как деталь реализации CPython) порядок вставки сохраняется. Это
   достигается тем, что хеш-таблица хранит индексы в отдельном массиве записей (ключ-значение), который сохраняет
   порядок вставки.

## **Senior Level (CPython, память, байткод и темные углы)**

1. **Структура PyDictObject:**

В `Include/cpython/dictobject.h`:

```c
typedef struct {
PyObject_HEAD
Py_ssize_t ma_used;      // Количество активных элементов
Py_ssize_t ma_version_tag // Уникальная версия для обнаружения изменений
PyDictKeysObject *ma_keys; // Указатель на ключи
PyObject **ma_values;      // Указатель на значения (для split-table)
} PyDictObject;
```

**Эволюция структур:**

- До Python 3.6: единая хеш-таблица размером 8 строк (indices + entries)
- Python 3.6+: **split-table layout**: отдельно массив индексов (`dk_indices`) и массив записей (`dk_entries`)
- Python 3.11+: **компактная модель** с кэшированием хешей

2. **Детали split-table layout:**

Массив `dk_indices` хранит не записи, а индексы в `dk_entries`:

- Для таблицы размером 8: `dk_indices[hash & 7] = i` где `i` — индекс в `dk_entries`
- `dk_entries` — массив структур `PyDictKeyEntry`, хранящих хеш, ключ, значение

Это дает:

- Сохранение порядка вставки (массив `dk_entries` заполняется последовательно)
- Улучшенную локальность памяти при итерации
- Меньшую фрагментацию памяти

3. **Хеш-атаки и SipHash:**

До Python 3.4 использовался простой хеш (FNV для строк), что позволяло проводить DoS-атаки через искусственное
создание коллизий. С Python 3.4 для строк, bytes и datetime используется **SipHash24** — криптографически стойкая
хеш-функция с ключом, рандомизируемым при запуске интерпретатора.

Ключ для SipHash хранится в `_Py_HashSecret` (глобальная переменная). Для тестирования можно установить
`PYTHONHASHSEED=0` для детерминированного поведения.

4. **Байткод операций с dict:**

- `LOAD_GLOBAL` → `PyDict_GetItem` по ключу-строке
- `STORE_SUBSCR` для dict → `PyObject_SetItem`
- `BUILD_MAP` → `_PyDict_NewPresized`

При компиляции словарных литералов `{k: v}` Python 3.9+ использует `BUILD_MAP` с предвычисленными хешами для
константных ключей.

5. **Оптимизации CPython 3.10+:**

- **PEP 603**: Добавлен `dict.__getitem__` с быстрым путем для строковых ключей
- **PEP 659**: Специализация байткода для операций со словарями:

```python
# Адаптивный (adaptive) байткод для dict[key]
# Первые несколько выполнений собирают статистику
# Если ключ всегда строка, генерируется специализированный байткод
# Который использует быстрый путь поиска в хеш-таблице
```

6. **Скрытые структуры для оптимизации:**

- **Keys-sharing (dict splitting)**: Когда создается много объектов с одинаковыми атрибутами (например, экземпляры
  класса), их `__dict__` могут разделять таблицу ключей (`ma_keys`), храня только значения в `ma_values`. Это
  экономит память.

- **Compact dict (Python 3.11)**: Запись `PyDictKeyEntry` уменьшена с 24 до 8 байт за счет выноса хеша в отдельный
  массив.

7. **Сборка мусора и weakref:**

Словари участвуют в циклическом GC через `Py_TPFLAGS_HAVE_GC`. Weakref-словари (`weakref.WeakKeyDictionary`,
`WeakValueDictionary`) используют специальные прокси-объекты, которые не увеличивают счетчик ссылок.

Важно: обычные словари хранят **сильные ссылки** на ключи и значения, что может приводить к утечкам памяти в циклах.

8. **Производительность и антипаттерны:**

- **Изменение ключа-объекта после вставки:** Если объект-ключ изменяется так, что меняется его хеш, он становится *
  *невозможным для нахождения**. Это коварный баг.

- **Частые resizing:** При добавлении N элементов в пустой словарь происходит ~log₂(N) ресайзов. Решение:
  `dict.fromkeys()` или предвыделение через `dict(initial_size)`.

- **Итерация с изменением:** Изменение размера словаря во время итерации вызывает `RuntimeError`. Но изменение
  значений (не ключей) безопасно.

- **Memory overhead:** Пустой словарь занимает ~72 байта (Python 3.11), каждая запись — дополнительно 8-32 байта в
  зависимости от размера.

9. **Интроспекция через CPython API:**

Для тестирования можно использовать:

```python
import sys

d = {}
sys.getsizeof(d)  # Размер всей структуры
d.__sizeof__()  # Тоже

# Просмотр внутренних структур (CPython specific)
d.__dictoffset__  # Смещение для __dict__ в объектах
```

10. **Для множеств (set):**

Используется та же хеш-таблица, но без значений (только ключи). Особенности:

- `set` хранит только хеш и ключ
- `frozenset` — неизменяемая версия, кэширует хеш самого множества
- Операции вроде `union`, `intersection` используют оптимизированные C-реализации

11. **Специфика тестирования хеш-таблиц:**

- **Тестирование коллизий:** Создание объектов с одинаковым хешем для проверки деградации производительности
- **Тестирование rehashing:** Измерение времени вставки при достижении порогов заполнения
- **Проверка сохранения порядка:** Гарантия, что `list(dict.keys())` соответствует порядку вставки
- **Тестирование memory leaks:** Убедиться, что удаление элементов освобождает память (но помнить про dummy-слоты)
- **Конкурентность:** Dict не потокобезопасен. Тестирование race conditions при одновременном чтении/записи.

- [Содержание](#содержание)

---

# *Встроенные функции*

Встроенные функции — это базовый набор функций Python, доступных без импорта, так как они находятся в автоматически
загружаемом модуле `builtins`. Они охватывают основные операции языка: преобразование типов, математические вычисления,
работу с коллекциями, ввод-вывод и интроспекцию. Будучи частью ядра языка, эти функции реализованы максимально
эффективно и имеют стандартизированное поведение.

## **Junior Level**

Встроенные функции — это функции, которые доступны в Python по умолчанию, без необходимости импорта каких-либо модулей.
Они представляют собой базовый инструментарий языка и всегда находятся в глобальной области видимости.

Эти функции охватывают основные операции: работу с типами данных (`str()`, `int()`, `list()`), математические
вычисления (`abs()`, `round()`, `sum()`), преобразования (`len()`, `sorted()`, `reversed()`), ввод-вывод (`print()`,
`input()`), итерации (`range()`, `enumerate()`, `zip()`), проверки (`isinstance()`, `hasattr()`), и другие
фундаментальные операции.

Важно понимать, что это не просто функции, а часть ядра языка. Они реализованы максимально эффективно и их поведение
стандартизировано.

## **Middle Level**

1. **Пространство имен `builtins`**: Все встроенные функции находятся в модуле `builtins`, который автоматически
   импортируется при запуске интерпретатора. Можно получить прямой доступ через `import builtins`. Переопределение
   функций в этом модуле (что крайне не рекомендуется) повлияет на всю программу.

2. **Категории встроенных функций**:

- **Конструкторы типов**: `int()`, `str()`, `list()`, `dict()`, `set()`, `tuple()`, `bytes()`, `bytearray()`,
  `memoryview()`, `frozenset()`
- **Математические**: `abs()`, `divmod()`, `pow()`, `round()`, `sum()`, `min()`, `max()`
- **Преобразования и проверки**: `bool()`, `complex()`, `float()`, `hash()`, `id()`, `isinstance()`, `issubclass()`,
  `callable()`
- **Работа с коллекциями**: `len()`, `sorted()`, `reversed()`, `enumerate()`, `zip()`, `filter()`, `map()`,
  `all()`, `any()`, `slice()`
- **Итераторы и генераторы**: `iter()`, `next()`, `range()`
- **Ввод-вывод**: `print()`, `input()`, `open()`
- **Компиляция и выполнение**: `eval()`, `exec()`, `compile()`
- **Отражение (introspection)**: `dir()`, `globals()`, `locals()`, `vars()`, `getattr()`, `setattr()`, `delattr()`,
  `hasattr()`, `property()`, `classmethod()`, `staticmethod()`, `super()`
- **Разное**: `breakpoint()`, `__import__()`, `format()`, `repr()`, `ascii()`, `chr()`, `ord()`, `bin()`, `oct()`,
  `hex()`

3. **Особенности поведения**:

- `sorted()` всегда возвращает новый список, тогда как метод `list.sort()` изменяет список на месте
- `reversed()` возвращает итератор, а не список
- `map()` и `filter()` в Python 3 возвращают итераторы, а не списки (как было в Python 2)
- `range()` тоже возвращает специальный объект, а не список
- `open()` является фабрикой, возвращающей файловый объект с разным поведением в зависимости от режима

4. **Функции высшего порядка**: `map()`, `filter()`, `sorted()` принимают функции в качестве аргументов. Это делает их
   мощным инструментом для функционального программирования.

## **Senior Level (CPython, байткод и системные вызовы)**

1. **Реализация в CPython**:

Встроенные функции реализованы в C в файлах CPython:

- `Python/bltinmodule.c` — основные встроенные функции
- `Objects/` — конструкторы типов (`listobject.c`, `dictobject.c` и т.д.)
- `Python/` — специализированные функции (`pythonrun.c` для `exec()`)

Каждая функция представлена структурой `PyMethodDef`:

```c
static PyMethodDef builtin_methods[] = {
{"abs",       builtin_abs,       METH_O,  abs_doc},
{"all",       builtin_all,       METH_O,  all_doc},
// ...
{NULL,        NULL}  /* Sentinel */
};
```

Флаг `METH_O` означает, что функция принимает один объект (позиционный аргумент). Есть также `METH_VARARGS`,
`METH_KEYWORDS` и их комбинации.

2. **Байткод и вызов встроенных функций**:

При вызове `len(obj)` генерируется байткод:

```
LOAD_NAME                0 (len)
LOAD_NAME                1 (obj)
CALL_FUNCTION            1
```

Но для некоторых часто используемых функций есть специализированные инструкции:

- `BUILD_LIST`, `BUILD_TUPLE`, `BUILD_SET`, `BUILD_MAP` — вместо вызова конструкторов
- `UNPACK_SEQUENCE`, `UNPACK_EX` — для распаковки
- `COMPARE_OP` — вместо вызова `cmp()` (удалена в Python 3)

3. **Оптимизации CPython 3.11+**:

**PEP 659 (специализация байткода)** добавила адаптивные инструкции для встроенных функций:

- `CALL` с кэшированием типа результата и побочных эффектов
- Для `len()`, `sum()`, `range()` создаются специализированные быстрые пути
- При частом вызове `len(list)` байткод заменяется на инструкцию, которая напрямую обращается к `PyList_GET_SIZE`

4. **`__builtins__` vs `builtins`**:

- `__builtins__` — это псевдомодуль, который есть в каждом модуле. В `__main__` это ссылка на модуль `builtins`, а в
  импортированных модулях — на его словарь `builtins.__dict__`
- `builtins` — реальный модуль, который можно импортировать
- Это различие важно для тестирования, так как переопределение в `__builtins__` влияет только на текущий модуль

5. **Опасные функции: `eval()`, `exec()`, `compile()`**:

- `eval()` принимает выражение и возвращает его значение. Работает в текущем пространстве имен
- `exec()` выполняет код (может быть многострочным). Возвращает `None`
- `compile()` преобразует строку в объект кода, который потом можно выполнить

**Безопасность**: Эти функции выполняют произвольный код. В production-коде нужно:

- Ограничивать глобальные и локальные пространства имен
- Использовать `ast.literal_eval()` для безопасного вычисления литералов
- В тестах — быть осторожным при тестировании кода, использующего эти функции

6. **`property()`, `classmethod()`, `staticmethod()` как дескрипторы**:

Эти функции не просто возвращают декорированные методы — они создают объекты-дескрипторы:

```c
// property() в CPython
property_new(PyTypeObject *type, PyObject *args, PyObject *kwds) {
// Создает property object с слотами для getter, setter, deleter, doc
}
```

При доступе к свойству через экземпляр класса срабатывает протокол дескриптора (`__get__`, `__set__`, `__delete__`).

7. **`super()` — магия на уровне C**:

`super()` не просто возвращает родительский класс. Она:

- Динамически вычисляет MRO (Method Resolution Order)
- Использует `__class__` и `self` из фрейма вызова
- В CPython: `super_new()` в `Objects/typeobject.c` анализирует стек вызовов через `PyThreadState_GET()->frame`

8. **`range()` — не просто функция, а фабрика объектов**:

В Python 3 `range()` возвращает объект типа `range`, который:

- Реализует `__len__`, `__getitem__`, `__contains__`
- Поддерживает слайсинг: `range(10)[2:5]` возвращает новый `range`
- Имеет постоянную память O(1) независимо от размера
- В CPython вычисляет элементы на лету через формулу: `start + i*step`

9. **`print()` и системные вызовы**:

Реализация `print()` в `builtin_print()`:

- Парсит аргументы: `sep`, `end`, `file`, `flush`
- По умолчанию `file=sys.stdout` (объект `PyTextIOWrapper`)
- Вызывает `PyFile_WriteObject()` для каждого аргумента
- При `flush=True` вызывает `PyObject_CallMethod(file, "flush", NULL)`
- В тестах можно перехватывать вывод через `io.StringIO` или мокать `sys.stdout`

10. **`open()` и файловые дескрипторы**:

`open()` — это фабрика, которая возвращает разные типы в зависимости от режима:

- Текстовый режим: `_io.TextIOWrapper`
- Бинарный: `_io.BufferedReader` или `_io.BufferedWriter`
- Режим 'x' (эксклюзивное создание): проверка через `os.O_EXCL`

На уровне системы вызывает `open()` из libc с флагами `O_RDONLY`, `O_WRONLY`, `O_CREAT` и т.д.

11. **`__import__()` — основа импорта**:

Эта функция:

- Вызывается оператором `import`
- Проходит через `importlib` и sys.meta_path
- Кэширует загруженные модули в `sys.modules`
- В тестах можно мокать для изоляции модулей

12. **Производительность и микрооптимизации**:

- `len()` для встроенных типов — O(1), так как обращается к полю `ob_size` в `PyObject`
- `sum()` использует быстрый путь для чисел, но медленный для других типов (из-за создания промежуточных объектов)
- `min()`/`max()` для отсортированных данных могут быть оптимизированы, но в общем случае — O(n)
- `sorted()` использует Timsort (гибрид сортировки слиянием и вставками)

13. **Для AQA: тестирование встроенных функций**:

- **Мокирование**: `unittest.mock.patch('builtins.open')` для тестирования работы с файлами
- **Перехват ввода-вывода**: `io.StringIO` для `input()`/`print()`
- **Изоляция**: временное изменение `sys.path` для тестирования импорта
- **Безопасность**: тестирование `eval()`/`exec()` на уязвимости инъекции кода
- **Производительность**: бенчмаркинг встроенных функций vs кастомных реализаций
- **Поведение при ошибках**: как функции реагируют на некорректные аргументы

14. **Диагностика через байткод**:

Можно анализировать, как используются встроенные функции в тестируемом коде:

```python
import dis

dis.dis(some_function)  # Показывает CALL_FUNCTION для встроенных функций
```

- [Содержание](#содержание)

---

# *Контекстные менеджеры*

Контекстные менеджеры в Python обеспечивают правильное управление ресурсами с помощью оператора `with`, автоматически
выполняя настройку и очистку. Они реализуют методы `__enter__` для инициализации и `__exit__` для гарантированного
освобождения ресурса, даже если в блоке кода возникло исключение. Это делает их идеальными для работы с файлами,
сетевыми соединениями и транзакциями баз данных.

## **Junior Level**

Контекстные менеджеры в Python — это специальные объекты, которые позволяют управлять ресурсами и выполнять
настройку/очистку до и после выполнения блока кода. Они используются с оператором `with`, который обеспечивает
правильное приобретение и освобождение ресурсов, даже если в блоке кода произошла ошибка.

Представьте, что вы открываете файл для чтения. Вам нужно гарантировать, что файл будет закрыт после работы, независимо
от того, успешно ли вы прочитали данные или произошла ошибка. Контекстный менеджер решает именно эту задачу:

```python
with open('file.txt') as f:
    data = f.read()
# Здесь файл уже гарантированно закрыт
```

Это работает не только для файлов, но и для любых ресурсов: сетевых соединений, транзакций баз данных, блокировок
потоков и т.д.

## **Middle Level**

1. **Протокол контекстного менеджера**: Любой объект может стать контекстным менеджером, если реализует два специальных
   метода:

- `__enter__(self)` — вызывается при входе в блок `with`. Возвращаемое значение присваивается переменной после `as`.
- `__exit__(self, exc_type, exc_value, traceback)` — вызывается при выходе из блока `with`. Получает информацию об
  исключении (если оно произошло). Если метод возвращает `True`, исключение считается обработанным.

2. **Классические способы создания**:

- Реализация класса с `__enter__` и `__exit__`
- Использование `contextlib.contextmanager` декоратора для генераторной функции
- Готовые менеджеры из `contextlib`: `closing()`, `suppress()`, `nullcontext()`

3. **Вложенность**: Можно использовать несколько контекстных менеджеров в одном операторе:

```python
with open('a.txt') as f1, open('b.txt') as f2:
# работа с двумя файлами
```

Это эквивалентно вложенным блокам `with`. Порядок выхода обратный порядку входа (LIFO).

4. **Особенности обработки исключений**:

- Если в `__exit__` не перехватить исключение, оно пробрасывается дальше
- Исключение в `__enter__` приводит к тому, что `__exit__` не вызывается
- Исключение в `__exit__` заменяет исходное исключение (если было)

5. **Применение в тестировании**:

- `unittest.mock.patch` является контекстным менеджером
- `pytest.raises` для проверки исключений
- Временное изменение настроек или переменных окружения

## **Senior Level (Байткод, CPython и системные вызовы)**

1. **Байткодовая реализация `with`**:

Рассмотрим байткод для простого `with`:

```python
with manager as var:
    body
```

Генерируется следующий байткод (упрощенно):

```
SETUP_WITH           L1     # Настраивает блок with, L1 - адрес для завершения
LOAD_FAST            manager
CALL_FUNCTION        0      # Вызов __enter__
STORE_FAST           var
...                   body  # Код тела
POP_BLOCK
LOAD_CONST           None
L1: WITH_CLEANUP_START       # Начало очистки
LOAD_FAST            manager
CALL_FUNCTION        3      # Вызов __exit__ с None, None, None
WITH_CLEANUP_FINISH
```

Для Python 3.9+ используется байткод `WITH_EXCEPT_START` для обработки исключений.

2. **CPython: макросы и фреймы**:

В `Python/ceval.c` операция `SETUP_WITH`:

- Сохраняет текущий контекст в стеке фрейма
- Устанавливает обработчик исключений (`f->f_exc_info`)
- Вызывает `PyObject_CallNoArgs` для `__enter__`

При возникновении исключения в теле `with`:

- Интерпретатор переходит в `WITH_EXCEPT_START`
- Извлекает `exc_type`, `exc_value`, `traceback` из `f->f_exc_info`
- Вызывает `__exit__(exc_type, exc_value, traceback)`

3. **Асинхронные контекстные менеджеры**:

Для `async with` используется протокол `__aenter__`/`__aexit__`. Байткод:

```
BEFORE_ASYNC_WITH
GET_AWAITABLE
LOAD_FAST
CALL_FUNCTION
YIELD_FROM          # Ожидание __aenter__
SETUP_ASYNC_WITH
...
```

Важно: асинхронные менеджеры приостанавливают выполнение корутины в точках `__aenter__` и `__aexit__`.

4. **`contextlib` и генераторы**:

Декоратор `@contextmanager` превращает генератор в контекстный менеджер:

```python
@contextmanager
def manager():
    setup()


try:
    yield resource
finally:
    cleanup()
```

Под капотом:

- Генератор оборачивается в класс `_GeneratorContextManager`
- `__enter__` вызывает `next(gen)` до `yield`
- `__exit__` вызывает `next(gen)` для завершения или `gen.throw()` при исключении
- Критически важно: если между `yield` и `finally` возникает исключение, оно преобразуется в `RuntimeError`

5. **Системные ресурсы и файловые дескрипторы**:

Для файлового менеджера `open()`:

- `__enter__` возвращает файловый объект (уже открытый)
- `__exit__` вызывает `file.close()`, который:
- Сбрасывает буферы (`fflush()` для текстовых режимов)
- Вызывает `close()` системного файлового дескриптора через `os.close(fd)`
- Устанавливает `file.closed = True`

Особенность: файловый объект имеет `__del__`, который тоже вызывает `close()`, но полагаться на это нельзя — сборщик
мусора может сработать поздно.

6. **Транзакции и атомарность**:

В базах данных контекстные менеджеры обеспечивают атомарность:

```python
with connection.transaction():
    cursor.execute("UPDATE ...")
```

Реализация:

- `__enter__`: `BEGIN TRANSACTION`
- `__exit__`: если исключение — `ROLLBACK`, иначе `COMMIT`

Нюанс: если в `__exit__` происходит исключение во время `ROLLBACK`, исходное исключение теряется. Правильная
реализация должна это учитывать.

7. **Блокировки и многопоточность**:

`threading.Lock` — классический контекстный менеджер:

- `__enter__`: `self.acquire()` (блокирующий вызов)
- `__exit__`: `self.release()`

Важно для тестирования:

- `__enter__` может принимать `timeout` параметр
- RLock (рекурсивная блокировка) считает количество входов
- При использовании в `with` гарантируется release даже при исключении

8. **Производительность и оптимизации CPython 3.11+**:

Python 3.11 добавляет специализацию байткода для контекстных менеджеров:

- Кэширование `__enter__` и `__exit__` методов
- Инлайн-кэширование для частых менеджеров
- Ускорение на 10-20% для вложенных `with`

Однако, для менеджеров на основе `@contextmanager` (генераторы) оптимизаций меньше — дополнительный накладные расходы
на фрейм генератора.

9. **Диагностика и отладка**:

Для AQA важно уметь отлаживать контекстные менеджеры:

- `sys.settrace` может отслеживать вход/выход из `with`
- Можно модифицировать байткод через `bytecode` или `code` модули
- `inspect.currentframe()` внутри `__enter__`/`__exit__` показывает стек вызовов

10. **Кастомные обработчики исключений**:

Продвинутый паттерн — менеджер, который преобразует исключения:

```python
class ConvertException:

    def __exit__(self, exc_type, exc_val, tb):
        if exc_type == ValueError:
            raise TypeError from exc_val
```

Важно: цепочка исключений (`__cause__`, `__context__`) сохраняется.

11. **Менеджеры для тестирования**:

В тестовых фреймворках контекстные менеджеры используются для:

- `pytest.raises(Exception)` — проверяет исключение
- `unittest.mock.patch` — мокает объекты
- Создания временных файлов/директорий (`tempfile.TemporaryDirectory`)
- Изменения окружения (`os.environ`)

Особенность: эти менеджеры часто используют `yield` в `@contextmanager`, что добавляет один фрейм в стек вызовов.

12. **Рекурсивные и композитные менеджеры**:

Можно создавать менеджеры, которые комбинируют другие:

```python
class CompositeManager:

    def __init__(self, *managers):
        self.managers = managers


def __enter__(self):
    return tuple(m.__enter__() for m in self.managers)


def __exit__(self, *args):
    for m in reversed(self.managers):
        m.__exit__(*args)
```

Проблема: если `__exit__` одного менеджера вернет `True`, исключение будет подавлено для остальных. Нужно аккуратно
обрабатывать это.

13. **Ресурсы и сборка мусора**:

Контекстные менеджеры не защищают от утечек ресурсов при циклических ссысках:

```python
class Resource:

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.cleanup()


res = Resource()
with res:
    res.self_ref = res  # Циклическая ссылка!
```

`__exit__` вызовется, но объект не удалится сборщиком мусора. Решение: weakref или явный вызов `del`.

14. **Сигналы и прерывания**:

В `__exit__` может прийти сигнал `KeyboardInterrupt`. Рекомендация:

- Выполнять минимально необходимую очистку
- Не блокировать надолго
- Возвращать `False`, чтобы прерывание всплыло

- [Содержание](#содержание)

---

# *Генераторы и итераторы*

Генератор — это частный случай итератора, который создаётся функцией с ключевым словом `yield` и генерирует значения по
мере необходимости, сохраняя своё состояние между вызовами. Ключевое отличие от обычных итераторов — ленивое
вычисление (lazy evaluation), которое позволяет эффективно работать с большими или бесконечными последовательностями, не
загружая их в память целиком. Генераторы также поддерживают двустороннюю коммуникацию через методы `.send()` и
`.throw()`.

## **Junior Level**

Итератор — это объект, который позволяет последовательно перебирать элементы коллекции. У каждого итератора есть метод
`__next__()`, который возвращает следующий элемент или вызывает исключение `StopIteration`, когда элементы закончились.
Итерируемый объект — это любой объект, который можно перебирать в цикле `for`. У него есть метод `__iter__()`, который
возвращает итератор.

Генератор — это особый вид итератора, который создается с помощью функции, содержащей ключевое слово `yield`. Когда
функция встречает `yield`, она приостанавливает выполнение и возвращает значение. При следующем вызове `next()`
выполнение продолжается с того же места. Генераторы позволяют создавать последовательности "лениво" (lazy evaluation),
не загружая все элементы в память сразу.

## **Middle Level**

1. **Протокол итератора**:

- `__iter__()` — должен возвращать объект-итератор
- `__next__()` — возвращает следующий элемент или вызывает `StopIteration`
- Встроенная функция `iter()` вызывает `__iter__()`, а `next()` — `__next__()`

2. **Протокол итерируемого объекта**:

- Любой объект с методом `__iter__()` является итерируемым
- Объекты с методом `__getitem__()` также считаются итерируемыми (для обратной совместимости)
- Цикл `for` сначала вызывает `iter()`, затем в цикле вызывает `next()`

3. **Генераторные функции**:

- Функция с `yield` при вызове возвращает объект-генератор
- Генератор сохраняет свое состояние (локальные переменные, точку выполнения)
- Можно использовать `return` в генераторе — при достижении `return` выбрасывается `StopIteration` с переданным
  значением как `value`

4. **Генераторные выражения**:

- `(x**2 for x in range(10))` — создает генератор
- Аналогично list comprehension, но не создает список сразу
- Может быть бесконечным: `(i for i in itertools.count())`

5. **Методы генераторов**:

- `.send(value)` — отправляет значение в генератор (становится результатом `yield`)
- `.throw(exc)` — бросает исключение в точке `yield`
- `.close()` — останавливает генератор (вызывает `GeneratorExit` внутри)

6. **`yield from`** (Python 3.3+):

- Позволяет делегировать выполнение другому генератору
- Упрощает композицию генераторов
- Также передает `send()`, `throw()` и `close()` вложенному генератору

## **Senior Level (CPython, байткод и системные структуры)**

1. **Структура PyGenObject**:

В `Include/genobject.h`:

```c
typedef struct {
PyObject_HEAD
PyFrameObject *gi_frame;      // Фрейм выполнения
PyObject *gi_code;            // Код объекта
PyObject *gi_weakreflist;     // Список weakref
int gi_running;               // Флаг выполнения (0/1)
PyObject *gi_name;            // Имя генератора
PyObject *gi_qualname;        // Полное имя
_PyErr_StackItem *gi_exc_state; // Информация об исключениях
} PyGenObject;
```

2. **Байткод генераторной функции**:

Функция с `yield` компилируется с флагом `CO_GENERATOR` в `code.co_flags`. Байткод:

```
def gen():
yield 1
yield 2

# Байткод:
0 LOAD_CONST               1 (1)
2 YIELD_VALUE
4 POP_TOP
6 LOAD_CONST               2 (2)
8 YIELD_VALUE
10 POP_TOP
12 LOAD_CONST               0 (None)
14 RETURN_VALUE
```

Инструкция `YIELD_VALUE`:

- Сохраняет текущее состояние фрейма
- Возвращает значение вызывающей стороне
- Устанавливает `f->f_lasti` (последнюю выполненную инструкцию)
- При следующем `next()` выполнение продолжается со следующей инструкции

3. **Фреймы и состояние генератора**:

Генератор хранит свой фрейм (`gi_frame`), который содержит:

- `f_code` — код объекта
- `f_lasti` — индекс последней выполненной инструкции
- `f_locals` — локальные переменные
- `f_valuestack` — стек значений
- `f_blockstack` — стек блоков (for/with/try)

Размер фрейма зависит от количества локальных переменных и глубины стека. Фреймы обычно размещаются в куче, но Python
3.11+ оптимизирует это.

4. **PyFrameObject и оптимизации CPython 3.11**:

Python 3.11 представляет **фреймы в куче (heap-allocated frames)**:

- Фреймы создаются только при необходимости (для генераторов, трассировки, исключений)
- Уменьшает использование памяти и улучшает производительность
- Генераторы всегда создают фреймы, так как они нужны для сохранения состояния

5. **`yield from` на уровне байткода**:

```python
def delegator():
    yield from subgenerator()
```

Байткод:

```
0 LOAD_GLOBAL              0 (subgenerator)
2 CALL_FUNCTION            0
4 GET_YIELD_FROM_ITER
6 LOAD_CONST               0 (None)
8 YIELD_FROM
```

`GET_YIELD_FROM_ITER` проверяет, является ли объект генератором. `YIELD_FROM`:

- Делегирует выполнение субгенератору
- Передает значения из `send()` и исключения из `throw()`
- Обрабатывает `StopIteration` и извлекает `value`

6. **Асинхронные генераторы (Python 3.6+)**:

Имеют отдельную структуру `PyAsyncGenObject`:

```c
typedef struct {
PyGenObject ag_gen;
PyObject *ag_finalizer;    // Финалайзер
PyObject *ag_hooks_inited; // Флаги хуков
} PyAsyncGenObject;
```

Асинхронные методы: `__anext__()`, `__aiter__()`. Используют `yield` в `async def`.

7. **Сборка мусора и циклические ссылки**:

Генераторы могут создавать циклические ссычки:

```python
def gen():
    while True:
        yield


g = gen()
g.gi_frame.f_locals['self_ref'] = g  # Циклическая ссылка
```

GC отслеживает генераторы через `Py_TPFLAGS_HAVE_GC`. При сборке мусора:

- Вызывается `__del__` у объектов в `gi_frame`
- Если генератор не закрыт, вызывается `gen_close()`

8. **Системные вызовы и производительность**:

Создание генератора:

- `PyGen_New` — выделяет память для `PyGenObject`
- `PyFrame_New` — создает фрейм (если его нет)
- Копирует ссылки на код и глобальные переменные

Вызов `next()`:

- `gen_iternext()` в `Objects/genobject.c`
- Восстанавливает состояние фрейма
- Выполняет байткод до следующего `YIELD_VALUE`

9. **Интроспекция и отладка**:

- `inspect.getgeneratorstate(g)` — возвращает `GEN_CREATED`, `GEN_RUNNING`, `GEN_SUSPENDED`, `GEN_CLOSED`
- `inspect.getgeneratorlocals(g)` — локальные переменные генератора
- `g.gi_frame` — прямой доступ к фрейму
- `g.gi_code` — код объекта
- `g.gi_running` — выполняется ли генератор (0/1)

10. **Корoutines и генераторы**:

До Python 3.5 корутины реализовывались через генераторы. Теперь:

- `async def` создает корутину (тип `PyCoroObject`)
- `await` использует механизм, аналогичный `yield from`
- Генераторы и корутины имеют разные типы, но схожую структуру

11. **Производительность и микрооптимизации**:

- Генераторы эффективны по памяти: O(1) вместо O(n) для списков
- Но вызов `next()` имеет накладные расходы: проверка состояния, восстановление фрейма
- Для больших последовательностей выгоднее итерировать по списку
- Python 3.11 ускоряет генераторы на 10-15% через оптимизацию фреймов

12. **Бесконечные генераторы и ресурсы**:

Бесконечные генераторы могут висеть в памяти:

```python
def infinite():
    while True:
        yield 1


g = infinite()
# Генератор живет, пока есть ссылка
```

Решение: использовать `contextlib.closing()` или явно вызывать `close()`.

13. **Тестирование генераторов для AQA**:

Особенности тестирования:

- Проверка состояния генератора после каждого `yield`
- Тестирование `send()`, `throw()`, `close()`
- Проверка обработки `StopIteration`
- Тестирование вложенных генераторов (`yield from`)
- Проверка утечек памяти при прерывании генераторов
- Тестирование асинхронных генераторов

Инструменты:

- `pytest` для тестирования
- `tracemalloc` для отслеживания утечек
- `gc` для проверки циклических ссылок
- `unittest.mock` для мокирования

14. **Байткод для `yield` и исключения**:

При `g.throw(ValueError)`:

```
RAISE_VARARGS           # В фрейме генератора
# Если исключение не поймано:
CLEANUP_THROW           # Очистка
# Возвращает исключение вызывающей стороне
```

Генератор может ловить исключения обычным `try/except`.

15. **Генераторы в контексте тестирования**:

- Создание тестовых данных "на лету" для экономии памяти
- Имитация потоков данных (сетевых, файловых)
- Тестирование обработки больших последовательностей
- Создание моков с состоянием (stateful mocks)

- [Содержание](#содержание)

---

# *Декораторы и замыкания*

Декораторы — это функции, которые принимают другую функцию и возвращают новую, модифицируя её поведение. Они реализуются
через замыкания — функции, которые запоминают окружение, в котором были созданы, сохраняя доступ к переменным внешней
функции. Это позволяет легко добавлять сквозную функциональность (логирование, кэширование, валидацию) без изменения
исходного кода.

## **Junior Level**

Декораторы в Python — это специальные функции или классы, которые позволяют модифицировать поведение других функций или
методов без изменения их исходного кода. Синтаксически декоратор выглядит как `@deкоратор` перед определением функции.

Представьте, что у вас есть функция, и вы хотите добавить к ней логирование времени выполнения. Вместо того чтобы
изменять код функции, вы можете создать декоратор, который обернет оригинальную функцию и добавит нужную
функциональность.

Замыкания — это функции, которые запоминают окружение, в котором они были созданы. Они сохраняют доступ к переменным из
внешней области видимости, даже когда эта внешняя функция уже завершила работу. Замыкания часто используются для
создания фабрик функций и являются основой для декораторов.

## **Middle Level**

1. **Простой декоратор**: Функция, принимающая другую функцию и возвращающая новую функцию-обертку. Когда Python видит
   `@decorator`, он делает примерно следующее: `func = decorator(func)`.

2. **Декораторы с аргументами**: Требуют дополнительного уровня вложенности. Функция с аргументами возвращает декоратор,
   который уже принимает и оборачивает целевую функцию.

3. **Цепочка декораторов**: Декораторы применяются снизу вверх (ближайший к функции применяется первым).
   `@a @b @c def f(): ...` превращается в `f = a(b(c(f)))`.

4. **`functools.wraps`**: Критически важный декоратор, который сохраняет метаданные оригинальной функции (имя,
   документацию, сигнатуру). Без него отладочная информация и интроспекция ломаются.

5. **Классы-декораторы**: Класс с методом `__call__` может быть декоратором. Также можно декорировать методы класса
   через `__get__`.

6. **Замыкания и LEGB правило**: Python ищет переменные в следующем порядке: Local → Enclosing (замыкания) → Global →
   Built-in. Замыкания захватывают переменные из Enclosing scope.

7. **`nonlocal`**: Ключевое слово для изменения переменных из замыкания. Без него попытка изменить захваченную
   переменную создаст новую локальную переменную.

8. **Применение в тестировании**:

- Декораторы для параметризации тестов (`@pytest.mark.parametrize`)
- Декораторы для мокирования (`@unittest.mock.patch`)
- Декораторы для setup/teardown (`@pytest.fixture`)

## **Senior Level**

1. **Байткод декоратора**:

Рассмотрим:

```python
@deco
def func():
    pass
```

Байткод:

```
LOAD_NAME                0 (deco)
LOAD_CONST               0 (<code object func>)
MAKE_FUNCTION            0
CALL_FUNCTION            1        # Вызов deco с func
STORE_NAME               1 (func)  # Сохранение результата
```

Ключевой момент: **декоратор выполняется во время загрузки модуля**, а не при вызове функции.

2. **Замыкания на уровне байткода**:

```python
def outer(x):
    def inner():
        return x

    return inner
```

Байткод для `inner`:

```
# Для замыкания переменной x
LOAD_DEREF               0 (x)    # Загрузка из cell
RETURN_VALUE
```

`LOAD_DEREF` и `STORE_DEREF` работают с **cell objects**, а не напрямую со стеком.

3. **Cell objects и `__closure__`**:

В CPython замыкания реализованы через объекты ячеек:

```c
typedef struct {
PyObject_HEAD
PyObject *ob_ref;    // Ссылка на значение в ячейке
} PyCellObject;
```

Когда `outer` создает `inner`:

- Создается `PyCellObject` для `x`
- Он помещается в `inner.__closure__` как кортеж ячеек
- При вызове `inner` байткод `LOAD_DEREF` обращается к ячейке

4. **Фреймы и пространства имен**:

Каждый вызов функции создает новый фрейм. Замыкания хранят ссылки на ячейки из фрейма внешней функции. Даже после
завершения `outer`, фрейм может остаться в памяти, если на него есть ссылки через замыкания.

В Python 3.11+ фреймы выделяются в куче только при необходимости (для замыканий, генераторов, исключений).

5. **Дескрипторы и методы**:

Декораторы методов класса — это особый случай. Когда декоратор применяется к методу:

```python
class A:

    @deco
    def method(self):
        pass
```

`deco` получает **обычную функцию**, а не связанный метод. При доступе `a.method` срабатывает протокол дескриптора,
создавая bound method.

Если декоратор не сохраняет `__get__` (через `functools.wraps`), методы ломаются.

6. **`functools.wraps` под микроскопом**:

Реализация в CPython (`_functoolsmodule.c`):

```c
static PyObject *
_functools_wraps(PyObject *wrapper, PyObject *wrapped)
{
// Копирует __module__, __name__, __qualname__, __doc__
// Копирует __annotations__, если есть
// Устанавливает __wrapped__ = wrapped
// Для Python 3.10+: копирует __type_params__
}
```

Важно: `wraps` не копирует `__code__`, поэтому сигнатура функции не сохраняется. Для этого нужен `inspect.signature`
или `functools.update_wrapper` с `assigned`, включающим `__signature__`.

7. **Проблема аргументов в замыканиях**:

Классическая ошибка:

```python
funcs = []
for i in range(3):
    def inner():
        return i
funcs.append(inner)
# Все функции вернут 2!
```

На байткод-уровне: все `inner` ссылаются на **одну и ту же ячейку** `i`, которая меняется в цикле.

Решение: `lambda i=i: i` создает новую ячейку для каждой итерации или использует `functools.partial`.

8. **Производительность декораторов**:

- Каждый декоратор добавляет один уровень вызова
- Множественные декораторы могут создать глубокую цепочку вызовов
- Python 3.11+ оптимизирует вызовы через **специализацию байткода**, но только для часто вызываемых функций

Замеры: декорированная функция вызывается на 10-20% медленнее из-за дополнительного фрейма.

9. **Декораторы в метаклассах и `__init_subclass__`**:

Декораторы класса выполняются **после** создания класса. Но можно влиять на декораторы через метаклассы:

```python
class Meta(type):

    def __new__(cls, name, bases, dct):
        # Здесь можно модифицировать декораторы в dct
        return super().__new__(cls, name, bases, dct)
```

`__init_subclass__` (Python 3.6+) вызывается после применения декораторов класса.

10. **`@property`, `@classmethod`, `@staticmethod` — системные декораторы**:

Реализованы на C в `Objects/descrobject.c`:

- `property` создает объект-дескриптор с `__get__`, `__set__`, `__delete__`
- `classmethod` и `staticmethod` меняют поведение `__get__`:
- `classmethod.__get__` возвращает bound method с классом как первым аргументом
- `staticmethod.__get__` возвращает оригинальную функцию без привязки

11. **Замыкания и сборка мусора**:

Замыкания создают циклические ссылки:

```python
def outer():
    x = []


def inner():
    x.append(1)  # inner ссылается на x

    return inner


f = outer()  # f.__closure__[0] ссылается на ячейку, ячейка ссылается на x
```

GC должен разорвать этот цикл. В CPython ячейки имеют `tp_traverse` и `tp_clear` методы.

12. **`inspect` и интроспекция замыканий**:

```python
import inspect


def outer(x):
    def inner():
        return x

    return inner


f = outer(42)
print(inspect.getclosurevars(f))
# ClosureVars(nonlocals={'x': 42}, globals={}, builtins={}, unbound=set())
```

Можно даже изменить значение в замыкании:

```python
f.__closure__[0].cell_contents = 100
```

13. **Декораторы для AQA: тестирование**:

- **Тестирование декораторов**: проверка, что декоратор сохраняет сигнатуру, документацию, правильно обрабатывает
  исключения
- **Мокирование декораторов**: `unittest.mock.patch.object` для временной замены декоратора
- **Производительность**: замер времени выполнения декорированных функций
- **Память**: проверка утечек через замыкания

14. **Особые виды декораторов**:

- **Ленивые декораторы**: откладывают вычисление до первого вызова
- **Декораторы с кэшированием**: `@lru_cache`, `@cached_property`
- **Декораторы валидации**: проверка аргументов/результатов
- **Асинхронные декораторы**: для `async def` функций

15. **Байткод для `nonlocal`**:

```python
def outer():
    x = 0


def inner():
    nonlocal x

    x += 1
    return inner
```

Байткод для `inner`:

```
LOAD_DEREF               0 (x)
LOAD_CONST               1 (1)
INPLACE_ADD
STORE_DEREF              0 (x)
```

Без `nonlocal` была бы ошибка: `x` рассматривался бы как локальная переменная.

- [Содержание](#содержание)

---

# **GIL (Global Interpreter Lock)**

GIL (Global Interpreter Lock) — это механизм в реализации CPython, который позволяет одновременно выполнять только один
поток Python-кода, даже на многоядерных системах. Он существует для защиты внутренних структур данных интерпретатора (
например, счётчика ссылок) от состояний гонки в многопоточном окружении. Это означает, что для CPU-интенсивных задач
многопоточность не даст прироста производительности, но для I/O-операций она остаётся полезной, так как поток
освобождает GIL во время ожидания.

## **Junior Level**

GIL (Global Interpreter Lock) — это механизм в реализации Python CPython, который позволяет выполнять только одному
потоку Python за раз, даже на многоядерных процессорах. Представьте, что у вас есть кухня с несколькими поварами (
потоками), но только один нож (интерпретатор Python). Хотя поваров много, они вынуждены ждать, пока нож освободится.

GIL необходим потому, что управление памятью в CPython не является потокобезопасным. В частности, счетчик ссылок (
reference counting), который используется для сборки мусора, требует защиты от одновременного доступа из нескольких
потоков. Вместо того чтобы делать каждую структуру данных потокобезопасной (что сильно замедлило бы однопоточное
выполнение), CPython использует одну глобальную блокировку на весь интерпретатор.

Это означает, что для CPU-интенсивных задач многопоточность в Python не даст прироста производительности на многоядерных
системах. Однако для I/O-интенсивных задач (сеть, файлы) многопоточность все равно полезна, потому что потоки
освобождают GIL во время ожидания ввода-вывода.

## **Middle Level**

1. **Техническая суть**: GIL — это мьютекс (в CPython реализованный через `PyThread_type_lock`), который защищает доступ
   к объектам Python и их счетчикам ссылок. Только поток, захвативший GIL, может выполнять байткод Python или вызывать
   C-API функции, которые работают с объектами Python.

2. **Переключение потоков**: Потоки не удерживают GIL навсегда. Существует механизм переключения, основанный на:

- **Интервалах переключения (check interval)**: По умолчанию каждые 100 тиков байткода (можно изменить через
  `sys.setswitchinterval()`). Тик — это обычно одна инструкция байткода, но не всегда.
- **I/O операциях**: При выполнении системных вызовов (чтение файла, сокета) поток освобождает GIL, позволяя другим
  потокам работать.
- **Явном освобождении**: В C-расширениях можно временно освободить GIL через `Py_BEGIN_ALLOW_THREADS` и
  `Py_END_ALLOW_THREADS`.

3. **Проблемы**:

- **Холодный старт (cold start)**: При создании нового потока требуется время на его инициализацию и захват GIL.
- **Голодание (starvation)**: Если один поток долго выполняет CPU-интенсивный код без I/O, другие потоки могут не
  получать GIL.
- **Нет true параллелизма**: Нельзя использовать несколько ядер CPU для выполнения кода Python одновременно.

4. **Обходные пути**:

- **Мультипроцессинг (`multiprocessing`)**: Запуск отдельных процессов, каждый со своим интерпретатором и GIL.
- **Асинхронное программирование (`asyncio`)**: Использование корутин в одном потоке.
- **C-расширения**: Вынос CPU-интенсивных задач в расширения, которые могут освобождать GIL.
- **Использование других реализаций Python**: Jython, IronPython (без GIL) или PyPy (с GIL, но с другими
  оптимизациями).

5. **Когда GIL не проблема**:

- I/O-интенсивные приложения (веб-серверы, клиенты)
- Смешанные нагрузки, где есть ожидание
- Использование внешних библиотек, которые освобождают GIL (например, NumPy, SciPy для тяжелых вычислений)

## **Senior Level (CPython, байткод и системные вызовы)**

1. **Реализация GIL в CPython**:

В `Python/ceval_gil.h` и `Python/ceval.c`:

```c
static PyThread_type_lock global_interpreter_lock = NULL;
```

GIL реализован через примитивы операционной системы:

- На Unix/Linux: `pthread_mutex_t` с `pthread_cond_t`
- На Windows: `CRITICAL_SECTION` с событием (`CreateEvent`)

В Python 3.9+ используется "новый GIL" (с 2009 года, но все еще актуальный), который учитывает системные часы для
более справедливого планирования.

2. **Механизм переключения (switch mechanism)**:

Каждый поток имеет структуру `_gil_runtime_state`:

```c
struct _gil_runtime_state {
unsigned long interval;  // Интервал переключения в микросекундах
PyThread_type_lock mutex;
PyThread_type_lock cond;
volatile unsigned long locked;
PyThreadState *current_thread;
// ...
};
```

Поток удерживает GIL до:

- Истечения интервала (`gil_interval`)
- Вызова `PyEval_ReleaseThread` (при I/O)
- Принудительного переключения сигналом

3. **Байткод и тики (ticks)**:

В виртуальной машине Python есть счетчик тиков:

```c
/* Python/ceval.c */
#define _PY_CHECK_INTERVAL 100

int _Py_CheckInterval = _PY_CHECK_INTERVAL;

// При выполнении байткода
if (--_Py_Ticker < 0) {
_Py_Ticker = _Py_CheckInterval;
if (interpreter_lock) {
// Проверяем, не нужно ли переключиться
PyThread_release_lock(interpreter_lock);
PyThread_acquire_lock(interpreter_lock, 1);
}
}
```

Тик уменьшается при выполнении многих инструкций байткода, но не всех. Некоторые "дорогие" инструкции (например,
`CALL_FUNCTION`) могут сбрасывать счетчик.

4. **`PyThreadState` и состояние потока**:

Каждый поток имеет свой `PyThreadState`, который хранит:

- Указатель на текущий фрейм выполнения
- Исключения, возникшие в потоке
- Словарь с контекстами (`contextvars`)
- Ссылку на следующее состояние потока в связном списке

GIL защищает доступ к глобальному связному списку всех `PyThreadState`.

5. **Сигналы и GIL**:

Обработка сигналов (например, Ctrl+C) требует особого внимания. Только главный поток (который создал GIL) может
обрабатывать сигналы. При получении сигнала:

- Устанавливается флаг `PyOS_InterruptOccurred()`
- Главный поток проверяет этот флаг при получении GIL
- Если флаг установлен, вызывается обработчик сигнала

6. **Подкапотная работа с I/O**:

Когда поток вызывает блокирующий системный вызов (например, `socket.recv()`):

```c
/* Модуль socket */
Py_BEGIN_ALLOW_THREADS
ret = recv(sock->fd, buf, len, flags);
Py_END_ALLOW_THREADS
```

Макросы `Py_BEGIN_ALLOW_THREADS` и `Py_END_ALLOW_THREADS`:

- Сохраняют состояние GIL
- Освобождают GIL
- Выполняют системный вызов
- Захватывают GIL обратно
- Восстанавливают состояние

7. **Проблема справедливости (fairness issue)**:

Старый GIL (до Python 3.2) страдал от "несчастливого" планирования: поток, освободивший GIL для I/O, мог сразу
захватить его снова, не давая другим потокам шанса. Новый GIL использует условные переменные и таймеры для более
справедливого распределения.

8. **Влияние на сборку мусора**:

Циклический сборщик мусора (GC) должен работать, когда другие потоки приостановлены. Перед началом сборки GC
захватывает GIL и приостанавливает все другие потоки через `PyThreadState_Swap(NULL)`.

Это может вызвать задержки (latency), так как все потоки останавливаются на время работы GC.

9. **Многопоточность vs Мультипроцессинг на уровне памяти**:

С процессами:

- Каждый процесс имеет свою копию данных
- Обмен данными через IPC (память, файлы, сокеты)
- Нагрузка на память и накладные расходы на сериализацию

С потоками:

- Общая память, но защищенная GIL
- Нет накладных расходов на копирование
- Но нет true параллелизма для Python-кода

10. **Асинхронность и GIL**:

Асинхронные фреймворки (`asyncio`) обходят GIL, используя один поток и кооперативную многозадачность. Однако:

- Блокирующий вызов в корутине блокирует весь поток
- Для асинхронных системных вызовов (через `select`, `epoll`, `kqueue`) GIL освобождается на время ожидания

11. **Тестирование многопоточного кода с GIL**:

Для AQA важно понимать, как тестировать в условиях GIL:

- **Race conditions**: Несмотря на GIL, race conditions возможны, потому что поток может быть прерван между
  инструкциями байткода
- **Deadlocks**: GIL не защищает от deadlock. Если поток захватывает пользовательскую блокировку, а затем ждет GIL,
  может возникнуть deadlock
- **Тестирование производительности**: Измерять, как GIL влияет на масштабируемость

12. **Будущее GIL**:

Python 3.13+ планирует экспериментальное отключение GIL (PEP 703). Это потребует:

- Изменения счетчика ссылок на thread-safe реализацию (возможно, с использованием hazard pointers или RCU)
- Внесения изменений во все структуры данных CPython
- Возможного замедления однопоточного выполнения

13. **Инструменты для анализа**:

- `sys._current_frames()`: Показывает текущие фреймы во всех потоках
- `threading.enumerate()`: Список всех активных потоков
- Профилировщики: `cProfile` (но он тоже зависит от GIL)
- Внешние утилиты: `py-spy`, `perf` для анализа на уровне системы

14. **C-расширения и GIL**:

При написании C-расширений для AQA (например, для ускорения тестов):

- Можно освобождать GIL на время длительных вычислений
- Нужно быть осторожным с объектами Python: нельзя удалять или изменять объекты без GIL
- Можно использовать `PyGILState_Ensure()` и `PyGILState_Release()` для рекурсивного захвата

- [Содержание](#содержание)

---

# **Изменение списка во время итерации**

Изменять список во время итерации по нему опасно, так как это нарушает внутреннюю логику работы итератора и может
привести к непредсказуемому поведению, например, к пропуску элементов или ошибкам. Это происходит потому, что итератор
хранит текущий индекс, а при добавлении или удалении элементов индексы сдвигаются. Для безопасной модификации следует
итерировать по копии списка или использовать подход с созданием нового списка.

## **Junior Level**

Изменение списка во время итерации по нему — это опасная операция, которая может привести к непредсказуемому поведению,
ошибкам или даже падению программы. Когда вы перебираете элементы списка в цикле `for` и одновременно добавляете или
удаляете элементы из этого же списка, вы нарушаете внутреннюю логику работы итератора.

Представьте, что вы читаете книгу и кто-то одновременно вырывает из нее страницы или вклеивает новые. Вы можете
пропустить страницы или прочитать одну страницу дважды. То же самое происходит с итератором списка: он отслеживает
текущую позицию, но если список изменяется, эта позиция может стать невалидной.

На практике это приводит к таким проблемам:

- Пропуск элементов
- Обработка одного элемента дважды
- Бесконечные циклы
- `IndexError` или `RuntimeError`

## **Middle Level**

1. **Механизм итерации**: Когда вы создаете цикл `for item in my_list:`, Python вызывает `iter(my_list)`, который
   возвращает объект-итератор. Этот итератор хранит ссылку на список и текущий индекс. При каждой итерации он вызывает
   `__next__()`, который возвращает элемент по текущему индексу и увеличивает индекс.

2. **Что происходит при изменении**:

- **Удаление текущего или предыдущих элементов**: Итератор уже прошел эти индексы, но при удалении все последующие
  элементы сдвигаются. Это может привести к пропуску элемента, который сдвинулся на место удаленного.
- **Удаление последующих элементов**: Менее проблематично, но может вызвать `IndexError`, если итератор попытается
  обратиться к индексу за пределами нового размера списка.
- **Добавление элементов в конец**: Может привести к бесконечному циклу, если условие выхода зависит от размера списка.
- **Вставка элементов перед текущей позицией**: Итератор может обработать новый элемент, который был вставлен перед
  текущей позицией.

3. **Защита в Python**: Начиная с Python 3.7, при обнаружении изменения размера списка во время итерации бросается
   `RuntimeError`:

```
RuntimeError: dictionary changed size during iteration
```

Но для списков такой защиты **нет** (она есть только для словарей и множеств). Со списками Python ведет себя более
снисходительно, что делает ошибки более коварными.

4. **Безопасные альтернативы**:

- Итерация по копии списка: `for item in list(my_list):`
- Создание нового списка через list comprehension
- Использование `while` с ручным управлением индексом
- Сбор элементов для удаления в отдельный список и удаление после итерации

## **Senior Level (Байткод, память и грабли компилятора)**

1. **Байткод итерации по списку**:

Рассмотрим простой цикл:

```python
for item in my_list:
    process(item)
```

Байткод:

```
SETUP_LOOP               L3
LOAD_FAST                0 (my_list)
GET_ITER
L1: FOR_ITER              L2
STORE_FAST               1 (item)
LOAD_FAST                2 (process)
LOAD_FAST                1 (item)
CALL_FUNCTION            1
POP_TOP
JUMP_ABSOLUTE           L1
L2: POP_BLOCK
L3: ...
```

Ключевая инструкция — `FOR_ITER`. Она вызывает `listiter_next()` у итератора списка.

2. **Структура list iterator в CPython**:

В `Include/listobject.h`:

```c
typedef struct {
PyObject_HEAD
Py_ssize_t it_index;      // Текущий индекс
PyListObject *it_seq;     // Ссылка на исходный список
} listiterobject;
```

Итератор хранит:

- `it_index`: текущую позицию (от 0 до `ob_size-1`)
- `it_seq`: указатель на `PyListObject` исходного списка

3. **Функция `listiter_next()`** (в `Objects/listobject.c`):

```c
static PyObject *
listiter_next(listiterobject *it)
{
PyListObject *seq = it->it_seq;
PyObject *item;

if (seq == NULL)
   return NULL;

// Проверяем, не вышел ли индекс за границы
if (it->it_index < PyList_GET_SIZE(seq)) {
   item = PyList_GET_ITEM(seq, it->it_index);
   ++it->it_index;  // Увеличиваем индекс ДО возврата элемента
   Py_INCREF(item);
   return item;
}

// Итерация завершена
it->it_seq = NULL;
Py_DECREF(seq);
return NULL;
}
```

4. **Критическая проблема**: Индекс увеличивается (`++it->it_index`) **после** получения элемента, но **до** того как
   элемент будет обработан в теле цикла. Если в теле цикла удалить элемент из списка, структура списка изменится, но
   индекс уже указывает на следующую позицию.

5. **Пример опасного сценария**:

Список: `[A, B, C, D]`

- Итератор на элементе B (индекс=1)
- Удаляем B в теле цикла
- Список становится: `[A, C, D]`
- Индекс увеличивается до 2
- Следующая итерация вернет D, пропустив C!

6. **Динамический массив и сдвиги памяти**:

Список в CPython — это динамический массив `PyObject**`. При удалении элемента:

```c
// Objects/listobject.c: list_ass_slice()
memmove(&item[i+1], &item[i], (n - i) * sizeof(PyObject *));
```

`memmove` сдвигает все элементы после удаленного на одну позицию влево. Итератор продолжает работать со старым индексом,
который теперь указывает на другой элемент.

7. **Глубокий грабель: изменение через `__setitem__`**:

Даже если не менять размер, но заменить элемент:

```python
for i, item in enumerate(my_list):
    my_list[i] = transform(item)  # Может вызвать неожиданное поведение
```

Это безопаснее, но если `transform()` вызывает `__del__` у старого объекта, который имеет сайд-эффекты, могут быть
проблемы.

8. **Параллельная модификация из разных потоков**:

Если один поток итерирует список, а другой его модифицирует, возникает **data race**. Даже с GIL это может привести к
повреждению памяти, потому что операции не атомарны на уровне байткода.

9. **Коллекции с защитой от модификации**:

В отличие от списков, словари и множества с Python 3.7+ имеют детектор модификаций:

```c
// Objects/dictobject.c
if (d->ma_used != ep->me_used) {
PyErr_SetString(PyExc_RuntimeError,
               "dictionary changed size during iteration");
return NULL;
}
```

Список такой защиты не имеет, так как проверка на каждую итерацию замедлила бы самый частый случай.

10. **Оптимизации компилятора Python 3.11+**:

В Python 3.11 появилась специализация байткода для циклов. Цикл `for` может быть скомпилирован в более эффективную
форму, но это не меняет фундаментальной проблемы с итератором.

11. **Наиболее опасный паттерн для AQA**:

```python
# Тест, который иногда проходит, иногда нет
def test_flaky():
    items = []


for i in range(5):
    items.append(i)

removed = []
for item in items:
    if item % 2 == 0:
        items.remove(item)  # Удаление во время итерации!
        removed.append(item)

# Результат зависит от фазы луны
assert removed == [0, 2, 4]
```

Такой тест будет вести себя нестабильно (flaky test).

12. **Правильные паттерны для тестирования**:

- **Копирование**: `for item in list(original):`
- **Обратная итерация**: `for i in range(len(lst)-1, -1, -1):`
- **Filter-подход**: `new_list = [x for x in lst if condition(x)]`
- **While с ручным индексом**:

```python
i = 0
while i < len(lst):
    if condition(lst[i]):
        del lst[i]
    else:
        i += 1
```

13. **Отладка и диагностика**:

Для поиска таких ошибок в тестируемом коде:

- Использовать `sys.settrace()` для отслеживания модификаций списка
- Создать wrapper-класс для списка, который бросает исключение при модификации во время итерации
- Использовать статический анализ (mypy, pylint с соответствующими правилами)

14. **Особый случай: `enumerate()`**:

`enumerate()` создает итератор, который возвращает пары (индекс, элемент). Индекс отражает позицию **на момент создания
итератора**, а не текущее состояние списка.

```python
for i, item in enumerate(lst):
    del lst[i]  # Удалит не тот элемент!
```

После удаления индексы смещаются, но `enumerate` продолжает выдавать старые индексы.

- [Содержание](#содержание)

---

# **Области видимости**

Области видимости в Python определяют доступность переменных и следуют правилу LEGB (Local, Enclosing, Global,
Built-in). Для изменения переменных из внешних областей используются ключевые слова `global` (для глобальных) и
`nonlocal` (для вложенных). Каждая область видимости имеет своё пространство имён, которое создаётся при вызове функции
или загрузке модуля.

## **Junior Level**

Область видимости в Python определяет, где и как можно использовать переменные в программе. Это как разные комнаты в
доме: переменные, созданные в одной комнате, не всегда видны в другой.

Python имеет четыре основные области видимости (в порядке поиска):

1. **Локальная (Local)** — внутри текущей функции
2. **Охватывающая (Enclosing)** — во внешних функциях (при вложенности)
3. **Глобальная (Global)** — на уровне модуля (файла)
4. **Встроенная (Built-in)** — встроенные функции и типы Python

Это правило называется **LEGB** (Local, Enclosing, Global, Built-in). Когда вы обращаетесь к переменной, Python ищет ее
в этом порядке.

Ключевые слова `global` и `nonlocal` позволяют изменять переменные из внешних областей видимости. Без них присвоение
значения переменной внутри функции создает новую локальную переменную, даже если существует внешняя с тем же именем.

## **Middle Level**

1. **Пространства имен (Namespaces)**: Каждая область видимости связана с пространством имен — словарем, который
   связывает имена переменных с объектами. В Python есть:

- Локальное пространство имен (функции)
- Глобальное пространство имен (модули)
- Встроенное пространство имен (builtins)
- Пространство имен классов

2. **Время жизни пространств имен**:

- Локальное: создается при вызове функции, уничтожается при ее завершении (если нет замыканий)
- Глобальное: создается при загрузке модуля, существует пока модуль загружен
- Встроенное: создается при запуске интерпретатора, существует до его завершения

3. **`global` и `nonlocal`**:

- `global` объявляет, что переменная относится к глобальному пространству имен модуля
- `nonlocal` (появилось в Python 3) указывает, что переменная принадлежит ближайшей охватывающей области видимости (
  но не глобальной)

4. **Замыкания (Closures)**: Когда вложенная функция использует переменные из внешней функции, эти переменные "
   захватываются" и живут дольше, чем внешняя функция. Это возможно благодаря тому, что внутренняя функция хранит ссылку
   на пространство имен внешней функции.

5. **Comprehensions и области видимости**: Начиная с Python 3, генераторные выражения и comprehensions имеют свою
   собственную область видимости. Переменные, определенные внутри них, не "протекают" наружу.

6. **Классы и области видимости**: Тело класса создает временное пространство имен, которое становится `__dict__`
   класса. Методы класса имеют доступ к атрибутам класса через `self` или имя класса.

7. **Модули и `__main__`**: При запуске скрипта напрямую он получает специальное имя `__main__`. Импортированные модули
   имеют свои собственные глобальные пространства имен.

## **Senior Level**

1. **Фреймы (Frame objects) и области видимости**:

Каждый вызов функции в CPython создает объект фрейма (`PyFrameObject`):

```c
typedef struct _frame {
   PyObject_VAR_HEAD
   struct _frame *f_back;      // Ссылка на предыдущий фрейм (вызывающая функция)
   PyCodeObject *f_code;       // Код объекта (байткод)
   PyObject *f_builtins;       // Встроенное пространство имен
   PyObject *f_globals;        // Глобальное пространство имен
   PyObject *f_locals;         // Локальное пространство имен
   PyObject **f_valuestack;    // Стек значений
   PyObject **f_stacktop;      // Верхушка стека
   PyObject *f_trace;          // Функция трассировки
   // ...
} PyFrameObject;
```

Фреймы образуют стек вызовов. `f_back` создает цепочку, по которой можно подняться к вызывающим функциям.

2. **Байткод для доступа к переменным**:

Python компилирует доступ к переменным в разные инструкции:

- `LOAD_FAST` — доступ к локальной переменной (по индексу в массиве)
- `LOAD_GLOBAL` — доступ к глобальной или встроенной переменной
- `LOAD_DEREF` — доступ к переменной из замыкания (через cell object)
- `LOAD_NAME` — универсальный доступ (медленнее, используется в глобальной области)
- `STORE_*` аналогичные инструкции для присваивания

3. **Cell objects и free variables**:

Для реализации замыканий Python использует **cell objects**:

```c
typedef struct {
   PyObject_HEAD
   PyObject *ob_ref;    // Ссылка на значение в ячейке
} PyCellObject;
```

Когда внешняя функция определяет переменную, которая используется во вложенной функции, эта переменная помещается в
cell object. Вложенная функция хранит ссылки на эти ячейки в `__closure__` (кортеж cell objects).

Байткод `LOAD_DEREF` и `STORE_DEREF` работают с этими ячейками.

4. **Оптимизации CPython**:

- **Локальные переменные как массив**: Внутри фрейма локальные переменные хранятся в массиве `f_localsplus`, а не в
  словаре. Доступ по индексу (`LOAD_FAST`) быстрее, чем поиск по имени в словаре.
- **Глобальные переменные**: Кэширование в байткоде через `LOAD_GLOBAL` с кэшем (Python 3.11+).
- **Встроенные переменные**: Хранятся в отдельном модуле `builtins`, доступ через `f_builtins`.

5. **`locals()` и `globals()` под капотом**:

- `locals()` возвращает фактический словарь локальных переменных, но в функциях это может быть **копия**, а не живой
  словарь.
- `globals()` возвращает `f_globals` текущего фрейма (обычно `__dict__` модуля).
- В глобальной области видимости `locals() is globals()` возвращает `True`.

6. **Динамическое создание переменных**:

Инструкции `exec()` и `eval()` могут создавать переменные динамически:

```python
exec("x = 42", globals(), locals())
```

Это создает переменную в указанном пространстве имен. В CPython это вызывает `PyEval_EvalCode` с переданными
словарями.

7. **Проблема позднего связывания (late binding) в замыканиях**:

Классическая проблема:

```python
funcs = []
for i in range(3):
    def inner():
        return i


    funcs.append(inner)
# Все функции вернут 2!
```

Почему: все `inner` ссылаются на **одну и ту же ячейку** `i`, которая меняется в цикле. Значение берется в момент
вызова функции, а не создания.

8. **Области видимости в comprehensions и генераторах**:

Начиная с Python 3, comprehensions выполняются в отдельной области видимости:

```python
x = 10
result = [x for x in range(5)]  # x внутри comprehension не перезаписывает внешний x
```

Это достигается компиляцией comprehension в отдельную функцию.

9. **Классы и метаклассы**:

При выполнении тела класса создается временное пространство имен:

- Имена внутри класса становятся ключами в `__dict__` класса
- Декораторы методов применяются до создания класса
- Метаклассы (`__prepare__`) могут создать кастомное пространство имен (например, `collections.OrderedDict`)

10. **Модульная система и `sys.modules`**:

Каждый модуль имеет свой `__dict__` как глобальное пространство имен. При импорте:

- Проверяется `sys.modules` (кэш загруженных модулей)
- Создается новый объект модуля
- Его `__dict__` становится `f_globals` для кода модуля

11. **Отладка и интроспекция**:

- `inspect.currentframe()` — текущий фрейм
- `frame.f_locals`, `frame.f_globals` — пространства имен
- `frame.f_code.co_names` — имена, используемые в коде
- `frame.f_code.co_varnames` — локальные переменные

12. **Производительность**:

- Доступ к локальным переменным (`LOAD_FAST`) в 2-3 раза быстрее, чем к глобальным (`LOAD_GLOBAL`)
- Замыкания (`LOAD_DEREF`) немного медленнее локальных переменных
- Python 3.11+ кэширует глобальные и встроенные переменные в байткоде

13. **Особые случаи**:

- **Параметры функции**: Локальные переменные, инициализированные значениями аргументов
- `*args` и `**kwargs`: Также локальные переменные
- **Исключения**: Переменная исключения в `except` блоке — локальная для этого блока
- **Контекстные менеджеры**: Переменная в `with ... as var` — локальная для блока

14. **Тестирование и области видимости**:

Для AQA важно понимать области видимости при:

- **Мокировании**: `unittest.mock.patch` работает с пространствами имен
- **Изоляции тестов**: Каждый тест должен иметь чистое окружение
- **Тестировании замыканий**: Проверка захваченных значений
- **Динамическом коде**: Тестирование `exec()`/`eval()`

- [Содержание](#содержание)

---

# **Lambda-функции**

Lambda-функции в Python — это анонимные функции, определяемые с помощью ключевого слова `lambda` и предназначенные для
выполнения простых операций в одну строку. Они могут содержать только одно выражение, которое автоматически
возвращается, и часто используются в функциях высшего порядка, таких как `map()`, `filter()` и `sorted()`. Несмотря на
ограничения, лямбды поддерживают замыкания и аргументы по умолчанию.

## **Junior Level**

Lambda-функции в Python — это анонимные (безымянные) функции, которые могут содержать только одно выражение и возвращают
результат его вычисления. Они создаются с помощью ключевого слова `lambda` и часто используются для простых операций,
когда не хочется определять полноценную функцию через `def`.

Представьте, что вам нужно быстро выполнить простое преобразование данных, например, удвоить каждое число в списке.
Вместо того чтобы писать отдельную функцию, можно использовать lambda: `map(lambda x: x * 2, numbers)`. Lambda-функции
особенно полезны в сочетании с функциями высшего порядка: `sorted()`, `filter()`, `map()` и другими.

Основные ограничения lambda:

- Может содержать только одно выражение
- Не может содержать операторы (только выражения)
- Не может иметь документацию
- Обычно помещается в одну строку

## **Middle Level**

1. **Синтаксис и семантика**: `lambda arguments: expression`

- `arguments` — ноль или более аргументов через запятую
- `expression` — одно выражение, результат которого возвращается
- Неявный `return`: результат выражения автоматически возвращается

2. **Замыкания и захват переменных**: Lambda может захватывать переменные из окружающей области видимости (closure). Это
   делает их мощным инструментом для создания фабрик функций на лету.

3. **Сравнение с обычными функциями**:

- Lambda создает объект функции, но без имени (`__name__ == '<lambda>'`)
- Не может содержать аннотации типов (до Python 3.12, где появилась поддержка синтаксиса, но с ограничениями)
- Не может быть декорирована обычным синтаксисом декораторов
- Всегда возвращает один результат

4. **Использование с функциями высшего порядка**:

- `sorted(items, key=lambda x: x[1])` — сортировка по второму элементу
- `filter(lambda x: x > 0, values)` — фильтрация положительных чисел
- `map(lambda x, y: x + y, list1, list2)` — поэлементное сложение
- `functools.reduce(lambda acc, x: acc * x, numbers)` — аккумуляция

5. **Пространства имен и аргументы по умолчанию**:
   Lambda может иметь аргументы со значениями по умолчанию: `lambda x, y=10: x + y`
   Важно: значения по умолчанию вычисляются в момент **определения** lambda, а не вызова.

6. **Распространенная ошибка с замыканиями**:

```python
funcs = [lambda: i for i in range(3)]
# Все функции вернут 2!
```

Причина: все lambda захватывают переменную `i`, а не ее значение на момент создания. Значение берется в момент
вызова, когда цикл уже завершился.

## **Senior Level**

1. **Компиляция lambda в байткод**:

Lambda-функция компилируется почти так же, как обычная функция. Рассмотрим:

```python
f = lambda x: x * 2
```

Байткод для этого выражения:

```
LOAD_CONST               <code object <lambda> at 0x...>
LOAD_CONST               '<lambda>'
MAKE_FUNCTION            0
STORE_FAST               f
```

Ключевые моменты:

- Объект кода lambda создается на этапе **компиляции** и помещается в константы родительского кода
- `MAKE_FUNCTION` создает объект функции во время **выполнения**
- Имя функции всегда `'<lambda>'`

2. **Объект кода lambda**:

В CPython объект `PyCodeObject` для lambda почти идентичен объекту кода обычной функции:

```c
typedef struct {
   PyObject_HEAD
   // ...
   PyObject *co_code;        // Байткод
   PyObject *co_consts;      // Константы
   PyObject *co_names;       // Имена глобальных переменных
   PyObject *co_varnames;    // Имена локальных переменных
   PyObject *co_freevars;    // Свободные переменные (для замыканий)
   PyObject *co_cellvars;    // Ячейки (cell variables)
   // ...
} PyCodeObject;
```

Отличие от обычной функции: `co_flags` не имеет флага `CO_NEWLOCALS`? На самом деле, lambda тоже создает локальное
пространство имен.

3. **Байткод внутри lambda**:

Для `lambda x: x * 2`:

```
0 LOAD_FAST                0 (x)
2 LOAD_CONST               1 (2)
4 BINARY_MULTIPLY
6 RETURN_VALUE
```

Важно: lambda использует `LOAD_FAST` для доступа к аргументам, как и обычная функция. Это быстрее, чем `LOAD_NAME`
или `LOAD_GLOBAL`.

4. **Замыкания в lambda**:

Когда lambda захватывает переменные из внешней области видимости:

```python
def outer(n):
    return lambda x: x * n
```

Байткод для lambda:

```
0 LOAD_DEREF               0 (n)   # Загружает из замыкания
2 LOAD_FAST                0 (x)
4 BINARY_MULTIPLY
6 RETURN_VALUE
```

Lambda хранит ссылки на cell objects в `__closure__`, как и обычная функция.

5. **Создание функции: `MAKE_FUNCTION`**:

Инструкция `MAKE_FUNCTION` (код 0x84 в Python 3.10+):

- Берет объект кода из стека
- Создает объект функции `PyFunctionObject`
- Устанавливает `__name__`, `__doc__`, `__qualname__`
- Для lambda `__name__` устанавливается в `'<lambda>'`

В CPython `PyFunction_New`:

```c
PyObject *
PyFunction_New(PyObject *code, PyObject *globals)
{
   PyFunctionObject *op = PyObject_GC_New(PyFunctionObject, 
                                          &PyFunction_Type);
   op->func_code = code;
   op->func_globals = globals;
   op->func_defaults = NULL;
   op->func_kwdefaults = NULL;
   op->func_closure = NULL;
   op->func_doc = NULL;
   op->func_name = ((PyCodeObject *)code)->co_name;
   // ...
}
```

6. **Проблема с аргументами по умолчанию**:

Рассмотрим:

```python
funcs = [lambda x, i=i: x + i for i in range(3)]
```

Здесь `i=i` создает **локальную** переменную в лямбде, которая инициализируется значением `i` на момент создания. В
байткоде:

```
# Для каждой lambda в цикле
LOAD_FAST                0 (i)    # Текущее значение i
BUILD_TUPLE              1
LOAD_CONST               <code>
LOAD_CONST               '<lambda>'
MAKE_FUNCTION            1        # 1 = есть defaults
```

Аргументы по умолчанию хранятся в `func_defaults` как кортеж.

7. **Производительность lambda vs def**:

- **Создание**: Lambda создается во время выполнения, как и функция через `def`. Нет разницы в производительности
  создания.
- **Вызов**: Вызов lambda идентичен вызову обычной функции. Одинаковое количество байткод-инструкций.
- **Чтение**: Lambda не имеет `__code__.co_firstlineno`, что может затруднить отладку.
- **Мемоизация**: Сложнее мемоизировать lambda, так как у них нет имени для кэширования.

8. **Специфика Python 3.12+**:

В Python 3.12 появилась ограниченная поддержка аннотаций типов в lambda:

```python
f: Callable[[int], int] = lambda x: x * 2
```

Но сама lambda не может содержать аннотации в своем синтаксисе. Также улучшены сообщения об ошибках в lambda.

9. **Интроспекция lambda**:

Lambda имеет те же атрибуты, что и обычная функция, но с особенностями:

- `lambda x: x.__name__` вернет `'<lambda>'`
- `lambda x: x.__code__` содержит байткод
- `lambda x: x.__defaults__` для значений по умолчанию
- `lambda x: x.__closure__` для замыканий

Однако `inspect.getsource()` не работает для lambda, так как у них нет исходного кода в отдельной строке.

10. **Рекурсия с lambda**:

Lambda не может рекурсивно вызывать себя по имени, так как у нее нет имени. Но можно использовать Y-комбинатор:

```python
Y = lambda f: (lambda x: f(lambda y: x(x)(y)))(lambda x: f(lambda y: x(x)(y)))
factorial = Y(lambda f: lambda n: 1 if n == 0 else n * f(n - 1))
```

Практического применения мало, но показывает выразительную мощность lambda.

11. **Оптимизации компилятора**:

В Python 3.11+ компилятор пытается оптимизировать вызовы lambda:

- Инлайн-кэширование для часто вызываемых lambda
- Специализация байткода для lambda с простыми операциями
- Однако lambda все равно создает полный объект функции и фрейм при вызове

12. **Ограничения и обходные пути**:

Lambda не может содержать:

- Присваивания (`=`)
- Операторы (`if`, `for`, `while`)
- Исключения (`try/except`)

Обходные пути:

- Условное выражение: `lambda x: True if x > 0 else False`
- Логические операторы: `lambda x: x > 0 and "positive" or "non-positive"`
- Вызовы других функций: `lambda x: some_func(x)`

13. **Тестирование кода с lambda**:

Для AQA важно уметь тестировать lambda-функции:

- **Мокирование**: Lambda может быть заменена mock-объектом
- **Проверка поведения**: Можно проверить, что lambda вызывается с правильными аргументами
- **Тестирование замыканий**: Проверка захваченных значений
- **Производительность**: Замер времени выполнения lambda в цикле

14. **Диагностика проблем**:

При ошибках в lambda трассировка стека показывает `<lambda>` вместо имени функции. Для отладки можно:

- Временно заменить lambda на именованную функцию
- Использовать `inspect.currentframe()` внутри lambda для получения контекста
- Добавить отладочный вывод через обертку

- [Содержание](#содержание)

---

# *Comprehensions и генераторные выражения*

Comprehensions — это лаконичный синтаксис для создания коллекций (списков, множеств, словарей) на основе итераций с
возможностью фильтрации. Генераторные выражения, оформленные в круглые скобки, создают итераторы, которые вычисляют
элементы «лениво», экономя память. Обе конструкции выполняются в собственной области видимости и обычно работают быстрее
эквивалентных циклов за счёт внутренних оптимизаций CPython.

## **Junior Level**

Comprehensions и генераторные выражения — это лаконичные конструкции в Python для создания коллекций на основе итераций.
Представьте, что вам нужно преобразовать один список в другой: вместо написания многострочного цикла `for` можно
использовать одну строку с comprehension.

List comprehension (списковое включение) создаёт новый список: `[x*2 for x in range(5)]` даст `[0, 2, 4, 6, 8]`. Set
comprehension создаёт множество, dict comprehension — словарь. Генераторное выражение выглядит похоже, но в круглых
скобках: `(x*2 for x in range(5))`. Разница в том, что генераторное выражение не создаёт коллекцию сразу, а возвращает
итератор, который вычисляет элементы «лениво», по одному, что экономит память при работе с большими объёмами данных.

Их удобно использовать для фильтрации (добавив `if`) и для преобразования элементов. Это делает код чище и часто
быстрее, чем аналогичные циклы.

## **Middle Level**

1. **Типы comprehensions**:

- List comprehension: `[выражение for элемент in итератор]`
- Set comprehension: `{выражение for элемент in итератор}`
- Dict comprehension: `{ключ: значение for элемент in итератор}`
- Generator expression: `(выражение for элемент in итератор)`

2. **Синтаксические возможности**:

- Могут содержать несколько циклов `for`: `[x+y for x in list1 for y in list2]`
- Поддерживают условия фильтрации `if`: `[x for x in range(10) if x % 2 == 0]`
- Условия могут быть вложенными и комбинированными

3. **Область видимости**: Начиная с Python 3, comprehensions и генераторные выражения выполняются в собственной области
   видимости. Переменные, созданные внутри (например, переменная цикла), не «просачиваются» наружу, что предотвращает
   случайные перезаписи.

4. **Производительность**: List comprehensions обычно выполняются быстрее эквивалентных циклов `for`, потому что они
   оптимизированы на уровне байткода и выполняют операции добавления элементов напрямую, минуя вызовы методов.
   Генераторные выражения экономят память, но имеют небольшие накладные расходы на каждый вызов `next()`.

5. **Ленивые вычисления**: Генераторные выражения вычисляют элементы только когда они запрашиваются (например, в цикле
   `for` или при вызове `next()`). Это позволяет работать с бесконечными последовательностями и потоками данных.

6. **Отличия от функций-генераторов**: Генераторные выражения — это синтаксический сахар для создания анонимных
   генераторов. Они не могут содержать сложную логику с несколькими `yield` или `return`, в отличие от
   функций-генераторов.

## **Senior Level**

1. **Компиляция comprehensions**:

В CPython каждое comprehension компилируется во **временную функцию**. Например, list comprehension
`[x*2 for x in iterable]` преобразуется в скрытую функцию, которая создаёт список, выполняет цикл и возвращает
результат.

Байткод для вызова list comprehension:

```
LOAD_CONST               <code object <listcomp> at 0x...>
LOAD_CONST               '<listcomp>'
MAKE_FUNCTION            0
LOAD_GLOBAL              iterable
GET_ITER
CALL_FUNCTION            1
```

Объект кода `<listcomp>` содержит байткод, реализующий логику comprehension.

2. **Внутренняя функция comprehension**:

Байткод внутренней функции для `[x*2 for x in range(5)]`:

```
0 BUILD_LIST               0       # Создаём пустой список
2 LOAD_FAST                0 (.0)  # Загружаем итератор
4 FOR_ITER                16 (to 22)
6 STORE_FAST               1 (x)   # Сохраняем текущий элемент в x
8 LOAD_FAST                1 (x)
10 LOAD_CONST               0 (2)
12 BINARY_MULTIPLY
14 LIST_APPEND              2       # Добавляем результат в список
16 JUMP_ABSOLUTE            4
18 POP_BLOCK
20 RETURN_VALUE
```

Ключевая инструкция `LIST_APPEND` (код 0x69) добавляет элемент напрямую во внутренний массив списка, что быстрее, чем
вызов метода `append()`.

3. **Генераторные выражения**:

Генераторное выражение `(x*2 for x in range(5))` компилируется в код, создающий объект генератора. Внутренняя функция
`<genexpr>` использует `YIELD_VALUE`:

```
0 LOAD_FAST                0 (.0)
2 FOR_ITER                12 (to 16)
4 STORE_FAST               1 (x)
6 LOAD_FAST                1 (x)
8 LOAD_CONST               0 (2)

10 BINARY_MULTIPLY
12 YIELD_VALUE # Возвращаем значение и приостанавливаемся
14 JUMP_ABSOLUTE 2
16 LOAD_CONST 1 (None)
18 RETURN_VALUE

```

При каждом вызове `next()` выполнение возобновляется с точки после последнего `YIELD_VALUE`.

4. **Специализированные инструкции байткода**:

Для каждого типа comprehension есть своя инструкция добавления элемента:

- `LIST_APPEND` (0x69) для list comprehensions
- `SET_ADD` (0x67) для set comprehensions
- `MAP_ADD` (0x68) для dict comprehensions

Эти инструкции работают напрямую с внутренними структурами данных (`PyListObject`, `PySetObject`, `PyDictObject`), минуя
вызовы методов Python.

5. **Оптимизации CPython**:

- **Предварительное выделение памяти**: Если итерируемый объект имеет метод `__len__`, CPython использует его для
  предварительного выделения памяти под список, уменьшая количество перераспределений.
- **Инлайн-кэширование**: В Python 3.11+ добавлен адаптивный байткод, который кэширует частые операции внутри
  comprehensions.
- **Оптимизация стека**: Внутренние переменные comprehension хранятся в массиве `f_localsplus` фрейма, а не в словаре,
  что ускоряет доступ.

6. **Области видимости и cell variables**:

Если comprehension использует переменные из внешней области видимости, они захватываются через cell objects. Например, в
`[x*y for x in range(3)]`, если `y` — внешняя переменная, она загружается инструкцией `LOAD_DEREF` из cell object.

7. **Производительность: comprehension vs цикл**:

List comprehension быстрее цикла с `append()` по трём причинам:

1. Операция добавления выполняется нативным кодом в `LIST_APPEND`
2. Нет накладных расходов на поиск и вызов метода `append`
3. Весь цикл выполняется в одной области видимости без переключения контекста

8. **Генераторные выражения и память**:

Генераторные выражения создают объект типа `PyGenObject`, который содержит фрейм выполнения. Этот фрейм сохраняет
состояние между вызовами `next()`. Память освобождается только после завершения итерации или явного вызова `close()`.

9. **Вложенные comprehensions**:

Вложенное comprehension `[[i*j for j in range(3)] for i in range(4)]` компилируется в две внутренние функции. Внешняя
функция создаёт внешний список, внутренняя — внутренние списки. Это может создавать дополнительные накладные расходы
из-за создания множества временных объектов.

10. **Словарные comprehension**:

Dict comprehension `{x: x**2 for x in range(5)}` использует инструкцию `MAP_ADD`, которая добавляет пару ключ-значение
напрямую в хеш-таблицу словаря. Это быстрее, чем вызов `dict.__setitem__`.

11. **Особенности множественных comprehension**:

Set comprehension использует `SET_ADD`, который проверяет уникальность элемента через хеш-таблицу множества. При
дублировании элементов выполняется лишняя работа, но результат остаётся корректным.

12. **Потенциальные проблемы**:

- **Утечка памяти в генераторах**: Если генераторное выражение не итерируется до конца (например, из-за `break`), его
  фрейм может остаться в памяти до сборки мусора.
- **Неправильный порядок вложенных циклов**: В comprehension вида `[x+y for x in A for y in B]` сначала фиксируется `x`,
  затем итерируется `y`. Это противоположно вложенным циклам `for x in A: for y in B:`.
- **Оценка условий**: Условия `if` оцениваются для каждого элемента, что может быть дорого, если условие сложное.

13. **Тестирование для AQA**:

При тестировании кода с comprehensions и генераторными выражениями важно:

- Проверять корректность выходных данных для всех типов входных данных (пустые, большие, с дубликатами)
- Измерять потребление памяти при использовании генераторных выражений
- Тестировать производительность на больших наборах данных
- Проверять обработку исключений внутри выражений
- Убеждаться, что генераторные выражения не используются повторно (они одноразовые)
- Проверять корректность работы с замыканиями

- [Содержание](#содержание)

---

# *copy() и deepcopy()*

copy() создает поверхностную копию объекта, копируя сам контейнер, но не вложенные объекты, в то время как deepcopy()
рекурсивно создает полную копию всей структуры. Поверхностная копия подходит, когда вложенные объекты неизменяемы или
нет необходимости их копировать, тогда как глубокая копия необходима для полной изоляции, особенно для конфигураций или
тестовых данных.

## **Junior Level**

`copy()` и `deepcopy()` — это функции из модуля `copy`, которые создают копии объектов Python.

`copy()` делает поверхностную (shallow) копию объекта — создает новый контейнер, но элементы внутри остаются теми же
самыми объектами. Для списка `copy()` создаст новый список, но элементы этого списка будут ссылаться на те же объекты,
что и в оригинале.

`deepcopy()` делает глубокую (deep) копию — рекурсивно копирует все объекты внутри контейнера, создавая полностью
независимую копию всей иерархии объектов. Это гарантирует, что изменения в копии не затронут оригинал, и наоборот.

Пример: если у вас есть список списков, `copy()` скопирует только внешний список, а внутренние списки останутся общими.
`deepcopy()` скопирует и внешний список, и все внутренние списки.

## **Middle Level**

1. **Поверхностное копирование (`copy()`)**:

- Создает новый объект того же типа
- Копирует ссылки на вложенные объекты, а не сами объекты
- Для изменяемых объектов (списков, словарей, множеств) изменение вложенных объектов в копии отразится на оригинале
- Для неизменяемых объектов (числа, строки, кортежи) разницы между поверхностной и глубокой копией нет
- Использует метод `__copy__()` объекта, если он определен

2. **Глубокое копирование (`deepcopy()`)**:

- Рекурсивно обходит всю структуру объекта
- Создает новые экземпляры для всех вложенных объектов
- Обрабатывает циклические ссылки через словарь `memo` для избежания бесконечной рекурсии
- Использует метод `__deepcopy__()` объекта, если он определен
- Может быть очень медленным для больших структур

3. **Когда использовать**:

- `copy()`: когда объекты неизменяемы или вы уверены, что не будете изменять вложенные объекты
- `deepcopy()`: когда нужна полная изоляция, особенно для конфигураций, тестовых данных, фикстур

4. **Особые случаи**:

- Файловые объекты, потоки, сокеты не могут быть скопированы нормально
- Некоторые объекты (модули, классы, функции) возвращаются как есть
- Дескрипторы и свойства требуют специальной обработки

## **Senior Level (CPython, память и рекурсивные алгоритмы)**

1. **Реализация в CPython**:

Модуль `copy` реализован на Python (`Lib/copy.py`), но использует низкоуровневые механизмы CPython.

```python
# Упрощенная структура
def copy(x):
    cls = type(x)
    copier = _copy_dispatch.get(cls)
    if copier:
        return copier(x)
    # Попытка использовать __copy__ метод
    # или создание через конструктор
```

`_copy_dispatch` — это словарь, который сопоставляет типы с функциями-копировщиками.

2. **Алгоритм `deepcopy()`**:

Основная функция `_deepcopy_atomic()`:

```python
def _deepcopy_atomic(x, memo):
    # Атомарные объекты (неизменяемые) возвращаются как есть
    return x


def _deepcopy_list(x, memo, recursive=0):
    id_x = id(x)
    if id_x in memo:
        return memo[id_x]

    memo[id_x] = y = []
    for item in x:
        y.append(deepcopy(item, memo))
    return y
```

Ключевые элементы:

- `memo` словарь: `{id(original): copy}` для обработки циклических ссылок
- Рекурсивный обход через `deepcopy(item, memo)`
- Специальные обработчики для разных типов в `_deepcopy_dispatch`

3. **Обработка циклических ссылок**:

Для структуры с циклической ссылкой:

```python
a = []
b = [a]
a.append(b)
```

`deepcopy()` работает так:

1. Видит список `a`, создает пустую копию `a'`, сохраняет `{id(a): a'}` в `memo`
2. Начинает копировать элементы `a` — видит `b`
3. Создает пустую копию `b'`, сохраняет `{id(b): b'}` в `memo`
4. Начинает копировать элементы `b` — видит ссылку на `a`
5. Находит `a'` в `memo`, вставляет ссылку на `a'` в `b'`
6. Возвращается к `a'`, вставляет `b'` в него

4. **Байткод и производительность**:

Каждый вызов `deepcopy()`:

- Создает новый фрейм для рекурсивных вызовов
- Выполняет множество проверок типа
- Использует `id()` для каждого объекта (хэш от адреса памяти)
- Для больших структур может вызвать `RecursionError`

Оптимизация: `deepcopy()` использует `_deepcopy_dispatch` для быстрого вызова специализированных функций.

5. **Специальные методы `__copy__()` и `__deepcopy__()`**:

Классы могут определить эти методы для кастомного поведения:

```python
class MyClass:
    def __init__(self, data):
        self.data = data
        self._cache = None

    def __deepcopy__(self, memo):
        # Копируем только data, cache не копируем
        new_obj = self.__class__(deepcopy(self.data, memo))
        memo[id(self)] = new_obj
        return new_obj
```

`__deepcopy__()` получает `memo` словарь и должен использовать его для рекурсивных вызовов.

6. **Проблемы с объектами CPython**:

Некоторые объекты не могут быть скопированы:

- Модули: возвращаются как есть
- Классы: копируются только ссылки
- Файловые дескрипторы: могут быть "скопированы", но это опасно
- Сокеты, потоки: обычно вызывают исключение
- Weak references: требуют специальной обработки

7. **Производительность `copy()`**:

Для встроенных типов `copy()` часто реализована на C:

- `list.copy()`: `PyList_Copy()` в `Objects/listobject.c`
- `dict.copy()`: `PyDict_Copy()` в `Objects/dictobject.c`
- `set.copy()`: `PySet_Copy()` в `Objects/setobject.c`

Эти функции работают за O(n) времени и создают новые структуры данных.

8. **Memoryview и копирование**:

Для объектов с буферами памяти (memoryview, array.array, numpy arrays):

- `copy()` создает новый объект с тем же буфером (shallow)
- `deepcopy()` обычно делает то же самое, что и `copy()`
- Для настоящего копирования данных нужны специальные методы (`copy()`, `copy.deepcopy()` с кастомной логикой)

9. **Копирование дескрипторов и property**:

При копировании класса или экземпляра с дескрипторами:

- Дескрипторы не копируются (они принадлежат классу)
- Property объекты копируются как есть
- Важно: копирование не создает новые функции-геттеры/сеттеры

10. **Тестирование и отладка**:

Для AQA важно тестировать копирование:

- Проверять идентичность вложенных объектов после `copy()`
- Проверять независимость после `deepcopy()`
- Тестировать циклические ссылки
- Проверять кастомные `__copy__`/`__deepcopy__` методы
- Измерять производительность для больших структур

11. **Альтернативные методы копирования**:

- `pickle.loads(pickle.dumps(obj))`: создает глубокую копию через сериализацию
- `json.loads(json.dumps(obj))`: для JSON-сериализуемых объектов
- Специализированные методы: `list()`, `dict()`, `set()` для простых случаев

12. **Копирование в многопоточной среде**:

При копировании объектов, к которым обращаются несколько потоков:

- `copy()` может захватить неконсистентное состояние
- `deepcopy()` рекурсивно захватывает объекты, что может привести к deadlock
- Нужно использовать блокировки или immutable структуры

13. **Сборка мусора и копирование**:

При копировании больших объектов:

- `copy()` увеличивает счетчики ссылок у вложенных объектов
- `deepcopy()` создает полностью новые объекты, увеличивая нагрузку на GC
- `memo` словарь в `deepcopy()` предотвращает дублирование, но может удерживать ссылки

- [Содержание](#содержание)

---

# **Асинхронность**

Асинхронность в Python — это модель программирования, которая позволяет выполнять множество операций ввода-вывода (I/O)
в одном потоке без блокировок, используя ключевые слова `async` и `await`. Она основана на корутинах, которые
приостанавливают и возобновляют выполнение, и цикле событий (event loop), который управляет их выполнением. Это особенно
эффективно для сетевых запросов, работы с базами данных и других операций, где важно избегать простоев в ожидании
ответа.

## **Junior Level**

Асинхронность в Python — это модель программирования, позволяющая выполнять множество операций без блокировки потока
выполнения. Ключевые слова `async` и `await` используются для определения и работы с асинхронным кодом.

`async def` определяет асинхронную функцию (корутину), которая может приостанавливать своё выполнение, не блокируя
другие операции. `await` используется для ожидания результата другой асинхронной операции, освобождая управление, пока
эта операция выполняется. Это особенно полезно для операций ввода-вывода (I/O), таких как сетевые запросы или чтение
файлов, где программа может тратить время в ожидании ответа.

Для запуска асинхронного кода нужен цикл событий (event loop), который управляет выполнением корутин. Модуль `asyncio`
предоставляет инфраструктуру для написания асинхронного кода, включая планирование задач и синхронизацию.

## **Middle Level**

1. **Корутины (coroutines)**: Функции, определённые с `async def`. При вызове они возвращают объект корутины, который
   необходимо запустить в цикле событий. Корутины могут быть приостановлены (`await`) и возобновлены.

2. **Event Loop**: Центральный механизм, управляющий выполнением асинхронных задач. Он запускает корутины, обрабатывает
   системные события (например, готовность сокета), и переключается между задачами при их приостановке.

3. **Задачи (Tasks)**: Обёртки вокруг корутин, которые планируются в цикле событий. `asyncio.create_task()` запускает
   корутину как конкурентную задачу.

4. **Awaitable объекты**: Любой объект, который можно использовать с `await`. Это включает корутины, задачи и объекты
   `Future` (представляющие результат асинхронной операции).

5. **Принципы работы**: При встрече `await` корутина приостанавливается, управление возвращается в цикл событий, который
   запускает другие задачи. Когда ожидаемая операция завершается, цикл событий возобновляет корутину.

6. **Синхронизация**: `asyncio` предоставляет примитивы для синхронизации задач: `Lock`, `Semaphore`, `Event`,
   `Condition`.

7. **Асинхронные контекстные менеджеры и итераторы**: `async with` и `async for` для работы с асинхронными ресурсами и
   итерируемыми объектами.

## **Senior Level**

**1. Реализация корутин в CPython**

Корутины строятся на генераторах. Асинхронная функция компилируется с флагом `CO_COROUTINE` (0x80) в `code.co_flags`.
Объект корутины имеет тип `PyCoroObject` (наследуется от `PyGenObject`):

```c
typedef struct {
    PyGenObject gen;
    PyObject *cr_origin;  // Для отладки
} PyCoroObject;
```

При вызове `async def` функции:

- Байткод `MAKE_FUNCTION` создаёт объект функции с флагом `CO_COROUTINE`
- Вызов функции возвращает объект корутины (не выполняя код)
- Для запуска корутины вызывается `coro.send(None)` или `await`

**2. Механизм `await` на уровне байткода**

Байткод для `await` в Python 3.9+ использует инструкции `GET_AWAITABLE` и `YIELD_FROM`:

```
async def foo():
    await bar()

# Байткод foo():
0 LOAD_GLOBAL              0 (bar)
2 CALL_FUNCTION            0
4 GET_AWAITABLE
6 LOAD_CONST               0 (None)
8 YIELD_FROM
10 POP_TOP
12 LOAD_CONST               0 (None)
14 RETURN_VALUE
```

`GET_AWAITABLE` проверяет, является ли объект awaitable (имеет метод `__await__`). `YIELD_FROM` приостанавливает
корутину и передаёт управление.

**3. Цикл событий (Event Loop)**

Реализация цикла событий в `asyncio` использует селекторы (select, epoll, kqueue) для мониторинга файловых дескрипторов.
Основные компоненты:

- **Ready Queue**: Очередь готовых к выполнению задач (корутин)
- **Scheduled Queue**: Очередь отложенных задач (через `asyncio.sleep`)
- **Selector**: Мониторит сокеты и файловые дескрипторы

Работа цикла:

```python
while True:
    # 1. Выполнить готовые задачи
    while ready_queue:
        task = ready_queue.popleft()
        task._step()  # Продвинуть выполнение

    # 2. Ожидать событий ввода-вывода
    timeout = calculate_timeout()
    events = selector.select(timeout)
    for fd, event in events:
        callback = fd_to_callback[fd]
        callback()

    # 3. Проверить отложенные задачи
    current_time = time()
    while scheduled_queue and scheduled_queue[0].when <= current_time:
        task = scheduled_queue.pop(0)
        ready_queue.append(task)
```

**4. Задачи (Tasks) и Future**

`Task` наследуется от `Future`. `Future` представляет результат асинхронной операции. Структура `PyTaskObject` включает:

- `_coro`: ссылка на корутину
- `_state`: состояние (PENDING, CANCELLED, FINISHED)
- `_result`: результат или исключение
- `_callbacks`: список колбэков при завершении

Когда задача завершается, она устанавливает результат во `Future` и запускает прикреплённые колбэки.

**5. Асинхронные генераторы**

Асинхронные генераторы (`async def` с `yield`) используют отдельный тип `PyAsyncGenObject`. Они поддерживают:

- `__anext__()` для асинхронной итерации
- `asend()`, `athrow()`, `aclose()` аналогично обычным генераторам
- Финализацию через `async_gen_finalizer()`

**6. Протокол `__await__`**

Любой объект может стать awaitable, реализовав `__await__()`, который должен возвращать итератор. Цикл событий будет
итерировать по нему до завершения.

**7. Отладка и интроспекция**

- `asyncio.current_task()`: текущая задача
- `asyncio.all_tasks()`: все активные задачи
- `task.get_stack()`: стек вызовов корутины
- `asyncio.get_event_loop_policy()`: политика цикла событий

**8. Производительность и оптимизации**

- **Быстрый путь для локального event loop**: `asyncio.get_running_loop()` кэширует ссылку на текущий цикл
- **Инлайн-кэширование в Python 3.11**: байткод `ASYNC_GEN_WRAP` оптимизирует асинхронные генераторы
- **Протокол буферизации**: асинхронные итераторы могут реализовать `__aiter__`, возвращающий асинхронный итератор, и
  `__anext__` для получения элементов

**9. GIL и асинхронность**

Асинхронный код выполняется в одном потоке, поэтому GIL не препятствует параллелизму. Однако:

- Блокирующие вызовы блокируют весь цикл событий
- Для CPU-интенсивных задач используется `run_in_executor()` (пул потоков/процессов)
- Освобождение GIL в `await` происходит автоматически при вызове системных функций

**10. Системные вызовы и обратные вызовы**

При асинхронном I/O:

- Системный вызов (например, `socket.recv`) регистрируется в селекторе
- Цикл событий добавляет колбэк, который будет вызван при готовности данных
- Колбэк возобновляет корутину через `task.set_result()`

**11. Обработка исключений**

Исключения в корутинах пробрасываются через механизм `throw()`:

- Если корутина не обрабатывает исключение, оно устанавливается в задачу
- `await` пробрасывает исключение вызывающей корутине
- `asyncio.gather()` собирает исключения из нескольких задач

**12. Тестирование асинхронного кода**

Для AQA критически важно:

- **Изоляция тестов**: Каждый тест должен запускаться в новом цикле событий
- **Мокирование**: Подмена асинхронных функций через `unittest.mock.AsyncMock`
- **Таймауты**: Контроль времени выполнения тестов
- **Обработка исключений**: Проверка асинхронных исключений через `pytest.raises()`
- **Нагрузочное тестирование**: Проверка поведения при множестве одновременных корутин

**13. Расширенные паттерны**

- **Шардинг циклов событий**: Несколько циклов в разных потоках
- **Custom Event Loop**: Реализация своего цикла событий для специализированных нужд
- **Протоколы транспорта**: Низкоуровневая работа с сокетами через `asyncio.Protocol`

- [Содержание](#содержание)

---

# Многопоточность

Многопоточность в Python позволяет выполнять несколько потоков в одном процессе, но из-за Global Interpreter Lock (GIL)
только один поток может выполнять байткод Python в любой момент времени. Это делает её эффективной для операций
ввода-вывода (I/O), где потоки освобождают GIL во время ожидания, но непригодной для параллелизации CPU-интенсивных
задач. Для работы с потоками используется модуль `threading`, предоставляющий примитивы синхронизации, такие как Lock и
Semaphore.

## **Junior Level**

Многопоточность в Python — это возможность выполнять несколько потоков (threads) в рамках одного процесса. Каждый поток
представляет собой последовательность инструкций, которая может выполняться параллельно с другими потоками. Это похоже
на несколько сотрудников, работающих над разными задачами в одном офисе, где они могут использовать общие ресурсы (
память, файлы).

Основная цель многопоточности — улучшить отзывчивость приложений и эффективно использовать время простоя, например, при
ожидании ввода-вывода (I/O). Однако в Python из-за Global Interpreter Lock (GIL) потоки не выполняются по-настоящему
параллельно при работе с Python-кодом — в один момент времени только один поток выполняет байткод Python.

Для работы с потоками используется модуль `threading`. Потоки создаются через класс `Thread`, а для синхронизации
используются примитивы вроде `Lock`, `RLock`, `Semaphore`, `Event`, `Condition`.

## **Middle Level**

1. **GIL (Global Interpreter Lock)**: Это мьютекс, который защищает доступ к объектам Python, предотвращая одновременное
   выполнение байткода Python несколькими потоками. GIL необходим потому, что менеджер памяти CPython не является
   потокобезопасным. Это означает, что для CPU-интенсивных задач многопоточность не даёт прироста производительности, но
   для I/O-операций (сеть, дисковые операции) она эффективна, так как во время ожидания I/O GIL может быть отпущен.

2. **Состояния потока**: Поток может находиться в состояниях: создан (new), готов (runnable), запущен (running),
   заблокирован (blocked) и завершён (terminated). Переходы между состояниями управляются планировщиком операционной
   системы.

3. **Примитивы синхронизации**:
    - **Lock**: базовый мьютекс для взаимного исключения
    - **RLock**: реентерабельный замок, который может быть захвачен несколько раз одним потоком
    - **Semaphore**: счётчик для ограничения доступа к ресурсу
    - **Event**: флаг для уведомления между потоками
    - **Condition**: более сложный механизм для ожидания условий

4. **Демонические потоки**: Потоки, помеченные как `daemon`, завершаются вместе с основным потоком. Обычные (
   не-демонические) потоки продолжают выполнение до завершения даже после завершения основного потока.

5. **Локальное хранилище потока**: `threading.local()` позволяет создавать переменные, уникальные для каждого потока.

6. **Пул потоков**: `concurrent.futures.ThreadPoolExecutor` предоставляет высокоуровневый интерфейс для пула потоков,
   упрощая параллельное выполнение задач.

7. **Взаимодействие с GIL**: При выполнении I/O-операций или вызове функций, отпускающих GIL (например, некоторые
   функции из `numpy` или `zlib`), другие потоки могут получить доступ к интерпретатору.

## **Senior Level**

**1. Реализация потоков в CPython**

Потоки в CPython реализованы через нативные потоки операционной системы (pthreads в Unix/Linux, Win32 threads в
Windows). Структура `PyThreadState` представляет состояние интерпретатора для каждого потока:

```c
typedef struct _ts {
    struct _ts *prev;
    struct _ts *next;
    PyInterpreterState *interp;
    
    PyFrameObject *frame;  // текущий фрейм выполнения
    int recursion_depth;
    long thread_id;  // идентификатор потока ОС
    
    // Стек исключений
    PyObject *exc_type;
    PyObject *exc_value;
    PyObject *exc_traceback;
    
    // Словарь для threading.local()
    PyObject *dict;
    
    // Счётчик ссылок на GIL
    int gilstate_counter;
} PyThreadState;
```

**2. GIL: детальная реализация**

GIL реализован через глобальную переменную `_PyRuntimeState.gil` и мьютекс + condition variable для синхронизации. В
Python 3.9+ используется усовершенствованный алгоритм с таймаутами:

```c
typedef struct {
    PyThread_type_lock lock;
    unsigned long locked;
    PyCOND_T cond;
    PyThreadState *last_holder;
    unsigned long switch_number;
} _PyGilState;
```

**3. Механизм переключения GIL**

Переключение GIL происходит по:

- **Числу выполненных инструкций**: Счётчик `_PyRuntimeState.gilstate.ticks` уменьшается при каждой инструкции. Когда он
  достигает 0, текущий поток отпускает GIL.
- **Таймауту ввода-вывода**: При системных вызовах, блокирующих поток, GIL отпускается.
- **Принудительному переключению**: Через `PyEval_ReleaseThread()`.

Алгоритм переключения (упрощённо):

1. Текущий поток устанавливает флаг `gil_drop_request`
2. Другие потоки опрашивают этот флаг
3. Когда поток замечает флаг, он пытается захватить GIL
4. После успешного захвата он уведомляет предыдущий поток

**4. Байткод и GIL**

При выполнении байткода интерпретатор периодически проверяет необходимость отпустить GIL. В байткоде нет явных
инструкций для GIL, но функция `_PyEval_EvalFrameDefault()` содержит проверку:

```c
if (_Py_atomic_load_relaxed(&_PyRuntimeState.gilstate.ticks) <= 0) {
    _PyEval_ReleaseThread(tstate);
}
```

**5. Deadlock detection и отладка**

В CPython нет автоматического обнаружения deadlock-ов, но есть механизмы отладки:

- `sys.setswitchinterval()`: установка интервала переключения GIL
- `threading.get_ident()`: получение идентификатора потока ОС
- `threading.enumerate()`: список всех активных потоков
- `faulthandler`: для дампов стека при deadlock

**6. Memory management и потоки**

Менеджер памяти CPython (pymalloc) имеет отдельные пулы для каждого потока (arena), чтобы минимизировать конфликты.
Каждый поток имеет свой кэш свободных блоков памяти, что уменьшает необходимость в глобальной синхронизации при
выделении памяти.

**7. Состояние интерпретатора и суб-интерпретаторы**

`PyInterpreterState` содержит глобальное состояние для группы потоков. В Python 3.12+ усиливается поддержка
суб-интерпретаторов, где каждый имеет свой собственный GIL, позволяя настоящий параллелизм.

**8. Взаимодействие с async/await и asyncio**

При использовании asyncio в многопоточном контексте:

- Цикл событий работает в основном потоке
- `asyncio.run_coroutine_threadsafe()` позволяет запускать корутины из других потоков
- Для передачи данных между потоками и циклом событий используется `concurrent.futures.Future`

**9. Производительность и оптимизации**

- **Явное освобождение GIL**: Расширения C могут отпускать GIL через макросы `Py_BEGIN_ALLOW_THREADS` /
  `Py_END_ALLOW_THREADS`
- **Атомарные операции**: `_Py_atomic_*` функции для атомарного доступа к переменным
- **Thread-local storage (TLS)**: Используется `pthread_key_t` для хранения `PyThreadState*`

**10. Планировщик потоков ОС vs Python**

CPython делегирует планирование потоков операционной системе. Однако GIL добавляет свой уровень планирования: даже если
ОС переключила контекст на другой поток Python, он не сможет выполнять байткод, пока не получит GIL.

**11. Обработка сигналов и потоки**

Сигналы Unix обрабатываются только в главном потоке. Вспомогательные потоки блокируют сигналы через `pthread_sigmask()`.
Это предотвращает прерывание выполнения байткода в случайный момент.

**12. Для AQA: тестирование многопоточного кода**

Критически важные аспекты тестирования:

- **Детерминизм**: Многопоточные тесты должны быть воспроизводимы. Использование `random.seed()` и фиксированных
  интервалов.
- **Гонки данных (data races)**: Инструменты вроде `threading.gettrace()` для отслеживания переключений потоков.
- **Deadlock detection**: Принудительное создание дампов стека через сигналы или таймауты.
- **Нагрузочное тестирование**: Создание сценариев с большим количеством потоков и проверка утечек ресурсов.
- **Изоляция тестов**: Каждый тест должен запускаться в свежем процессе, чтобы избежать влияния глобального состояния.
- **Мокирование**: Подмена `threading.Thread` или примитивов синхронизации для контроля выполнения.
- **Проверка отложенных состояний (flaky tests)**: Автоматический повтор тестов с разными интервалами переключения GIL
  через `sys.setswitchinterval()`.

**13. Будущие изменения (Python 3.12+)**

- **Без-GIL режим**: Экспериментальная возможность запуска интерпретатора без GIL (флаг `--disable-gil`)
- **Per-interpreter GIL**: Разные GIL для разных суб-интерпретаторов
- **Улучшенная поддержка атомарных операций**: Новые API для безопасной работы с разделяемыми данными

- [Содержание](#содержание)

---

# Мультипроцессинг

Мультипроцессинг в Python использует несколько процессов, каждый со своим интерпретатором и памятью, что позволяет
обойти ограничения GIL и использовать все ядра процессора для CPU-интенсивных задач. Процессы изолированы, поэтому обмен
данными требует межпроцессного взаимодействия (IPC), но обеспечивает настоящий параллелизм.

## **Junior Level**

Мультипроцессинг в Python — это подход к параллельным вычислениям, использующий несколько процессов вместо потоков.
Каждый процесс работает в своём собственном пространстве памяти, имеет отдельный экземпляр интерпретатора Python и
собственный GIL. Это позволяет обойти ограничения GIL и использовать все доступные ядра процессора для выполнения
CPU-интенсивных задач.

Процессы полностью изолированы друг от друга, что обеспечивает лучшую стабильность (падение одного процесса не
затрагивает другие), но усложняет обмен данными. Для создания и управления процессами используется модуль
`multiprocessing`, который предоставляет высокоуровневый API, похожий на `threading`, но для процессов.

## **Middle Level**

1. **Архитектура процессов**: Каждый процесс — это отдельный экземпляр Python-интерпретатора с собственным адресным
   пространством, кучей (heap) и стеком. Процессы создаются через системные вызовы `fork()` (Unix) или
   `CreateProcess()` (Windows).

2. **Межпроцессное взаимодействие (IPC)**:
    - **Очереди (Queue)**: Потокобезопасные FIFO-очереди, реализованные через каналы (pipes) и блокировки.
    - **Каналы (Pipe)**: Двунаправленные каналы связи между двумя процессами.
    - **Разделяемая память (Shared Memory)**: `multiprocessing.Value` и `multiprocessing.Array` для создания переменных
      в общей памяти.
    - **Менеджеры (Managers)**: Высокоуровневый API для создания разделяемых объектов (словари, списки) через
      прокси-объекты.

3. **Способы создания процессов**:
    - **Fork** (Unix): Быстрое копирование памяти родительского процесса. Проблема: копируются все дескрипторы файлов и
      блокировки.
    - **Spawn** (по умолчанию с Python 3.8+): Запуск нового интерпретатора Python с импортом модуля и выполнением
      целевой функции. Безопаснее, но медленнее.
    - **Forkserver**: Компромиссный вариант — серверный процесс, от которого форкаются все остальные.

4. **Синхронизация**: Те же примитивы, что и в threading (Lock, Semaphore, Event, Condition), но работающие через общую
   память или семафоры ОС.

5. **Пулы процессов (Process Pool)**: `multiprocessing.Pool` управляет фиксированным числом рабочих процессов,
   предоставляет методы `map()`, `apply()`, `apply_async()` для распределения задач.

6. **Наследование данных**: При создании процесса через fork дочерний процесс получает копию всей памяти родителя. При
   spawn — только необходимые для выполнения функции данные.

7. **Производительность**: Процессы эффективны для CPU-задач благодаря отсутствию GIL, но имеют высокие накладные
   расходы на создание и IPC.

## **Senior Level**

**1. Реализация процессов в CPython**

Модуль `multiprocessing` написан на Python и C, использует низкоуровневые API ОС. На Unix системах используется
`os.fork()`:

```c
// Упрощённая реализация fork в CPython
pid_t fork(void) {
    pid_t pid = fork_syscall();  // Системный вызов fork()
    if (pid == 0) {
        // Дочерний процесс
        _PyRuntimeState_Reinitialize();  // Переинициализация runtime
        reset_all_locks();  // Сброс всех Python-блокировок
    }
    return pid;
}
```

**2. Структура Process объекта**

`Process` объект в CPython содержит:

- `_popen`: Объект `Popen`, управляющий дочерним процессом
- `_target`: Вызываемая функция
- `_args`, `_kwargs`: Аргументы функции
- `_pid`: Идентификатор процесса
- `_sentinel`: Файловый дескриптор для отслеживания завершения

**3. Межпроцессное взаимодействие на низком уровне**

**Очереди (Queue)**:

- Используют `Pipe` (неименованные каналы) для передачи данных
- Данные сериализуются через `pickle` с оптимизациями для больших объектов
- Синхронизация через семафоры POSIX или именованные семафоры System V

**Разделяемая память**:

- На Unix: `mmap()` с флагом `MAP_SHARED`
- На Windows: `CreateFileMapping()` + `MapViewOfFile()`
- В Python: `multiprocessing.shared_memory.SharedMemory` (Python 3.8+)

```c
// Создание разделяемой памяти в Unix
void* create_shared_memory(size_t size) {
    int fd = shm_open("/python_shm_XXXXXX", O_CREAT | O_RDWR, 0666);
    ftruncate(fd, size);
    return mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
}
```

**4. Сериализация и десериализация**

Все данные для IPC проходят через `pickle`. Важные оптимизации:

- **Протокол 5 (Python 3.8+)**: Поддержка out-of-band данных для больших объектов
- **Reduce/Reconstruct**: Кастомные классы могут реализовать `__reduce__` для контроля сериализации
- **Копирование при записи (Copy-on-Write)**: При fork память не копируется физически до модификации

**5. Пул процессов: внутренняя работа**

`Pool` использует паттерн "мастер-воркер":

1. Мастер создает N рабочих процессов
2. Создаются две очереди: task_queue и result_queue
3. Мастер отправляет задачи через `SimpleQueue` (pipe + lock)
4. Воркеры выполняют задачи и возвращают результаты
5. Для load balancing используется предварительное извлечение задач

**6. Синхронизация на уровне ядра**

Примитивы синхронизации реализованы через:

- **Семафоры POSIX**: `sem_open()`, `sem_wait()`, `sem_post()`
- **Мьютексы в разделяемой памяти**: Для `Lock` и `RLock`
- **Condition variables**: Через мьютексы и condition variables POSIX

**7. Проблема fork и GIL**

При использовании `fork()` в многопоточном приложении:

- Дочерний процесс получает копию только вызывающего потока
- Другие потоки "исчезают" в дочернем процессе
- Все блокировки (включая GIL) остаются в том же состоянии, что может вызвать deadlock

Решение: `multiprocessing` использует `pthread_atfork()` для регистрации обработчиков, которые сбрасывают GIL и другие
блокировки.

**8. Управление ресурсами процессов**

- **Дескрипторы файлов**: При fork все открытые файлы копируются. `multiprocessing` закрывает ненужные дескрипторы.
- **Сигналы**: Установка обработчиков для SIGINT, SIGTERM, SIGCHLD
- **Зомби-процессы**: Использование `waitpid()` с флагом WNOHANG для предотвращения

**9. Производительность и оптимизации**

- **Pickle протокол 5 с out-of-band**: Позволяет передавать большие данные (массивы numpy) без копирования
- **Наследование дескрипторов**: Передача открытых файлов или сокетов через fork
- **Предзагрузка (preloading)**: Импорт модулей до fork для экономии памяти

**10. Для AQA: тестирование многопроцессного кода**

Критически важные аспекты:

- **Детерминированность**: Использование фиксированного seed для random, контроль порядка выполнения
- **Изоляция тестов**: Каждый тест должен запускаться в отдельном процессе или использовать чистые фикстуры
- **Проверка утечек ресурсов**:
    - Отслеживание числа процессов через `psutil`
    - Проверка file descriptor leak через `os.sysconf('SC_OPEN_MAX')`
    - Мониторинг памяти через `resource.getrusage()`
- **Тестирование граничных условий**:
    - Обработка SIGKILL/SIGTERM
    - Переполнение очередей
    - Deadlock в IPC
- **Производительность**:
    - Замеры скорости IPC для разных размеров данных
    - Scalability testing: увеличение числа процессов
    - Нагрузочное тестирование с ограничением памяти
- **Интеграционное тестирование**:
    - Воспроизведение production-окружения
    - Тестирование с разными методами запуска (fork, spawn, forkserver)
    - Проверка восстановления после сбоев
- **Mocking и stubbing**:
    - Подмена `multiprocessing.Process` для unit-тестов
    - Заглушки для системных вызовов через `unittest.mock`
- **Отладка сложных сценариев**:
    - Использование `faulthandler` для дампов при deadlock
    - Логирование с идентификаторами процессов
    - Визуализация взаимодействия процессов

**11. Интеграция с asyncio**

Python 3.8+ предоставляет `asyncio.run_in_executor()` с `ProcessPoolExecutor`:

- Асинхронный запуск CPU-интенсивных задач
- Использование `loop.subprocess_exec()` для запуска внешних процессов
- Обработка результатов через callbacks без блокировки event loop

**12. Альтернативные реализации**

- **`concurrent.futures.ProcessPoolExecutor`**: Более высокоуровневый API
- **`joblib`**: Специализированная библиотека для численных вычислений
- **`ray`**: Распределенные вычисления с продвинутым IPC
- **`dask`**: Параллельные вычисления с отложенным выполнением

**13. Безопасность**

- **Валидация входных данных**: Pickle уязвим для атак, необходимо проверять данные
- **Изоляция процессов**: Использование chroot, namespaces, cgroups
- **Очистка разделяемой памяти**: Гарантированное удаление shm объектов

**14. Отладка и профилирование**

- **`multiprocessing.get_logger()`**: Встроенное логирование
- **`cProfile` + `pstats`**: Профилирование каждого процесса отдельно
- **`tracemalloc`**: Отслеживание утечек памяти между процессами
- **Системные утилиты**: `strace`, `perf`, `valgrind` для низкоуровневой отладки

- [Содержание](#содержание)

---

# **Dataclass**

Dataclass — это декоратор из модуля `dataclasses`, который автоматически генерирует специальные методы (такие как
`__init__`, `__repr__` и `__eq__`) для классов, предназначенных в основном для хранения данных. Это позволяет избежать
написания шаблонного кода, делает классы чище и менее подверженными ошибкам. Dataclass поддерживает гибкую настройку
через параметры декоратора и функцию `field()`, включая создание неизменяемых (frozen) классов и оптимизацию памяти с
помощью `slots=True`.

## **Junior Level**

Dataclass — это декоратор из модуля `dataclasses`, который автоматически добавляет в класс специальные методы на основе
объявленных полей. Он предназначен для создания классов, которые в основном хранят данные, избавляя разработчика от
написания шаблонного кода.

Вместо того чтобы вручную писать `__init__`, `__repr__`, `__eq__` и другие методы для классов-контейнеров данных, можно
использовать `@dataclass`, и Python сам сгенерирует эти методы. Это делает код чище, уменьшает вероятность ошибок и
упрощает поддержку.

Например, класс для представления точки с координатами x и y можно определить одной строкой:
`@dataclass class Point: x: int; y: int`. Автоматически появятся конструктор, строковое представление и сравнение по
значениям полей.

## **Middle Level**

1. **Автоматически генерируемые методы**:
    - `__init__()`: Инициализатор с параметрами для всех полей
    - `__repr__()`: Читаемое строковое представление
    - `__eq__()`: Сравнение по значениям всех полей
    - `__ne__()`: Автоматически на основе `__eq__`
    - Опционально (через параметры декоратора):
        - `__lt__()`, `__le__()`, `__gt__()`, `__ge__()`: при `order=True`
        - `__hash__()`: при `frozen=True` или явном указании

2. **Параметры декоратора `@dataclass`**:
    - `init=True`: Генерация `__init__`
    - `repr=True`: Генерация `__repr__`
    - `eq=True`: Генерация `__eq__`
    - `order=False`: Генерация методов сравнения
    - `frozen=False`: Запрет изменения полей после создания
    - `unsafe_hash=False`: Принудительная генерация `__hash__`
    - `match_args=True`: Поддержка паттерн-матчинга (Python 3.10+)

3. **Поле `field()`**: Функция для тонкой настройки полей:
    - `default`: Значение по умолчанию
    - `default_factory`: Фабрика для изменяемых значений по умолчанию
    - `init`: Включать ли поле в `__init__`
    - `repr`: Включать ли поле в `__repr__`
    - `compare`: Участвует ли поле в сравнении
    - `hash`: Участвует ли поле в вычислении хеша
    - `metadata`: Произвольные метаданные

4. **Наследование**: Dataclasses поддерживают наследование, поля собираются от всех родительских классов с учётом MRO.
   Важно правильно использовать значения по умолчанию при наследовании.

5. **Пост-инициализация**: Метод `__post_init__()` вызывается после автоматического `__init__` и может использоваться
   для дополнительной валидации или вычисления производных полей.

6. **Иммутабельные dataclasses**: При `frozen=True` экземпляры становятся неизменяемыми. Попытка изменить поле вызывает
   `FrozenInstanceError`.

## **Senior Level**

**1. Внутренняя реализация в CPython**

Модуль `dataclasses` написан на чистом Python, но использует низкоуровневые механизмы CPython:

```python
# Логика генерации методов
def _create_fn(name, args, body, *, globals=None, locals=None):
    # Создание объекта кода через compile()
    # Исполнение через exec() для создания функции
    pass
```

Декоратор `@dataclass` работает в несколько этапов:

1. Сканирование аннотаций класса
2. Сбор информации о полях через `Field` объекты
3. Генерация исходного кода методов как строк
4. Компиляция и привязка методов к классу

**2. Генерация `__init__`**

Для класса:

```python
@dataclass
class Point:
    x: int
    y: int = 0
```

Генерируется код, эквивалентный:

```python
def __init__(self, x: int, y: int = 0):
    self.x = x
    self.y = y
```

Но реальная реализация использует `object.__setattr__` для обхода возможных ограничений (например, для frozen классов
или классов со `__slots__`).

**3. Сбор полей и разрешение MRO**

Алгоритм сбора полей:

- Обход MRO от базовых классов к производным
- Сбор полей в порядке определения
- Разрешение конфликтов: поля из производных классов переопределяют поля из базовых
- Обработка полей без аннотаций (игнорируются в Python 3.10+)

**4. Оптимизация с `__slots__`**

Начиная с Python 3.10, можно использовать `slots=True`:

```python
@dataclass(slots=True)
class Point:
    x: int
    y: int
```

Это:

- Создаёт `__slots__` автоматически
- Уменьшает потребление памяти на ~40%
- Ускоряет доступ к атрибутам
- Но запрещает добавление новых атрибутов

Реализация: декоратор динамически создаёт класс с помощью `type()`.

**5. Хеширование и frozen-классы**

Для frozen dataclass:

- `__hash__` генерируется на основе всех полей
- Используется кортеж из хешей полей: `hash((self.x, self.y, ...))`
- При `unsafe_hash=True` хеш генерируется даже для изменяемых классов (опасно!)

**6. Паттерн-матчинг (Python 3.10+)**

При `match_args=True` (по умолчанию) генерируется `__match_args__`:

```python
Point.__match_args__ = ('x', 'y')
```

Это позволяет использовать dataclass в match:

```python
match point:
    case Point(x=0, y=0):
        print("Origin")
```

**7. Поля с `init=False`**

Поля, исключённые из `__init__`, часто вычисляются в `__post_init__`. Пример:

```python
@dataclass
class Rectangle:
    width: float
    height: float
    area: float = field(init=False)

    def __post_init__(self):
        self.area = self.width * self.height
```

**8. Наследование и default-значения**

Проблема: mutable default values. Решение — использовать `default_factory`:

```python
@dataclass
class Node:
    children: list = field(default_factory=list)  # А не children: list = []
```

При наследовании важно: сначала идут поля без значений по умолчанию, затем с значениями по умолчанию.

**9. Методы сравнения при `order=True`**

Генерируются все методы сравнения на основе кортежа полей:

```python
def __lt__(self, other):
    if isinstance(other, self.__class__):
        return (self.x, self.y) < (other.x, other.y)
    return NotImplemented
```

Этот подход гарантирует тотальный порядок.

**10. Производительность**

- **Плюсы**: Сгенерированный код на C-уровне быстрее, чем ручной Python-код
- **Минусы**: На создание класса уходит больше времени (генерация методов)
- **Память**: Обычные dataclass используют `__dict__`, что увеличивает потребление памяти

Оптимизация: `@dataclass(slots=True)` + `__slots__` для экономии памяти.

**11. Интроспекция**

Dataclass предоставляет API для интроспекции:

- `fields(class_or_instance)`: Список полей
- `asdict(instance)`: Конвертация в словарь
- `astuple(instance)`: Конвертация в кортеж
- `is_dataclass(obj)`: Проверка, является ли dataclass

**12. Кастомные дескрипторы и property**

Можно комбинировать с property:

```python
@dataclass
class Circle:
    radius: float

    @property
    def diameter(self):
        return self.radius * 2
```

Но property не участвует в автогенерации методов.

**13. Тестирование для AQA**

При тестировании dataclass важно проверять:

1. **Корректность генерации методов**: `__init__`, `__repr__`, `__eq__`, etc.
2. **Наследование**: Правильный порядок и обработка полей
3. **Frozen-классы**: Неизменяемость после создания
4. **Значения по умолчанию**: Особенно изменяемые (list, dict)
5. **Хеширование**: Для frozen классов и при `unsafe_hash`
6. **Сериализация**: `asdict()` и `astuple()`
7. **Паттерн-матчинг**: Корректность работы с match/case
8. **Производительность**: Время создания экземпляров, сравнения
9. **Память**: Потребление при использовании `slots=True` vs обычных

**14. Отладка и внутренние структуры**

Можно исследовать сгенерированный код:

```python
import dis

dis.dis(Point.__init__)  # Посмотреть байткод __init__
print(Point.__annotations__)  # Аннотации
from dataclasses import fields

print(fields(Point))  # Объекты Field
```

- [Содержание](#содержание)

---

# **Enum**

Enum (перечисление) в Python — это специальный тип данных, позволяющий создавать набор именованных констант, что делает
код более читаемым и защищённым от опечаток. Члены Enum являются иммутабельными синглтонами и поддерживают итерацию,
сравнение и доступ по имени или значению. Python предоставляет несколько вариантов: базовый `Enum`, `IntEnum` (
наследующий `int`), `Flag` для битовых операций и `StrEnum` (с Python 3.11) для строковых констант.

## **Junior Level**

Enum (перечисление) в Python — это специальный тип данных, который позволяет создавать набор именованных констант.
Вместо использования "магических чисел" или строк в коде вы можете создать понятные имена для значений, что делает код
более читаемым и защищённым от опечаток.

Например, вместо использования чисел 0, 1, 2 для статусов задачи можно создать Enum:

```python
from enum import Enum


class TaskStatus(Enum):
    PENDING = 0
    RUNNING = 1
    COMPLETED = 2
    FAILED = 3
```

Теперь в коде можно использовать `TaskStatus.RUNNING` вместо просто `1`. Enum предоставляет итерацию, сравнение и доступ
к членам по имени или значению. Особенно полезны они для ограничения допустимых значений параметров функции и замены
строковых констант.

## **Middle Level**

1. **Типы Enum**:
    - `Enum`: Базовый класс для создания перечислений
    - `IntEnum`: Члены являются подклассами `int`, могут использоваться везде, где ожидается целое число
    - `Flag`, `IntFlag`: Для битовых флагов, поддерживают побитовые операции
    - `StrEnum` (Python 3.11+): Члены являются подклассами `str`

2. **Свойства членов Enum**:
    - `name`: Имя члена (строка)
    - `value`: Значение члена (может быть любого типа)
    - `__members__`: Словарь всех членов {имя: член}

3. **Автоматические значения**:
    - `auto()`: Автоматически присваивает уникальные значения (начиная с 1)
    - `@enum.unique`: Декоратор, гарантирующий уникальность значений

4. **Методы и операции**:
    - Итерация: `for status in TaskStatus:`
    - Доступ по значению: `TaskStatus(1)`
    - Доступ по имени: `TaskStatus['RUNNING']`
    - Проверка принадлежности: `isinstance(value, TaskStatus)`

5. **Расширенные возможности**:
    - Методы класса: можно добавлять методы в класс Enum
    - Свойства (property): вычисляемые атрибуты
    - Наследование: Enum может наследоваться от других классов (кроме другого Enum)

6. **Иммутабельность**: Члены Enum — синглтоны. Нельзя изменить их значение после создания.

## **Senior Level**

**1. Метапрограммирование и EnumMeta**

Ключ к пониманию Enum — метакласс `EnumMeta`. При создании класса Enum:

- Метакласс `EnumMeta` перехватывает создание класса
- Сканирует атрибуты класса, собирая члены перечисления
- Заменяет простые присваивания (например, `RED = 1`) объектами-членами Enum

```python
# Внутренний процесс создания Enum
class Color(Enum):
    RED = 1
    GREEN = 2

# На самом деле создаётся:
# Color.RED = Color(name='RED', value=1)
# Color.GREEN = Color(name='GREEN', value=2)
```

**2. Структура объекта члена Enum**

В CPython (`Modules/enummodule.c`):

```c
typedef struct {
    PyObject_HEAD
    PyObject *name;     // Имя члена (строка)
    PyObject *value;    // Значение члена (любой объект Python)
    PyObject *type;     // Ссылка на класс Enum
} enumobject;
```

Члены Enum наследуются от своего класса, но являются отдельными экземплярами:

- `Color.RED.__class__` вернёт `<enum 'Color'>`
- `isinstance(Color.RED, Color)` вернёт `True`

**3. Синглтон-поведение и `__new__`**

Метод `__new__` в классе Enum переопределён для обеспечения синглтон поведения:

```python
def __new__(cls, value):
    # Если значение уже существует, возвращаем существующий член
    if value in cls._value2member_map_:
        return cls._value2member_map_[value]
    # Иначе создаём новый
    obj = object.__new__(cls)
    obj._value_ = value
    return obj
```

Кэширование в `_value2member_map_` обеспечивает O(1) доступ по значению.

**4. IntEnum и наследование от встроенных типов**

`IntEnum` наследуется от `int` и `Enum`:

```python
class IntEnum(int, Enum):
    pass
```

Это создаёт сложности с MRO (Method Resolution Order). Python использует специальную логику в метаклассе `EnumMeta` для
разрешения конфликтов.

**5. Flag и IntFlag: битовые операции**

`Flag` и `IntFlag` используют другой подход:

- Значения должны быть степенями двойки (1, 2, 4, 8...)
- Поддерживают побитовые операции: `|`, `&`, `^`, `~`
- Могут комбинироваться: `Permissions.READ | Permissions.WRITE`

Внутренне комбинации флагов представляются как отдельные экземпляры с составными значениями.

**6. Дескрипторы и доступ к членам**

При доступе к атрибуту Enum через точку (`Color.RED`):

- Срабатывает дескриптор в метаклассе
- Возвращается заранее созданный объект-член
- Дескриптор обеспечивает иммутабельность — нельзя переназначить `Color.RED`

**7. `__members__` и интроспекция**

`__members__` — это `collections.OrderedDict`, создаваемый метаклассом:

- Ключи: имена членов
- Значения: объекты-члены Enum
- Сохраняет порядок определения

Метакласс также создаёт:

- `_member_names_`: список имён в порядке определения
- `_member_map_`: {имя: член}
- `_value2member_map_`: {значение: член}

**8. Оптимизации производительности**

- **Кэширование**: Доступ по значению кэшируется в `_value2member_map_`
- **Слоты**: Объекты членов могут использовать `__slots__` для экономии памяти
- **Хеширование**: Члены Enum хешируемы и могут использоваться как ключи словаря

**9. Создание Enum во время выполнения**

Enum можно создавать динамически:

```python
Color = Enum('Color', ['RED', 'GREEN', 'BLUE'])
# Эквивалентно:
# class Color(Enum):
#     RED = 1
#     GREEN = 2
#     BLUE = 3
```

Внутренне это использует `type()` для создания класса.

**10. Сериализация и pickle**

Enum корректно сериализуется pickle:

- При сериализации сохраняется ссылка на член
- При десериализации восстанавливается тот же объект (синглтон)
- Это гарантирует, что `Color.RED is pickle.loads(pickle.dumps(Color.RED))` равно `True`

**11. Проверка типов и аннотации**

Для аннотаций типов можно использовать:

- `Literal[Color.RED, Color.GREEN]` для конкретных значений
- `Color` для любого члена перечисления
- `typing.get_args()` и `typing.get_origin()` для интроспекции

**12. Проблемы и ограничения**

- **Наследование**: Нельзя наследоваться от другого Enum (кроме смешивания с другими классами)
- **Изменяемые значения**: Если значение Enum изменяемое (например, список), это может привести к неожиданному поведению
- **Производительность**: Создание Enum класса медленнее, чем создание простого класса
- **Память**: Каждый член Enum — отдельный объект, что увеличивает потребление памяти

**13. Тестирование для AQA**

При тестировании Enum важно проверять:

1. **Уникальность значений**: Все значения должны быть уникальны (если используется `@unique`)
2. **Корректность операций сравнения**: `==`, `is`, `in`
3. **Итерацию**: Порядок итерации должен соответствовать порядку определения
4. **Сериализацию**: Корректность pickle и JSON сериализации
5. **Типизацию**: Корректность работы с type hints и mypy
6. **Битовые операции**: Для Flag и IntFlag проверка побитовых операций
7. **Граничные случаи**: Доступ к несуществующим членам, сравнение с не-Enum
8. **Производительность**: Время доступа к членам, итерации

**14. Отладка и интроспекция**

Можно исследовать внутреннюю структуру:

```python
print(Color._member_names_)
print(Color._member_map_)
print(Color._value2member_map_)
print(Color.RED._value_)
```

- [Содержание](#содержание)

---

# **Garbage Collector (Сборщик мусора)**

Сборщик мусора в Python использует два основных механизма: подсчёт ссылок для немедленного освобождения памяти, когда
счётчик достигает нуля, и циклический сборщик для обнаружения и удаления недостижимых групп объектов, ссылающихся друг
на друга. Объекты делятся на три поколения, что оптимизирует процесс, так как большинство объектов живут недолго. Модуль
`gc` позволяет управлять этим процессом, получать статистику и настраивать поведение сборщика.

## **Junior Level**

Сборщик мусора в Python — это автоматический механизм управления памятью, который освобождает память от объектов,
которые больше не используются программой. Python использует комбинированный подход: **подсчет ссылок (reference
counting)** для немедленного освобождения памяти и **циклический сборщик мусора (cycle collector)** для обнаружения и
удаления циклических ссылок.

Представьте, что каждый объект в Python имеет счётчик, который увеличивается при создании новой ссылки на объект и
уменьшается при удалении ссылки. Когда счётчик достигает нуля, объект немедленно удаляется. Однако, если два объекта
ссылаются друг на друга (циклическая ссылка), их счётчики никогда не станут нулевыми. Для таких случаев существует
циклический сборщик мусора, который периодически запускается и находит недостижимые циклы.

Модуль `gc` позволяет управлять сборщиком мусора, получать статистику и настраивать поведение.

## **Middle Level**

1. **Два механизма управления памятью**:
    - **Подсчет ссылок**: Каждый объект хранит счётчик ссылок (`ob_refcnt`). При создании ссылки счётчик увеличивается,
      при удалении — уменьшается. При достижении нуля вызывается деструктор (`__del__`) и память освобождается. Это
      происходит немедленно и предсказуемо.
    - **Циклический сборщик**: Обнаруживает и удаляет группы объектов, которые ссылаются друг на друга, но недостижимы
      из основной программы.

2. **Поколения объектов (Generational GC)**:
    - Объекты делятся на три поколения (0, 1, 2). Новые объекты попадают в поколение 0.
    - Чаще всего запускается сборка в поколении 0, реже — в 1, ещё реже — в 2.
    - Основано на эмпирическом наблюдении: большинство объектов живут недолго.

3. **Пороги сборки**:
    - Параметры `gc.get_threshold()` возвращают кортеж (threshold0, threshold1, threshold2).
    - Сборка в поколении 0 запускается, когда количество выделений минус освобождений превышает threshold0.
    - После определённого числа сборок в поколении 0 запускается сборка в поколении 1 и т.д.

4. **Модуль `gc`**:
    - `gc.enable()` / `gc.disable()`: Включение/выключение циклического сборщика.
    - `gc.collect(generation=None)`: Принудительный запуск сборки.
    - `gc.get_referents(obj)`: Объекты, на которые ссылается `obj`.
    - `gc.get_referrers(obj)`: Объекты, которые ссылаются на `obj`.
    - `gc.set_debug(flags)`: Установка флагов отладки.

5. **Проблемные случаи**:
    - **Циклические ссылки с `__del__`**: Если объекты в цикле имеют метод `__del__`, сборщик не может безопасно удалить
      их (возникает `uncollectable`).
    - **Слабые ссылки (`weakref`)**: Не увеличивают счётчик ссылок, не препятствуют сборке мусора.

6. **Производительность**:
    - Подсчёт ссылок происходит при каждой операции с объектами (быстро, но постоянно).
    - Циклический сборщик запускается периодически и может вызывать паузы.

## **Senior Level**

**1. Реализация подсчёта ссылок в CPython**

В `Include/object.h`:

```c
typedef struct _object {
    Py_ssize_t ob_refcnt;        // Счётчик ссылок
    PyTypeObject *ob_type;
} PyObject;
```

Макросы для работы со счётчиком:

- `Py_INCREF(op)`: Увеличивает счётчик (атомарно в многопоточном режиме).
- `Py_DECREF(op)`: Уменьшает счётчик. При достижении нуля вызывает `_Py_Dealloc(op)`.

**2. Циклический сборщик: алгоритм и структуры**

**Поколения**: Каждое поколение — это двусвязный список объектов (`_PyGC_Head`):

```c
typedef struct _gc_head {
    uintptr_t _gc_next;   // Следующий объект в списке поколения
    uintptr_t _gc_prev;   // Предыдущий объект
    Py_ssize_t gc_refs;   // Временное поле для алгоритма
} PyGC_Head;
```

**Алгоритм обнаружения циклов (трехцветная маркировка)**:

1. **Идентификация**: Все объекты в поколении помечаются как белые (непосещённые).
2. **Поиск корней**: Обход корневых объектов (глобальные переменные, локальные переменные в стеке вызовов, регистры
   процессора). Они помечаются как серые.
3. **Распространение**: Пока есть серые объекты:
    - Берём серый объект, обходим все его ссылки.
    - Если ссылка ведёт на белый объект, делаем его серым.
    - Исходный объект становится чёрным.
4. **Очистка**: Все белые объекты (недостижимые) удаляются.

**3. Поколения и пороги**

Внутренние счетчики:

- `generations[0].count`: Количество объектов в поколении 0.
- `generations[0].threshold`: Порог для запуска сборки (по умолчанию 700).

Когда `generations[0].count > generations[0].threshold`, запускается сборка в поколении 0. После сборки выжившие объекты
перемещаются в поколение 1, и т.д.

**4. Обработка `__del__` и `gc.garbage`**

Если в цикле есть объекты с методом `__del__`, сборщик не может определить порядок вызова деструкторов. Такие объекты
помещаются в `gc.garbage` (список), чтобы программист мог обработать их вручную.

**5. Многопоточность и GIL**

Подсчёт ссылок атомарен благодаря GIL. Циклический сборщик работает так:

- При запуске сборки все потоки приостанавливаются.
- Сборщик захватывает GIL и выполняет сборку.
- Потоки возобновляются после завершения.

**6. Оптимизации CPython**

**Быстрое выделение памяти (pymalloc)**:

- Использует собственный аллокатор для небольших объектов (до 512 байт).
- Уменьшает фрагментацию и ускоряет выделение/освобождение.

**Кэширование свободной памяти**:

- Освобождённая память кэшируется для быстрого повторного использования.
- Для каждого размера объекта есть свой пул свободных блоков.

**7. Слабые ссылки (weakref)**

Реализованы через отдельную структуру:

```c
typedef struct _PyWeakReference {
    PyObject_HEAD
    PyObject *wr_object;    // Слабая ссылка (может быть NULL)
    PyObject *wr_callback;  // Колбэк при удалении объекта
    // ...
} PyWeakReference;
```

Слабые ссылки не увеличивают `ob_refcnt`, поэтому не препятствуют сборке мусора.

**8. Отладка и диагностика**

Флаги отладки (`gc.set_debug()`):

- `DEBUG_STATS`: Вывод статистики после сборки.
- `DEBUG_COLLECTABLE`: Вывод информации об удаляемых объектах.
- `DEBUG_UNCOLLECTABLE`: Вывод информации о неудаляемых объектах (с `__del__`).
- `DEBUG_SAVEALL`: Сохранение всех удаляемых объектов в `gc.garbage`.

**9. Производительность и настройка**

**Проблемы**:

- **Паузы**: Циклический сборщик может вызывать заметные паузы в работе программы.
- **Фрагментация памяти**: Частые выделения/освобождения приводят к фрагментации.

**Оптимизации**:

- Настройка порогов через `gc.set_threshold()`.
- Отключение сборщика (`gc.disable()`) в критичных по времени участках кода (с последующим ручным запуском).
- Использование слабых ссылок для предотвращения циклических ссылок.

**10. Тестирование для AQA**

**Подходы к тестированию**:

1. **Тестирование утечек памяти**:
    - Использование `tracemalloc` для отслеживания выделений.
    - Создание сценариев, которые должны освобождать память.
    - Проверка, что `gc.collect()` не находит новых недостижимых объектов.

2. **Тестирование циклических ссылок**:
    - Создание искусственных циклов и проверка их удаления.
    - Проверка объектов с `__del__` (должны попадать в `gc.garbage`).

3. **Нагрузочное тестирование**:
    - Длительная работа приложения с проверкой стабильности потребления памяти.
    - Тестирование в условиях нехватки памяти (out-of-memory).

4. **Интеграция с профилировщиками**:
    - Использование `objgraph` для визуализации графа объектов.
    - Интеграция с `pympler` для детального анализа памяти.

**11. Инструменты для анализа памяти**

- `tracemalloc`: Трассировка выделений памяти.
- `objgraph`: Визуализация графа объектов.
- `pympler`: Детальный анализ использования памяти.
- `memory_profiler`: Профилирование использования памяти построчно.
- `gc.get_objects()`: Получение списка всех отслеживаемых объектов (осторожно — может быть медленно).

**12. Специфика для разных реализаций Python**

- **CPython**: Описанный выше подход (подсчёт ссылок + циклический сборщик).
- **PyPy**: Использует сложный сборщик мусора с перемещением объектов (moving GC), более агрессивный и эффективный.
- **Jython/IronPython**: Полагаются на сборщик мусора JVM/.NET соответственно.

- [Содержание](#содержание)

---

# **Сложность кода**

## **Junior Level**

1. **Асимптотическая сложность** — как быстро растёт время выполнения программы или использование памяти при увеличении
   объёма данных. Говорят "O-большое": O(1) — постоянно, O(n) — линейно, O(n²) — квадратично и т.д.

2. **Цикломатическая сложность** — мера количества путей выполнения в программе. Чем больше if/else, for/while, тем выше
   эта сложность и тем больше тестов нужно написать.

3. **Сложность связей** — насколько модули/классы/функции зависят друг от друга. Сильно связанный код труднее
   тестировать, потому что приходится мокать много зависимостей.

4. **Сложность поддержки** — насколько легко изменять и развивать код. Включает читаемость, документированность,
   соответствие стандартам.

Для тестировщика: высокая асимптотическая сложность → проверяем производительность на больших данных; высокая
цикломатическая сложность → больше тестовых сценариев; высокая связность → сложнее изолировать модули для тестирования;
низкая поддерживаемость → больше времени на понимание кода перед тестированием.

## **Middle Level**

**1. Асимптотическая сложность (Asymptotic Complexity)**

- **Верхняя граница (O-нотация)**: O(f(n)) — время выполнения не превысит c·f(n) для больших n.
- **Нижняя граница (Ω-нотация)**: Ω(f(n)) — время выполнения не меньше c·f(n).
- **Точная граница (Θ-нотация)**: Θ(f(n)) — и O(f(n)), и Ω(f(n)).
- **Амортизированный анализ**: Средняя производительность по серии операций.
- **Космическая сложность**: Аналогично временной, но для памяти.

**Практика для AQA**:

- Тестирование boundary values: малые, средние, большие объёмы данных.
- Профилирование с помощью cProfile, memory_profiler.
- Нагрузочное тестирование с постепенным увеличением нагрузки.

**2. Цикломатическая сложность (Cyclomatic Complexity)**

- **Расчёт**: M = E - N + 2P, где E — рёбра в графе потока управления, N — узлы, P — компоненты связности.
- **Практические рекомендации**:
    - 1-10: простая функция, низкий риск
    - 11-20: умеренная сложность
    - 21-50: высокая сложность, требует переработки
    - 51+: очень высокая, неприемлемо
- **Инструменты**: radon, mccabe, pylint, sonarqube.

**Связь с тестированием**:

- Минимальное количество тестов ≥ цикломатической сложности.
- Каждый независимый путь должен быть покрыт тестом.
- Особое внимание сложным условиям и вложенным циклам.

**3. Сложность связей (Coupling Complexity)**

**Типы связности**:

- **Content coupling**: Прямой доступ к внутренним данным.
- **Common coupling**: Использование глобальных переменных.
- **Control coupling**: Передача флагов управления.
- **Stamp coupling**: Передача составных объектов.
- **Data coupling**: Передача только необходимых данных (идеал).

**Метрики**:

- **CBO (Coupling Between Objects)**: Количество классов, с которыми связан данный класс.
- **Ca (Afferent Coupling)**: Сколько классов зависят от данного.
- **Ce (Efferent Coupling)**: От скольких классов зависит данный.

**Для тестирования**:

- Высокая связность → сложнее мокировать зависимости.
- Необходимость интеграционных тестов в дополнение к модульным.
- Риск каскадных изменений при модификации кода.

**4. Сложность поддержки (Maintainability Complexity)**

**Компоненты**:

- **Анализируемость**: Насколько легко понять код.
- **Изменяемость**: Насколько легко вносить изменения.
- **Стабильность**: Насколько изменения в одной части влияют на другие.
- **Тестируемость**: Насколько легко писать и выполнять тесты.

**Метрики**:

- **MI (Maintainability Index)**: Рассчитывается на основе цикломатической сложности, LOC, комментариев.
- **Комментарии/код**: Соотношение комментариев к объёму кода.
- **Нарушения кодстайла**: Количество нарушений PEP8, других стандартов.

**Инструменты**: SonarQube, CodeClimate, Codacy.

- [Содержание](#содержание)

---

# **ООП**

## **Junior Level**

Объектно-ориентированное программирование (ООП) — это подход к разработке программ, где основными строительными блоками
являются объекты, которые объединяют данные и методы для работы с ними.

Основные концепции ООП:

1. **Класс** — чертёж или шаблон для создания объектов (например, класс `Dog`).
2. **Объект** — конкретный экземпляр класса (например, `my_dog = Dog()`).
3. **Инкапсуляция** — объединение данных и методов в одном объекте и сокрытие внутренней реализации от внешнего мира.
4. **Наследование** — возможность создавать новые классы на основе существующих, перенимая их свойства и методы (
   например, класс `Poodle` наследует от `Dog`).
5. **Полиморфизм** — возможность использовать объекты разных классов через одинаковый интерфейс (например, методы
   `speak()` для `Dog` и `Cat` могут работать по-разному).
6. **Абстракция** - это концепция, которая позволяет выделить существенные характеристики объекта, игнорируя
   несущественные детали реализации. Она помогает работать со сложными системами, представляя их в виде упрощённых
   моделей.

ООП помогает организовать код, делает его более понятным, удобным для повторного использования и изменения.

## **Middle Level**

В Python ООП реализовано динамически и поддерживает все классические принципы, а также некоторые уникальные особенности:

**1. Динамическая природа классов и объектов**:

- Классы являются объектами первого класса (можно присваивать переменным, передавать в функции).
- Атрибуты и методы могут добавляться, изменяться или удаляться во время выполнения.
- Поддержка динамического создания классов через `type()`.

**2. Множественное наследование и MRO**:

- Python поддерживает множественное наследование.
- Алгоритм C3 Linearization определяет порядок разрешения методов (Method Resolution Order, MRO).
- `super()` используется для корректного вызова методов родительских классов.

**3. Дескрипторы и свойства**:

- **Дескрипторы** — объекты, реализующие протокол `__get__`, `__set__`, `__delete__`. Лежат в основе свойств, методов,
  статических методов и методов класса.
- **Свойства (property)** — позволяют использовать getter, setter и deleter для атрибутов, сохраняя синтаксис доступа к
  атрибуту.

**4. Абстрактные классы**:

- Модуль `abc` позволяет создавать абстрактные классы и методы.
- Абстрактный класс не может быть инстанциирован, требует переопределения абстрактных методов в дочерних классах.

**5. Магические методы (dunder methods)**:

- Методы вида `__init__`, `__str__`, `__add__` и т.д.
- Позволяют переопределять поведение операторов и встроенных функций для объектов пользовательских классов.

**6. Метаклассы**:

- Классы, экземпляры которых являются другими классами.
- Позволяют вмешиваться в процесс создания класса, добавлять валидацию, регистрацию и т.д.

**7. Принципы SOLID в Python**:

- **Single Responsibility**: Каждый класс должен иметь одну причину для изменения.
- **Open/Closed**: Классы должны быть открыты для расширения, но закрыты для изменения.
- **Liskov Substitution**: Объекты базового класса должны быть заменяемы объектами производных классов без изменения
  корректности программы.
- **Interface Segregation**: Много специализированных интерфейсов лучше одного универсального.
- **Dependency Inversion**: Зависимости должны строиться на абстракциях, а не на деталях.

**8. Протоколы и утиная типизация**:

- Python использует утиную типизацию: интерфейс объекта определяется его поведением (наличием методов), а не явным
  объявлением.
- **Протоколы** — неформальные интерфейсы, например, протокол итератора требует методы `__iter__` и `__next__`.

## **Senior Level**

**1. Философские основы ООП и их реализация в Python**

**Онтология объектов**:

- В Python всё является объектом, включая классы и метаклассы, что создаёт рекурсивную онтологическую структуру.
- Классы как объекты порождают эпистемологическую проблему: класс одновременно является и шаблоном, и экземпляром.

**Проблема идентичности vs равенства**:

- `is` проверяет идентичность (совпадение объектов в памяти).
- `==` проверяет равенство (через `__eq__`).
- Для иммутабельных объектов (строк, чисел) Python кэширует и переиспользует объекты, что размывает границу.

**2. Динамизм как фундаментальное свойство**

**Модификация во время выполнения**:

- Возможность изменения класса влияет на все существующие экземпляры (через обновление `__class__.__dict__`).
- Паттерн "Monkey Patching" — модификация поведения во время выполнения, мощный, но опасный инструмент.

**Интроспекция и рефлексия**:

- `inspect` модуль позволяет исследовать структуру классов во время выполнения.
- `__dict__` и `__slots__` как различные подходы к хранению атрибутов.
- `getattr`, `setattr`, `hasattr` — доступ к атрибутам через строковые имена.

**3. Множественное наследование и проблема ромбовидного наследования**

**Алгоритм C3 Linearization**:

- Решает проблему неоднозначности порядка вызова методов.
- Гарантирует сохранение порядка из родительских классов и монотонность.
- Формально: пусть L[C] — линеаризация класса C, тогда L[C] = [C] + merge(L[B1], ..., L[Bn], [B1, ..., Bn]), где merge
  удаляет первый элемент из списков, который не встречается в хвостах других списков.

**Миксины (Mixins)**:

- Классы, предназначенные для расширения функциональности через множественное наследование.
- Не предназначены для самостоятельного использования, только в комбинации с другими классами.

**4. Дескрипторы и метапрограммирование**

**Теория дескрипторов**:

- **Data descriptors** (имеют `__set__` или `__delete__`) имеют приоритет над записью в `__dict__`.
- **Non-data descriptors** (только `__get__`) уступают `__dict__`.
- Цепочка вызовов: `obj.attr` → `type(obj).__dict__['attr'].__get__(obj, type(obj))`.

**Метаклассы как вершина метапрограммирования**:

- `__prepare__` возвращает пространство имён для тела класса (может быть `OrderedDict` для сохранения порядка).
- `__new__` создаёт объект класса, `__init__` инициализирует.
- Использование метаклассов для регистрации классов, валидации атрибутов, автоматического создания свойств.

**5. Альтернативные парадигмы в контексте Python**

**Композиция vs Наследование**:

- Наследование создаёт жёсткую связь "является" (is-a).
- Композиция создаёт гибкую связь "имеет" (has-a).
- В Python композиция часто предпочтительнее из-за динамической природы.

**Функциональное программирование в ООП**:

- Функции как объекты первого класса.
- Замыкания и декораторы как функциональные элементы.
- Иммутабельность и чистота функций vs изменяемое состояние объектов.

**Протокольно-ориентированное программирование**:

- Акцент на поведении (протоколах), а не иерархии классов.
- `typing.Protocol` (PEP 544) для статической проверки протоколов.
- Структурная типизация вместо номинальной.

**6. ООП и тестируемость**

**Проблемы тестирования ООП-кода**:

- **Состояние, рассеянное по многим объектам**: Трудно воспроизвести конкретное состояние для теста.
- **Наследование усложняет мокирование**: Зависимости от родительских классов.
- **Побочные эффекты**: Изменение состояния объектов влияет на последующие вызовы.

**Паттерны для улучшения тестируемости**:

- **Dependency Injection**: Внедрение зависимостей через конструктор для упрощения мокирования.
- **Сохранение чистоты функций**: Выделение бизнес-логики в чистые функции.
- **Интерфейсы и абстракции**: Использование `abc` для определения чётких контрактов.

**7. Эволюция ООП в Python**

**Историческое развитие**:

- От классических классов к new-style классам (Python 2.2+).
- Унификация типов и классов в Python 3.

**Современные тенденции**:

- **Data Classes** (Python 3.7+): Автоматическая генерация boilerplate кода для классов-данных.
- **Protocols** (Python 3.8+): Структурная типизация для статической проверки.
- **Pattern Matching** (Python 3.10+): Новый способ работы с структурами данных.

**8. Критика ООП и ответы на неё**

**Аргументы против ООП**:

- **Наследование создаёт хрупкие иерархии**: Изменение базового класса ломает дочерние.
- **Проблема "взрыва подклассов"**: Необходимость создавать множество подклассов для комбинаций признаков.
- **Состояние как источник ошибок**: Разделяемое изменяемое состояние усложняет многопоточность.

**Ответы в контексте Python**:

- **Миксины и композиция** решают проблему взрыва подклассов.
- **Иммутабельные объекты** уменьшают проблемы с состоянием.
- **Модули и функции** как альтернатива излишней объектной ориентированности.

**9. Будущее ООП и парадигм в Python**

**Конвергенция парадигм**:

- Слияние ООП, функционального и асинхронного программирования.
- Декораторы и контекстные менеджеры как элементы аспектно-ориентированного программирования.

**Влияние машинного обучения**:

- Автоматическая генерация и оптимизация кода.
- Нейросетевые метапрограммирование: AI, предлагающий архитектурные решения.

**Квантовые вычисления**:

- Объектно-ориентированное представление квантовых состояний и операций.
- Новая парадигма для квантовых алгоритмов.

- [Содержание](#содержание)

---

# **Абстракция**

## **Junior Level**

Абстракция в объектно-ориентированном программировании — это процесс выделения существенных характеристик объекта и
игнорирования несущественных деталей. Представьте, что вы заказываете пиццу: вас интересует её состав и цена, но не
детали того, как её готовят на кухне или доставляют курьером. Вы абстрагируетесь от сложных процессов, фокусируясь
только на том, что важно для вас как заказчика.

В программировании абстракция позволяет создавать модели реальных объектов, которые содержат только те свойства и
методы, которые нужны для решения конкретной задачи. Например, в программе для библиотеки класс "Книга" может иметь
свойства "автор", "название", "год издания" и методы "взять", "вернуть". Но он не будет включать такие детали, как "вес
книги", "цвет обложки" или "материал страниц", если они не важны для работы библиотечной системы.

Абстракция делает код проще, понятнее и легче для изменения.

## **Middle Level**

В Python абстракция реализуется через несколько механизмов, каждый из которых предоставляет свой уровень сокрытия
деталей:

1. **Абстрактные классы**:
    - Определяются с помощью модуля `abc` (abstract base classes).
    - Не могут быть инстанциированы напрямую.
    - Содержат абстрактные методы (помеченные `@abstractmethod`), которые должны быть реализованы в дочерних классах.
    - `@abstractproperty` (устаревшее в Python 3.3+) и комбинация `@property` с `@abstractmethod`.

2. **Интерфейсы**:
    - В Python нет явных интерфейсов как в Java/C#, но их роль выполняют абстрактные классы и протоколы.
    - Протоколы — неформальные интерфейсы, основанные на утиной типизации.

3. **Инкапсуляция как средство абстракции**:
    - `_single_underscore`: защищённый атрибут (соглашение).
    - `__double_underscore`: приватный атрибут с name mangling.
    - Свойства (`@property`) для контроля доступа к атрибутам.

4. **Уровни абстракции**:
    - **Высокоуровневые абстракции**: интерфейсы, абстрактные классы.
    - **Среднеуровневые**: конкретные классы с детализированным поведением.
    - **Низкоуровневые**: вспомогательные классы, детали реализации.

5. **Шаблоны проектирования, основанные на абстракции**:
    - **Фабричный метод**: абстрагирует процесс создания объектов.
    - **Мост (Bridge)**: разделяет абстракцию и реализацию.
    - **Стратегия (Strategy)**: абстрагирует семейство алгоритмов.

6. **Абстракция данных**:
    - Создание типов данных, которые скрывают свою внутреннюю структуру.
    - Предоставление только операций для работы с данными.

- [Содержание](#содержание)

---

# **Инкапсуляция**

## **Junior Level**

Инкапсуляция — это принцип объектно-ориентированного программирования, который объединяет данные и методы, работающие с
этими данными, внутри одного объекта и скрывает внутренние детали реализации от внешнего мира.

Представьте, что у вас есть банковский счет. Вы можете пополнять его, снимать деньги и проверять баланс, но вам не нужно
знать, как именно банк хранит ваши данные, как они обрабатывают транзакции или как обновляют баланс. Вам предоставляют
простой интерфейс (например, банкомат или мобильное приложение), а все сложности скрыты внутри.

В Python инкапсуляция реализуется через модификаторы доступа: публичные (public), защищенные (protected) и приватные (
private) атрибуты и методы. Это помогает защитить данные от неправильного использования и обеспечивает контроль над тем,
как объект взаимодействует с внешним миром.

## **Middle Level**

В Python инкапсуляция имеет свои особенности из-за динамической природы языка:

1. **Уровни доступа**:
    - **Публичные (public)**: Обычные атрибуты и методы, доступные отовсюду.
    - **Защищенные (protected)**: Имена с одним подчёркиванием (`_attr`). Это соглашение, а не строгая защита. Говорит
      разработчикам: "это для внутреннего использования".
    - **Приватные (private)**: Имена с двумя подчёркиваниями (`__attr`). Python применяет **name mangling** (искажение
      имён), превращая `__attr` в `_ClassName__attr`. Это затрудняет случайный доступ, но не делает атрибут полностью
      недоступным.

2. **Свойства (property)**:
    - Декораторы `@property`, `@attr.setter`, `@attr.deleter`.
    - Позволяют контролировать доступ к атрибутам, добавляя логику при чтении, записи или удалении.
    - Сохраняют синтаксис доступа как к обычному атрибуту.

3. **Дескрипторы**:
    - Классы, реализующие протокол `__get__`, `__set__`, `__delete__`.
    - Лежат в основе свойств, методов класса, статических методов.
    - Позволяют создавать переиспользуемые механизмы управления доступом.

4. **Методы доступа (getters/setters)**:
    - В Python не принято создавать простые getters/setters для всех атрибутов.
    - Используются только при необходимости добавить логику (валидацию, вычисления).
    - Иначе нарушается принцип "We're all consenting adults here".

5. **`__slots__`**:
    - Ограничивает набор атрибутов экземпляра.
    - Экономит память, предотвращая создание `__dict__`.
    - Ограничивает динамическое добавление атрибутов.

6. **Интерфейсы и абстракции**:
    - Абстрактные классы (`abc.ABC`) определяют контракты без раскрытия реализации.
    - Протоколы (`typing.Protocol`) для структурной типизации.

- [Содержание](#содержание)

---

# **Наследование**

## **Junior Level**

Наследование в ООП — это механизм, позволяющий создавать новый класс на основе существующего, перенимая его свойства и
методы. Новый класс называется **дочерним** (подклассом), а существующий — **родительским** (суперклассом).

Представьте, что у вас есть класс `Animal` с методами `eat()` и `sleep()`. Вы можете создать класс `Dog`, который
наследует от `Animal`, и автоматически получит эти методы. Затем вы можете добавить специфичное поведение для собаки:
метод `bark()`. Это позволяет избежать дублирования кода и создавать логические иерархии объектов.

Наследование представляет отношение **"является"** (is-a): собака является животным. Это один из основных способов
достижения полиморфизма — возможности использовать объекты разных классов через общий интерфейс.

## **Middle Level**

В Python наследование имеет несколько ключевых особенностей:

1. **Синтаксис наследования**:
   ```python
   class Parent:
       pass
   
   class Child(Parent):
       pass
   ```

2. **Множественное наследование**:
   Python поддерживает наследование от нескольких классов:
   ```python
   class Child(Parent1, Parent2, Parent3):
       pass
   ```

3. **Переопределение методов**:
   Дочерний класс может переопределять методы родительского класса, предоставляя свою реализацию.

4. **Расширение методов**:
   Использование `super()` для вызова родительской реализации:
   ```python
   def method(self):
       super().method()  # Вызов родительского метода
       # Дополнительная логика
   ```

5. **Method Resolution Order (MRO)**:
   Порядок поиска методов при множественном наследовании. Определяется алгоритмом C3 и доступен через
   `ClassName.__mro__`.

6. **Абстрактные классы и наследование**:
   Использование `abc.ABC` для создания классов, которые нельзя инстанциировать, но от которых можно наследоваться.

7. **Миксины (Mixins)**:
   Классы, предназначенные для добавления функциональности через множественное наследование. Не предназначены для
   самостоятельного использования.

8. **Доступ к родительским атрибутам**:
    - Через `super()`
    - Через явное указание имени родительского класса: `ParentClass.method(self)`

9. **Наследование встроенных типов**:
   Можно наследоваться от `list`, `dict`, `str` и других встроенных типов, но это требует осторожности.

10. **Композиция vs наследование**:
    Наследование создаёт жёсткую связь "является", композиция — гибкую связь "имеет". Композиция часто предпочтительнее.

- [Содержание](#содержание)

---

# **Полиморфизм**

## **Junior Level**

Полиморфизм — это способность объектов с разной внутренней структурой иметь одинаковый интерфейс и реагировать на одни и
те же команды по-разному.

Представьте, что у вас есть пульт дистанционного управления, который работает с разными
устройствами: нажатие кнопки "включить" включает и телевизор, и кондиционер, и музыку, но делает это по-разному. Каждое
устройство понимает команду "включить" по-своему.

В программировании полиморфизм позволяет использовать один и тот же метод или функцию для работы с объектами разных
классов. Например, если у классов `Dog` и `Cat` есть метод `speak()`, то при вызове `dog.speak()` мы услышим "Гав!", а
при `cat.speak()` — "Мяу!". Это позволяет писать более гибкий и универсальный код.

В Python полиморфизм тесно связан с концепцией "утиной типизации" (duck typing): если объект ходит как утка и крякает
как утка, то он утка. То есть Python не проверяет тип объекта заранее, а смотрит, есть ли у него нужный метод в момент
вызова.

## **Middle Level**

В Python полиморфизм проявляется в нескольких формах:

1. **Утиная типизация (Duck Typing)**:
    - Основной механизм полиморфизма в Python
    - Объекты используются на основе наличия методов/атрибутов, а не их типа
    - Пример: любой объект с методом `__len__()` можно передать в функцию `len()`

2. **Переопределение методов в наследовании**:
    - Дочерние классы могут переопределять методы родительских классов
    - Вызов метода у объекта дочернего класса использует переопределённую версию

3. **Перегрузка операторов**:
    - Магические методы (`__add__`, `__str__`, `__getitem__`) позволяют определить поведение операторов для
      пользовательских классов
    - Один и тот же оператор (`+`, `==`, `[]`) работает по-разному для разных типов

4. **Абстрактные базовые классы (ABC)**:
    - Определяют интерфейсы, которые должны быть реализованы
    - `collections.abc` содержит абстрактные классы для коллекций (`Iterable`, `Sequence`, `Mapping`)

5. **Протоколы**:
    - Неформальные интерфейсы, основанные на наличии определённых методов
    - Пример: протокол итератора требует методы `__iter__()` и `__next__()`

6. **Функции высшего порядка**:
    - Функции, принимающие другие функции как аргументы или возвращающие функции
    - `map()`, `filter()`, `sorted()` с параметром `key`

7. **Множественная диспетчеризация**:
    - Используется в библиотеках типа `multipledispatch`
    - Выбор реализации функции на основе типов нескольких аргументов

8. **`singledispatch` из `functools`**:
    - Позволяет создавать перегруженные функции на основе типа первого аргумента

- [Содержание](#содержание)

---

# **Diamond Problem**

## **Junior Level**

Проблема ромбовидного наследования (Diamond Problem) — это классическая проблема в объектно-ориентированном
программировании, возникающая при множественном наследовании.
Представьте себе ромб (алмаз), где вершина — это базовый класс A, от него наследуются
два класса B и C, а от обоих наследуется класс D. Если в классе A есть метод `some_method()`, и классы B и C
переопределяют его по-разному, то возникает вопрос: какую реализацию унаследует класс D — от B или от C?

Эта проблема создает неоднозначность в поведении программы. В Python эта проблема решается с помощью четко определенного
**порядка разрешения методов (MRO - Method Resolution Order)**, который определяет, в каком порядке интерпретатор будет
искать метод в иерархии наследования.

## **Middle Level**

Технически в Python проблема решается алгоритмом **C3 linearization**, который гарантирует детерминированный и
предсказуемый порядок поиска методов.

1. **MRO (Method Resolution Order):**
    - MRO — это порядок, в котором Python ищет методы в иерархии классов. Его можно посмотреть через атрибут класса
      `__mro__` или метод `mro()`.
    - Алгоритм C3 гарантирует, что:
        - Каждый класс встречается в MRO ровно один раз.
        - Подклассы идут перед своими суперклассами.
        - Порядок наследования, указанный в определении класса, сохраняется.

2. **Пример:**
   Для иерархии A -> B, A -> C, B -> D, C -> D, MRO для D будет `[D, B, C, A, object]`. При вызове метода из D Python
   будет искать его в этом порядке.

3. **Механизм `super()`:**
    - `super()` — это не вызов родительского класса в традиционном понимании, а вызов следующего класса в MRO.
    - Это позволяет реализовать **кооперативное множественное наследование**, где каждый класс в цепочке может добавить
      свою функциональность, вызывая `super()`.
    - При правильном использовании `super()` во всех классах иерархии (A, B, C, D) метод будет вызван у каждого из них,
      что позволяет избежать "потери" вызова.

- [Содержание](#содержание)

---

# **Магические методы**

## **Junior Level**

Магические методы в Python — это специальные методы, которые начинаются и заканчиваются двойным подчеркиванием (dunder),
например `__init__`, `__str__`, `__add__`. Они позволяют настраивать поведение объектов при выполнении стандартных
операций: создание, вывод на печать, сложение, сравнение и т.д.

Когда вы пишете `obj + other`, Python внутри вызывает `obj.__add__(other)`. Если в вашем классе определен этот метод, вы
можете задать, как именно будет работать сложение для ваших объектов. Это делает код интуитивно понятным и позволяет
вашим объектам вести себя как встроенные типы данных.

Самые распространенные магические методы:

- `__init__` — инициализатор объекта (конструктор)
- `__str__` — строковое представление для человека (`str(obj)`, `print(obj)`)
- `__repr__` — строковое представление для разработчика (показывается в консоли)
- `__len__` — поддержка функции `len(obj)`
- `__getitem__`, `__setitem__` — доступ по индексу или ключу: `obj[key]`

Использование магических методов делает ваш код более "питоничным" и понятным.

## **Middle Level**

Магические методы — это основа протоколов в Python, реализующих полиморфизм через утиную типизацию. Они делятся на
несколько категорий:

1. **Методы жизненного цикла**:
    - `__new__` — создание объекта (вызывается до `__init__`)
    - `__init__` — инициализация объекта
    - `__del__` — финализатор (вызывается перед удалением объекта)

2. **Строковые представления**:
    - `__str__` — для неформального представления (читабельного для человека)
    - `__repr__` — для формального представления (должно позволять воссоздать объект)
    - `__format__` — для поддержки `format(obj, spec)`
    - `__bytes__` — для `bytes(obj)`

3. **Протоколы сравнения**:
    - `__eq__`, `__ne__` — равенство/неравенство (`==`, `!=`)
    - `__lt__`, `__le__`, `__gt__`, `__ge__` — сравнения (`<`, `<=`, `>`, `>=`)
    - `__hash__` — вычисление хеша (обязателен для использования в множествах и как ключ словаря)

4. **Протоколы числовых операций**:
    - Арифметические: `__add__`, `__sub__`, `__mul__`, `__truediv__`
    - Унарные: `__neg__`, `__pos__`, `__abs__`
    - С преобразованием типов: `__int__`, `__float__`, `__bool__`

5. **Протоколы коллекций**:
    - `__len__` — длина
    - `__getitem__`, `__setitem__`, `__delitem__` — доступ по индексу/ключу
    - `__contains__` — поддержка оператора `in`
    - `__iter__`, `__next__` — итерация

6. **Протоколы вызова и контекста**:
    - `__call__` — вызов объекта как функции
    - `__enter__`, `__exit__` — контекстные менеджеры (оператор `with`)

7. **Дескрипторы и атрибуты**:
    - `__getattr__`, `__setattr__`, `__delattr__` — доступ к атрибутам
    - `__getattribute__` — перехват всех обращений к атрибутам
    - `__dir__` — список атрибутов для `dir(obj)`

8. **Сериализация**:
    - `__getstate__`, `__setstate__` — для pickle
    - `__reduce__`, `__reduce_ex__` — кастомная сериализация


- [Содержание](#содержание)

---

# **Инвариантность и ковариантность**

## **Junior Level**

Инвариантность и ковариантность — это понятия из теории типов, которые описывают, как отношения между типами сохраняются
при использовании в обобщённых (generic) конструкциях.

Представьте, что у вас есть коробки для фруктов. У вас есть:

- Коробка для любых фруктов (`Box[Fruit]`)
- Коробка для яблок (`Box[Apple]`), где `Apple` — подтип `Fruit`

**Ковариантность** означает, что отношение "является подтипом" сохраняется и для коробок: если `Apple` — подтип `Fruit`,
то `Box[Apple]` — подтип `Box[Fruit]`. То есть коробку яблок можно использовать там, где ожидается коробка фруктов.

**Инвариантность** означает, что эти типы не связаны: `Box[Apple]` и `Box[Fruit]` — совершенно разные, независимые типы.
Коробку яблок нельзя использовать вместо коробки фруктов, и наоборот.

**Контравариантность** (обратное ковариантности) — когда отношение обращается: если `Apple` — подтип `Fruit`, то
`Box[Fruit]` — подтип `Box[Apple]`.

В Python эти концепции важны при работе с типизацией (type hints), особенно при использовании обобщённых типов вроде
`List[T]`, `Callable[[T], R]`.

## **Middle Level**

В контексте Python и статической типизации:

1. **Ковариантность (covariant)**:
    - Обозначается `+T` в определении типа
    - Если `A` — подтип `B`, то `Generic[A]` — подтип `Generic[B]`
    - Пример: `typing.Sequence` ковариантен по типу элемента

2. **Контравариантность (contravariant)**:
    - Обозначается `-T`
    - Если `A` — подтип `B`, то `Generic[B]` — подтип `Generic[A]` (обратное отношение)
    - Пример: `typing.Callable` контравариантен по типам аргументов

3. **Инвариантность (invariant)**:
    - Наиболее распространённый случай по умолчанию
    - Никаких отношений между `Generic[A]` и `Generic[B]`, даже если `A` и `B` связаны
    - Пример: `List[T]` инвариантен (по соображениям безопасности)

4. **Проблема изменяемости**:
    - Ковариантность безопасна для "read-only" типов (`Sequence`, `Iterable`)
    - Изменяемые типы (`List`, `Dict`) должны быть инвариантны для безопасности типов
    - Иначе можно было бы добавить апельсин в список яблок

5. **Практическое применение в Python**:
   ```python
   from typing import TypeVar, Generic, List, Sequence
   
   class Fruit: pass
   class Apple(Fruit): pass
   
   # Ковариантный тип
   T_co = TypeVar('T_co', covariant=True)
   class ReadOnlyBox(Generic[T_co]): pass
   
   # Контравариантный тип  
   T_contra = TypeVar('T_contra', contravariant=True)
   class Consumer(Generic[T_contra]): pass
   ```

6. **Проверка типов (mypy, pyright)**:
    - Статические анализаторы используют информацию о вариативности для проверки безопасности типов
    - Ошибки вариативности — частые причины ошибок типизации


- [Содержание](#содержание)

---

# **Декораторы классов и методов**

## **Junior Level**

Декораторы классов и методов — это специальные функции или классы, которые позволяют модифицировать поведение классов и
методов без изменения их исходного кода. Синтаксически они выглядят как `@декоратор` перед определением класса или
метода.

Для классов декоратор принимает класс, изменяет его (добавляет/удаляет/изменяет методы или атрибуты) и возвращает
модифицированный класс. Например, декоратор может автоматически добавить логирование ко всем методам класса.

Для методов декоратор принимает метод, оборачивает его дополнительной логикой и возвращает новую функцию. Классические
примеры: `@staticmethod`, `@classmethod`, `@property`. Также можно создавать собственные декораторы, например, для
проверки прав доступа, кэширования результатов или измерения времени выполнения.

## **Middle Level**

1. **Декораторы методов**:
    - **Стандартные декораторы**: `@staticmethod`, `@classmethod`, `@property` (с `@setter`, `@deleter`)
    - **Пользовательские декораторы**: функции, принимающие функцию и возвращающие новую функцию
    - **Декораторы с параметрами**: требуют дополнительного уровня вложенности
    - `@functools.wraps(func)` для сохранения метаданных оригинальной функции

2. **Декораторы классов**:
    - Принимают класс, модифицируют его и возвращают (часто тот же класс, но изменённый)
    - Могут добавлять/удалять/изменять атрибуты и методы
    - Могут регистрировать класс в каком-либо реестре
    - Могут заменять класс другим классом (фабрика классов)

3. **Особенности декораторов методов в классах**:
    - При декорировании метода в классе декоратор получает функцию, а не связанный метод
    - Во время создания класса метод существует как обычная функция в пространстве имён класса
    - При доступе через экземпляр (`instance.method`) срабатывает протокол дескриптора

4. **Порядок применения декораторов**:
    - Декораторы применяются снизу вверх (ближайший к определению применяется первым)
    - `@a @b def method(): ...` → `method = a(b(method))`
    - Для классов аналогично

5. **Использование в тестировании**:
    - `@unittest.mock.patch` для мокирования атрибутов и методов
    - `@pytest.fixture` для создания фикстур
    - `@pytest.mark.parametrize` для параметризации тестов
    - `@pytest.mark.skip` для пропуска тестов

6. **Метаклассы vs декораторы классов**:
    - Метаклассы работают на уровне создания класса
    - Декораторы классов работают после создания класса
    - Часто одну задачу можно решить как метаклассом, так и декоратором

## **Senior Level (Философские глубины, теория и метапрограммирование)**

**1. Философия декораторов: трансформация сущностей**

**Декоратор как морфизм в категории программ**:

- Категория: объекты — функции/классы, морфизмы — декораторы
- Композиция декораторов: ассоциативность (но не коммутативность!)
- Идентичный морфизм: `identity_decorator = lambda x: x`

**Герменевтика декораторов**:

- Декоратор меняет интерпретацию (герменевтику) функции/класса
- Декоррированная сущность существует в двух смысловых слоях: оригинальный и декорированный
- Герменевтический круг: чтобы понять декорированную функцию, нужно понять декоратор, но чтобы понять декоратор, нужно
  понять его применение

**Эпистемология скрытого поведения**:

- Декораторы скрывают дополнительное поведение
- Это создаёт эпистемическую асимметрию: вызывающий код не знает о декораторе
- Нарушение принципа наименьшего удивления: неожиданное поведение из-за скрытых декораторов

**2. Теория трансформаций и рефлексия**

**Структурная трансформация AST**:

- Декораторы работают на уровне Abstract Syntax Tree
- В момент компиляции декоратор получает объект функции/класса
- Возможность анализировать и модифицировать структуру

**Интроспекция и метаданные**:

- `__wrapped__` атрибут для доступа к оригинальной функции
- `__qualname__`, `__name__`, `__doc__` — важность сохранения
- `inspect.signature` для анализа сигнатуры

**Рефлексивные декораторы**:

- Декораторы, которые анализируют и модифицируют код на основе его структуры
- Пример: автоматическое логирование всех вызовов метода

**3. Время и темпоральность декораторов**

**Время применения декораторов**:

- Декораторы методов применяются при создании класса (во время компиляции модуля)
- Декораторы классов применяются после создания класса, но до создания экземпляров
- Это создаёт сложную временную структуру

**Декораторы как временные модификаторы**:

- Изменяют поведение во времени (кэширование, таймауты, повторные попытки)
- `@lru_cache` — модификация на оси времени выполнения
- `@retry` — работа со временем неудач

**Телеология декораторов**:

- Декораторы добавляют цель (телос) к функции
- Оригинальная функция: вычислить значение
- С декоратором: вычислить значение + залогировать + кэшировать + ...

**4. Декораторы и теория типов**

**Эффекты и декораторы**:

- Декораторы добавляют эффекты к чистым функциям
- Логирование, ввод-вывод, исключения — все это эффекты
- Теория: монады и алгебраические эффекты

**Типизация декорированных функций**:

- Проблема: тип декорированной функции не совпадает с типом оригинальной
- Решение: generic декораторы с `ParamSpec` и `TypeVar` (PEP 612)
- `typing.Concatenate` для изменения сигнатуры

**Высшие kinded types (HKT)**:

- Для типобезопасных декораторов нужны типы высшего порядка
- `Functor`, `Monad` — паттерны, которые могут быть реализованы как декораторы

**5. Социальные и культурные аспекты**

**Декораторы как социальные конвенции**:

- Определённые декораторы становятся стандартными в сообществе
- `@app.route` в Flask, `@login_required` в Django
- Язык фреймворка формируется через декораторы

**Власть и контроль через декораторы**:

- Декораторы могут навязывать архитектурные решения
- Фреймворки используют декораторы для контроля потока выполнения
- "Магия" декораторов vs явность

**Обучение декораторам**:

- Когнитивная сложность понимания декораторов
- Метафоры для обучения: обёртывание подарка, слои лука

**6. Проблемы и антипаттерны**

**Накопление декораторов**:

- Цепочка декораторов создаёт глубокую вложенность
- Проблема отладки: стек вызовов усложняется
- Производительность: каждый декоратор добавляет overhead

**Нарушение сигнатуры**:

- Декораторы могут неправильно обрабатывать `*args`, `**kwargs`
- Проблема с `inspect.signature` и аннотациями типов

**Побочные эффекты при декорировании**:

- Декораторы выполняются при импорте модуля
- Неожиданные побочные эффекты на этапе загрузки
- Проблемы с цикличными импортами

**7. Декораторы в распределённых системах**

**Декораторы для микросервисов**:

- `@circuit_breaker` — автоматическое отключение при сбоях
- `@rate_limit` — ограничение частоты запросов
- `@distributed_lock` — распределённые блокировки

**Телеметрия и observability**:

- `@trace` — трассировка вызовов
- `@metrics` — сбор метрик
- `@log` — структурированное логирование

**Безопасность**:

- `@authenticated` — проверка аутентификации
- `@authorized` — проверка прав доступа
- `@encrypted` — автоматическое шифрование/дешифрование

**8. Декораторы в тестировании (Senior AQA)**

**Мета-тестирование декораторов**:

- Тестирование самих декораторов
- Проверка сохранения сигнатуры и метаданных
- Тестирование побочных эффектов

**Декораторы для тестов**:

- `@pytest.fixture` — инъекция зависимостей
- `@unittest.mock.patch` — мокирование
- `@freeze_time` — управление временем в тестах

**Параметризация тестов**:

- Декораторы для генерации тестовых случаев
- `@pytest.mark.parametrize` как декларативный подход
- Динамическая параметризация на основе данных

**Изоляция тестов**:

- `@isolate` — изоляция тестов друг от друга
- `@clean_database` — очистка базы данных перед тестом
- `@mock_external_services` — автоматическое мокирование внешних сервисов

**Производительность тестов**:

- `@slow` — маркировка медленных тестов
- `@skip_if` — условный пропуск тестов
- `@timeout` — ограничение времени выполнения теста

**9. Будущее декораторов**

**Декораторы времени компиляции**:

- Возможность применения декораторов на этапе компиляции
- Более глубокие трансформации кода
- Интеграция с mypy для статического анализа

**Декораторы, генерируемые AI**:

- Автоматическое предложение декораторов на основе анализа кода
- Умные декораторы, адаптирующиеся к контексту

**Визуальные декораторы**:

- Декораторы для UI/UX
- Автоматическая генерация интерфейсов на основе декораторов

**Квантовые декораторы**:

- Декораторы для квантовых вычислений
- `@quantum_circuit` — преобразование классической функции в квантовую схему

**10. Философский синтез: декораторы как фундаментальный паттерн**

**Декораторы в природе**:

- Биологические аналоги: эпигенетические модификации (декор ДНК)
- Химические катализаторы как "декораторы" реакций

**Декораторы в культуре**:

- Украшение как фундаментальная человеческая практика
- Ритуалы и церемонии как "декораторы" повседневных действий

**Декораторы в познании**:

- Когнитивные искажения как "декораторы" восприятия
- Языковые конструкции как декораторы мыслей

- [Содержание](#содержание)

---

# **Множественное наследование и MRO**

## **Junior Level**

Множественное наследование — это возможность класса наследоваться от нескольких родительских классов одновременно. В
Python вы можете указать несколько классов в скобках при определении класса: `class Child(Parent1, Parent2, Parent3):`.
Это позволяет комбинировать функциональность из разных источников.

Проблема возникает, когда несколько родительских классов имеют методы с одинаковыми именами. Какой из них должен быть
вызван? Для решения этой проблемы Python использует **MRO (Method Resolution Order)** — порядок разрешения методов. Это
алгоритм, который определяет последовательность, в которой Python ищет методы в иерархии классов.

MRO можно посмотреть с помощью атрибута `__mro__` или метода `mro()`. Например, `MyClass.__mro__` покажет кортеж классов
в порядке их поиска.

## **Middle Level**

1. **Алгоритм C3 Linearization**:
    - Алгоритм, используемый в Python для вычисления MRO (начиная с Python 2.3).
    - Гарантирует, что порядок будет **монотонным** (если класс A предшествует классу B в MRO класса C, то так будет и
      во всех подклассах C).
    - Учитывает порядок родителей в определении класса.

2. **Принципы MRO**:
    - **Локальный порядок приоритетов**: порядок классов в списке наследования важен.
    - **Сохранение прямых родительских классов**: родительский класс проверяется до своих собственных родителей.
    - **Монотонность**: если класс X предшествует классу Y в MRO класса C, то X предшествует Y в MRO любого подкласса C.

3. **Проблема ромбовидного наследования (diamond problem)**:
   ```python
   class A: pass
   class B(A): pass
   class C(A): pass
   class D(B, C): pass
   ```
   MRO для D будет: D → B → C → A (а не D → B → A → C → A, что было бы с повторением A).

4. **`super()` и MRO**:
    - `super()` использует MRO для определения следующего класса в цепочке.
    - Важно для кооперативного множественного наследования, когда методы вызывают `super()` для передачи управления
      следующему классу.

5. **Миксины (Mixins)**:
    - Классы, предназначенные для множественного наследования.
    - Не предназначены для самостоятельного использования.
    - Обычно добавляют конкретную функциональность (например, логирование, сериализацию).

6. **Практические рекомендации**:
    - Избегайте слишком сложных иерархий множественного наследования.
    - Используйте абстрактные классы и интерфейсы для определения контрактов.
    - Композиция часто предпочтительнее множественного наследования.

## **Senior Level**

**1. Философские и онтологические основания множественного наследования**

**Проблема идентичности при множественном наследовании**:

- Если объект наследует от `Animal` и `Machine`, что он такое? Гибридная идентичность.
- Философская проблема: можно ли быть одновременно двумя разными сущностями?
- В программировании: объект обладает всеми характеристиками родителей, но это создаёт семантические конфликты.

**Эпистемология MRO**:

- MRO как способ организации знания о классах.
- Как мы узнаём, какой метод будет вызван? Через алгоритм C3, который формализует это знание.
- Эпистемическая неопределённость: без понимания MRO программист не знает поведение объекта.

**Теория прототипов в контексте множественного наследования**:

- Вместо классической иерархии — множественные прототипы.
- Python ближе к классической модели, но с элементами прототипности (динамическое изменение классов).

**2. Математические основы алгоритма C3**

**Формальное определение**:
Пусть C — класс с прямыми родителями B₁, B₂, ..., Bₙ.
MRO C вычисляется как:

```
L[C] = [C] + merge(L[B₁], L[B₂], ..., L[Bₙ], [B₁, B₂, ..., Bₙ])
```

где merge:

1. Берёт первый элемент первого списка.
2. Если этот элемент не встречается в хвостах других списков (хвост — все элементы кроме первого), добавляет его в
   результат и удаляет из всех списков.
3. Повторяет, пока списки не станут пустыми.
4. Если на каком-то шаге нет подходящего элемента, алгоритм завершается с ошибкой (несовместимая иерархия).

**Теорема о монотонности**:
Алгоритм C3 гарантирует, что если класс A предшествует классу B в MRO класса C, то A предшествует B в MRO любого
подкласса C.

**Доказательство корректности**:
Основано на том, что merge сохраняет локальный порядок и устраняет дубликаты.

**3. Когнитивная сложность восприятия множественного наследования**

**Когнитивная нагрузка**:

- Разработчик должен удерживать в голове всю иерархию.
- MRO добавляет дополнительную сложность.
- Закон Миллера (7±2 элемента в рабочей памяти) часто нарушается.

**Ментальные модели и метафоры**:

- Разные разработчики используют разные метафоры для понимания MRO.
- Некоторые видят это как цепочку делегирования, другие — как граф.

**Ошибки восприятия**:

- Ожидание, что порядок поиска соответствует порядку в определении класса (не всегда верно).
- Заблуждение, что `super()` всегда вызывает родительский класс (вызывает следующий в MRO).

**4. Социальные и культурные аспекты**

**Сообщество Python и MRO**:

- Алгоритм C3 был принят после долгих дискуссий.
- Сообщество предпочитает композицию множественному наследованию.
- Миксины как социальный контракт: классы, предназначенные только для наследования.

**Кросс-языковые сравнения**:

- Java: нет множественного наследования классов (только интерфейсы).
- C++: множественное наследование с виртуальным наследованием.
- Ruby: миксины через модули.
- Каждый подход отражает философию языка.

**5. Временные аспекты MRO**

**MRO как порядок во времени**:

- Последовательность вызовов методов во времени.
- Кооперативные методы с `super()` создают временную цепочку.

**Изменение MRO во время выполнения**:

- Технически возможно изменить `__bases__` класса.
- Но это опасно и нарушает монотонность.
- Метапрограммирование vs стабильность.

**6. Метафизика `super()`**

**`super()` не вызывает родительский класс**:

- Возвращает proxy-объект, который делегирует следующему классу в MRO.
- Это важно при множественном наследовании.

**Кооперативное наследование как социальный договор**:

- Все методы в цепочке должны вызывать `super()`.
- Нарушение этого договора ломает цепочку.
- Аналогия: эстафета, где каждый передаёт палочку следующему.

**7. Проблемы и патологии**

**Класс-гробница (Tombstone class)**:

- Класс, который должен быть последним в MRO, но из-за порядка наследования оказывается не там.
- Решение: тщательное проектирование иерархии.

**Мёртвый код в MRO**:

- Методы, которые никогда не будут вызваны из-за MRO.
- Статический анализ может выявлять такие случаи.

**Циклическое наследование**:

- Python обнаруживает это на этапе создания класса.
- Вызывает `TypeError`.

**8. Тестирование и MRO (Senior AQA)**

**Тестирование порядка вызовов**:

- Мокирование методов для проверки последовательности вызовов.
- Использование `unittest.mock` для отслеживания.

**Тестирование миксинов**:

- Тестирование миксинов в изоляции.
- Тестирование в комбинации с разными классами.
- Проверка, что миксин не ломает существующую функциональность.

**Проверка MRO**:

- Написание тестов, которые проверяют `__mro__` для критичных классов.
- Проверка, что важные классы находятся в правильной позиции.

**Тестирование `super()`**:

- Проверка, что все методы правильно вызывают `super()`.
- Тестирование edge cases: что происходит, если метод не вызывает `super()`?

**Интеграционные тесты для сложных иерархий**:

- Тестирование всего пути вызова в сложной иерархии.
- Проверка семантической корректности (не только синтаксической).

**9. Будущее множественного наследования**

**Декораторы как альтернатива**:

- Декораторы классов могут добавлять функциональность без наследования.
- Более гибко, но менее стандартизировано.

**Трейты и типажи**:

- Заимствование из Scala, Rust.
- Более строгая система, чем миксины.

**Статический анализ и AI**:

- Инструменты для автоматического обнаружения проблем с MRO.
- AI-ассистенты, предлагающие рефакторинг.

**Эволюция Python**:

- Новые возможности в typing (`Protocol`, `Final`).
- Возможны изменения в механизме наследования.

**10. Множественное наследование в распределённых системах**

**Наследование от удалённых классов**:

- Теоретическая возможность в системах типа CORBA.
- Практические проблемы: задержки, надёжность.

**Микросервисы и наследование**:

- Наследование от классов разных сервисов проблематично.
- Альтернатива: композиция через API.

- [Содержание](#содержание)

---

# **ABC**

## **Junior Level*

Представьте, что вы архитектор, который разрабатывает проект дома (интерфейс, контракт). Вы создаете подробный чертеж,
где указано: "здесь должна быть дверь", "здесь должно быть не менее двух окон", "обязательно должна быть крыша". Однако
сам чертеж — это не дом, по нему нельзя жить. Вы просто задаете стандарт, которому должен следовать любой построенный по
этому проекту дом.

В Python ABC (Abstract Base Classes) — это и есть такие "чертежи" для классов. Это специальный механизм, который
позволяет нам декларативно сказать: "Любой класс, который будет моим наследником, ОБЯЗАН реализовать конкретные методы (
например, `save()`, `load()`, `validate()`)". Если наследник не реализует все обязательные методы, Python не даст
создать его экземпляр и сразу укажет на ошибку. Это делает код предсказуемым, помогает избежать ошибок в рантайме и
четко документирует ожидания от объекта. Для QA инженера это мощный инструмент для стандартизации тестовых утилит,
плагинов и проверки контрактов в системе.

# ## **Middle Level**

На техническом уровне ABC — это классы, наследующиеся от встроенного класса `abc.ABC` и использующие декоратор
`@abstractmethod` (а также `@abstractclassmethod`, `@abstractstaticmethod`, `@abstractproperty`) для пометки абстрактных
методов. Ключевые аспекты:

1. **Инстанцирование:** Попытка создать экземпляр класса, у которого есть хотя бы один не переопределенный
   `@abstractmethod`, немедленно вызовет исключение `TypeError`. Это проверка происходит в момент вызова `__new__`
   класса, еще до вызова `__init__`.

2. **Регистрация (Registration):** Помимо явного наследования, класс может быть "зарегистрирован" как виртуальный
   подкласс ABC с помощью метода `register()`. После этого `isinstance()` и `issubclass()` будут возвращать для него
   `True`, но при этом **не проводится проверка на наличие абстрактных методов!** Это "мягкий" способ заявить о
   соответствии интерфейсу, полезный для интеграции сторонних или унаследованных классов.

3. **`__subclasshook__`:** Это специальный метод класса (`@classmethod`), который позволяет кастомизировать логику
   проверки `issubclass()`. Метод может проверять наличие у класса требуемых атрибутов или методов (через `hasattr`), а
   не только факт прямого наследования или регистрации. Это самый гибкий и "питонический" способ определения виртуальных
   подклассов.

4. **Встроенные ABCs в `collections.abc`:** Модуль предоставляет богатейший набор готовых абстрактных классов для
   стандартных протоколов: `Iterable`, `Iterator`, `Container`, `Sequence`, `Mapping`, `Callable` и т.д. Использование
   `isinstance(obj, collections.abc.Sequence)` вместо `isinstance(obj, list)` делает код полиморфным и независимым от
   конкретной реализации.

Для AQA: Мы используем ABC для создания базовых классов тест-кейсов, абстрактных проверок (`Validator`), провайдеров
тестовых данных, плагинов для отчетности. Это гарантирует, что любой новый модуль, добавленный в фреймворк, будет иметь
ожидаемую структуру.

# ## **Senior Level**

Здесь мы говорим о метаклассах, тонкостях разрешения методов (MRO) и системном дизайне.

1. **Метакласс `ABCMeta`:** За кулисами `abc.ABC` — это просто удобный сахар. Настоящая магия происходит в метаклассе
   `abc.ABCMeta`. Именно он отвечает за:
    * Накопление множества абстрактных методов в атрибуте `__abstractmethods__` (это `frozenset`).
    * **Перехват момента создания класса.** При создании нового класса (наследника ABC) `ABCMeta.__new__` проверяет,
      остались ли в нем элементы из `__abstractmethods__`. Если да — инстанцирование блокируется.
    * Реализацию методов `register()` и `__subclasshook__`.

2. **Динамика `__abstractmethods__`:** Это не статический атрибут. При наследовании Python строит MRO (Method Resolution
   Order) для нового класса и динамически вычисляет итоговый набор `__abstractmethods__` как разность между абстрактными
   методами всех родителей и уже реализованными методами в самом классе. Это позволяет создавать сложные иерархии с
   частичными реализациями.

3. **`__instancecheck__` и `__subclasscheck__`:** Метакласс `ABCMeta` переопределяет эти специальные методы. Именно
   благодаря этому `isinstance()` и `issubclass()` работают с виртуальными подклассами. `ABCMeta.__subclasscheck__`
   последовательно проверяет: является ли класс зарегистрированным виртуальным подклассом, есть ли в MRO целевого класса
   данный ABC, и наконец, вызывается ли пользовательский `__subclasshook__`. Это цепочка ответственности (Chain of
   Responsibility) на уровне метаклассов.

4. **Для AQA (Кровь и Кишки):**
    * **Проектирование фреймворка:** Senior AQA использует ABC для создания *строгих контрактов* плагинов. Например,
      метакласс `ABCMeta` гарантирует, что забытый метод `teardown` в кастомном плагине для фикстур будет обнаружен не
      когда тесты упадут в CI, а в момент импорта модуля с плагином.
    * **Валидация архитектуры:** В интеграционных и системных тестах мы можем проверять, что фабрики возвращают объекты,
      соответствующие не конкретному классу, а абстрактному протоколу (`Mapping`, `AsyncIterator`). Это защищает тесты
      от изменений реализации.
    * **Моки и стабы:** При использовании `unittest.mock` или создании собственных заглушек ABC — идеальная основа. Мы
      можем создать абстрактный класс `DataService`, написать против него тесты, а затем подменить его строгим моком (
      `Mock(spec=AbstractDataService)`), который будет валидировать сигнатуры всех вызовов.
    * **Обратная сторона:** Слепо наследоваться от ABC для каждого интерфейса — овердизайн. Python — язык с утиной
      типизацией. Часто достаточно использовать `__subclasshook__`, который проверяет наличие методов, или вообще
      полагаться на протоколы (PEP 544 – `typing.Protocol`), которые появились как альтернатива ABC для структурной
      проверки типов. Senior-инженер должен понимать, когда `Protocol` (проверка статическим анализатором)
      предпочтительнее `ABC` (проверка в рантайме).

- [Содержание](#содержание)

---

# **Протокол (Protocol)**

## **Junior Level*

Представьте, что вы описываете не конкретный предмет, а его роль. Например, "то, что можно включить". Под это описание
подходит и лампа, и компьютер, и телевизор — у всех есть кнопка "включить". Вам неважно, что это за устройство внутри;
важно, что оно поддерживает операцию "включения".

В Python **Протокол (Protocol)** — это именно такое формальное описание роли или поведения. Он говорит: "Если у объекта
есть вот такие методы и атрибуты, то он автоматически считается подходящим для определенной цели, даже если он не был
изначально для этого предназначен". Это продвинутая, "официальная" версия принципа утиной типизации ("если ходит как
утка и крякает как утка, то это утка"). В отличие от Abstract Base Class (ABC), где класс должен явно заявить "я
наследуюсь от этого чертежа", протокол работает на уровне "если ты имеешь эти черты, то ты соответствуешь". Для QA
инженера это мощный инструмент для описания гибких требований к тестируемым компонентам, мокам и заглушкам, не
завязываясь на конкретные классы в коде.

# ## **Middle Level**

Технически `Protocol` — это конструкция системы типизации, определенная в PEP 544 и доступная в модуле `typing`. Его
ключевые аспекты:

1. **Структурная, а не номинативная типизация:** Класс соответствует протоколу, если его **структура** (набор методов и
   атрибутов с правильными сигнатурами) совпадает с протоколом. Ему не нужно явно наследоваться от протокола. Это
   фундаментальное отличие от ABC, где требуется явное номинативное объявление (наследование или регистрация).

2. **Синтаксис и `@runtime_checkable`:** Протокол определяется как класс, наследующийся от `typing.Protocol`. Его методы
   часто помечаются как абстрактные, используя `...` (ellipsis) в теле. По умолчанию протоколы используются **только
   статическими анализаторами типов** (mypy, pyright). Чтобы позволить проверку соответствия протоколу во время
   выполнения (`isinstance(obj, MyProtocol)`), протокол необходимо декорировать `@typing.runtime_checkable`. Однако
   такая проверка ограничена: она проверяет только **наличие** указанных атрибутов, но не их сигнатуры или типы.

3. **Generic и вариативность:** Протоколы могут быть параметризованы (Generic), что позволяет описывать типы, зависящие
   от других типов (например, `Iterable[T]`). Они также поддерживают ковариантность и контравариантность через параметры
   `covariant=True`/`contravariant=True`, что критично для точного описания отношений между сложными типами.

4. **Встроенные протоколы:** Модуль `typing` и `collections.abc` предоставляют множество встроенных протоколов:
   `SupportsInt`, `SupportsBytes`, `ContextManager`, `Iterable`, `Sized` и др. Их использование в аннотациях делает код
   гораздо более выразительным и безопасным.

Для AQA: Протоколы позволяют формально описывать ожидания от зависимостей в тестах. Мы можем объявить протокол
`DataFetcher` с методом `fetch()` и затем написать тест, который работает с любым объектом, соответствующим этому
протоколу — будь то реальный объект, мок или заглушка. Это снижает связность тестов с конкретными реализациями.

# ## **Senior Level**

Здесь мы погружаемся в механику структурной типизации, взаимодействие с системой типов и метаклассами.

1. **Двойственная природа `Protocol`:** `Protocol` — это одновременно и конструкция для статического анализа, и
   полноценный объект Python. На уровне метаклассов `typing.Protocol` наследуется от `abc.ABCMeta`. Это делает его
   метаклассом, который объединяет возможности ABC (вроде `register()`) с новой семантикой структурной проверки.
   Декоратор `@runtime_checkable` модифицирует класс протокола, добавляя корректную реализацию `__instancecheck__`.

2. **Механика `__instancecheck__` при `@runtime_checkable`:** Когда используется `isinstance(obj, MyProtocol)`,
   вызывается `MyProtocol.__instancecheck__()`. Его реализация, предоставленная декоратором, выполняет итерацию по
   аннотациям протокола (полученным через `get_type_hints()`) и для каждого атрибута выполняет проверку
   `hasattr(obj, attr_name)`. Это **поверхностная проверка существования**, без валидации типов, сигнатур или свойств
   дескрипторов. Она не может отличить метод от простого атрибута с тем же именем. Это ее ключевое и часто упускаемое из
   виду ограничение.

3. **Статический анализ vs Runtime:** Настоящая сила протоколов раскрывается в статическом анализаторе. Анализатор
   строит карту всех типов и для каждого места, где ожидается `ProtocolX`, ищет все классы, структура которых совместима
   с ним. Это включает проверку сигнатур методов, типов возвращаемых значений, свойств (property) и даже
   позиционных/ключевых аргументов. Протокол может требовать наличия определенного `@property` или `__slots__`.

4. **Протоколы и `__protocol_attrs__`:** На низком уровне протоколы могут использовать специальные атрибуты (например,
   `_is_protocol`, `_is_runtime_protocol`) для самоидентификации. Некоторые реализации статических анализаторов кэшируют
   информацию о протоколах для ускорения проверки.

5. **Для AQA (Кровь и Безумие):**
    * **Проектирование тестовых артефактов:** Senior AQA использует протоколы для создания *архитектурных тестов*. Мы
      можем определить протоколы, описывающие контракты критических модулей системы (например, `CacheBackend`,
      `MessageQueue`), а затем написать тест, который рефлексией проверяет, что все реализующие их классы в проекте
      действительно соответствуют не только по наличию методов, но и по сигнатурам (используя `inspect.signature`). Это
      следующий уровень после `@runtime_checkable`.
    * **Моки с глубокой валидацией:** Мы можем создать фабрику моков, которая на основе протокола динамически генерирует
      объект-заглушку с помощью `unittest.mock.Mock` или `unittest.mock.AsyncMock`, автоматически настраивая сигнатуры
      методов. Это дает автодополнение в IDE и предотвращает ошибки в тестах при изменении реального интерфейса.
    * **Инверсия зависимости в тестах:** Используя протоколы в аннотациях, мы явно декларируем, что наши тестовые
      хелперы или фикстуры зависят не от `ConcreteClassA`, а от `SupportsFeatureX`. Это делает тестовый код максимально
      устойчивым к рефакторингу продакшен-кода.
    * **Ограничения и тонкости:** Важно понимать, что `isinstance` с `@runtime_checkable` — это компромисс. Он не
      заменяет статический анализ. Также протоколы не могут требовать наличия специальных методов, которые ищутся
      напрямую в классе (через `__class__`), а не в экземпляре (например, `__len__` является методом класса, но
      `hasattr(instance, '__len__')` часто возвращает `True` из-за наследования). Для таких случаев все еще могут
      потребоваться ABC или прямое использование `issubclass`.

- [Содержание](#содержание)

---

# **Паттерны проектирования**

## **Junior Level*

Паттерны проектирования — это проверенные временем решения типовых проблем в разработке программного обеспечения. Можно
сравнить их с архитектурными чертежами для строительства: вместо того чтобы каждый раз изобретать велосипед, опытные
архитекторы используют готовые схемы организации кода, которые уже доказали свою эффективность для конкретных ситуаций.

Это не готовые куски кода, а скорее концепции или шаблоны мышления о том, как структурировать взаимодействие между
компонентами системы. Например, "Фабрика" — это паттерн для создания объектов, не привязываясь к их конкретным классам,
а "Наблюдатель" описывает, как один объект может уведомлять множество других о произошедших изменениях.

Для AQA инженера понимание паттернов критически важно по нескольким причинам: во-первых, чтобы понимать архитектуру
тестируемого приложения и находить в ней слабые места; во-вторых, чтобы проектировать собственные тестовые фреймворки,
которые будут гибкими, расширяемыми и поддерживаемыми; в-третьих, чтобы говорить с разработчиками на одном языке при
обсуждении дизайна и потенциальных проблем.

# ## **Middle Level**

С технической точки зрения, паттерны проектирования (особенно из классического каталога GoF — "Банды четырех") в Python
часто реализуются с учетом уникальных особенностей языка.

1. **Идиоматичность Python:** Многие классические паттерны в Python могут быть реализованы проще и элегантнее, чем в
   статически типизированных языках. Например:
    * **Стратегия (Strategy):** Вместо создания иерархии классов можно просто передавать callable-объекты (функции,
      лямбды, экземпляры с `__call__`).
    * **Декоратор (Decorator):** Прямо соответствует синтаксическому декоратору Python, который является "обёрткой"
      вокруг функции или метода, модифицирующей его поведение.
    * **Адаптер (Adapter):** Часто реализуется не через наследование, а через композицию и магический метод
      `__getattr__` для перенаправления вызовов.

2. **Категории паттернов:**
    * **Порождающие (Creational):** Решают задачу гибкого и контролируемого создания объектов (Синглтон, Фабрика,
      Строитель). В Python Синглтон часто реализуют через метакласс или модуль (сам модуль по своей природе — синглтон).
    * **Структурные (Structural):** Организуют композицию классов и объектов для образования более крупных структур (
      Адаптер, Мост, Компоновщик, Декоратор, Фасад).
    * **Поведенческие (Behavioral):** Определяют эффективные способы взаимодействия и распределения ответственности
      между объектами (Наблюдатель, Стратегия, Команда, Цепочка обязанностей, Состояние).

3. **Для AQA:**
    * **Page Object** — это специализированный структурный паттерн для UI-тестирования, по сути являющийся Фасадом,
      скрывающим детали HTML-структуры за удобным API.
    * **Фабрика** и **Строитель (Builder)** незаменимы для создания сложных тестовых данных и фикстур.
    * **Наблюдатель (Observer)** или **Издатель-Подписчик (Pub/Sub)** лежит в основе систем событийного логирования или
      сбора результатов тестов в реальном времени.
    * **Прокси (Proxy)** и **Заместитель (Mock/Stub)** — основа библиотек для мокирования (unittest.mock), позволяющих
      подменять реальные зависимости в тестах.

4. **Dependency Injection (Внедрение зависимостей):** Хотя формально не входит в каталог GoF, это ключевой архитектурный
   паттерн, который в Python часто реализуется просто через передачу зависимостей в конструктор (Constructor Injection),
   без использования тяжеловесных фреймворков. Это краеугольный камень тестируемого дизайна.

# ## **Senior Level**

На этом уровне мы рассматриваем паттерны не как рецепты, а как отражение фундаментальных принципов проектирования (
SOLID, DRY, LoD) и глубоко анализируем их реализацию через механизмы Python.

1. **Метапаттерны и Python-идиомы:**
    * **Синглтон:** Реализация через метакласс, переопределяющий `__call__`, чтобы всегда возвращать один экземпляр. Но
      истинный Python-way — это *borg-паттерн* (он же "монограф"): все экземпляры разделяют одно состояние через общий
      атрибут `__dict__`. Это дает преимущества синглтона без жесткой привязки к единственному экземпляру.
    * **Декоратор:** Под капотом `@decorator` — это синтаксический сахар для `func = decorator(func)`. Настоящая мощь в
      том, что декоратор может быть классом с `__call__` (сохраняющим состояние между вызовами) или функцией,
      возвращающей обертку. Декораторы с аргументами — это функция, возвращающая декоратор, который возвращает обертку.
    * **Наблюдатель:** Помимо классической реализации со списком callback'ов, в Python можно использовать
      `weakref.WeakSet` для автоматического удаления "мертвых" подписчиков и предотвращения утечек памяти. Или применить
      механизм дескрипторов (через `property.setter`) для автоматической нотификации при изменении атрибута.

2. **Паттерны и тестируемость:**
    * **Инверсия зависимостей (DIP)** через абстракции (ABC или Protocol) — это главный паттерн, делающий код пригодным
      для автотестов. Код, написанный с нарушением DIP, будет сопротивляться изоляции.
    * **Шаблонный метод (Template Method)** в Python часто заменяется на *Hollywood Principle* ("не звоните нам, мы вам
      позвоним") через передачу хуков (hook methods) в виде callable-объектов, что делает его более гибким и
      тестируемым.
    * **Цепочка обязанностей (Chain of Responsibility)** может быть реализована через корутины (generator-based) или
      асинхронные цепочки (async/await), что особенно актуально для тестирования middleware в веб-приложениях.

3. **Антипаттерны и "запахи кода":**
   Senior AQA должен уметь распознавать, когда паттерн применен неправильно (over-engineering):
    * **Божественный объект (God Object)** под маской Фасада.
    * **Избыточное наследование** вместо композиции.
    * **Ненужный синглтон**, который превращается в глобальное состояние и убивает параллельный запуск тестов.
    * **Увлечение паттернами ради паттернов**, когда простая функция решает задачу лучше сложной иерархии классов.

4. **Паттерны для тестовых фреймворков (Blood & Guts):**
    * **Посетитель (Visitor)** для обхода AST (Abstract Syntax Tree) при статическом анализе тестового кода или
      генерации отчетов.
    * **Состояние (State)** для моделирования сложного жизненного цикла тестового запуска (INIT -> SETUP -> TEST ->
      TEARDOWN -> FINALIZE).
    * **Команда (Command)** для инкапсуляции отдельных шагов теста в объекты, которые можно логгировать, отменять или
      повторять.
    * **Итератор (Iterator)** и **Генератор (Generator)** — не просто паттерны, а встроенные протоколы Python, которые
      лежат в основе `pytest` фикстур с `yield` и ленивой загрузки тестовых данных.

5. **Дизайн через контракты (Design by Contract):**
   Это надстройка над паттернами. Используя ABC, Protocol, декораторы (для pre/post-conditions) и аннотации типов, мы
   можем формализовать ожидания от компонентов. Для AQA это прямой мост к созданию *автоматизированных проверок
   архитектуры* (архитектурных тестов), которые валидируют соблюдение паттернов и контрактов в кодовой базе.

- [Содержание](#содержание)

---

# **Композиция и агрегация**

## **Junior Level*

Композиция и агрегация — это два способа создания отношений между объектами в объектно-ориентированном программировании.
Обе описывают ситуацию, когда один объект содержит в себе другой, но с критически важным различием в силе связи и
управлении жизненным циклом.

Представьте, что вы строите дом. **Композиция** — это как комната в доме. Комната не существует отдельно от дома. Когда
дом сносят, комната исчезает вместе с ним. Объект-владелец (дом) полностью контролирует жизнь объекта-части (комнаты). *
*Агрегация** — это как мебель в доме. Стол, стул, диван существуют независимо от дома. Их занесли в дом, а потом могут
вынести в другой дом или на склад. Объект-владелец (дом) использует объект-часть (мебель), но не управляет его рождением
и смертью.

В разработке композиция означает, что при уничтожении основного объекта уничтожаются и все его составные части.
Агрегация означает, что объекты собраны вместе, но могут жить самостоятельно. Для QA инженера понимание этого различия
помогает проектировать тестовые фикстуры и моки, правильно управлять их жизненным циклом и понимать, какие зависимости
нужно создавать заново, а какие можно переиспользовать между тестами.

# ## **Middle Level**

С технической точки зрения, композиция и агрегация реализуются через **атрибуты класса**, но с разной семантикой
создания и владения.

1. **Реализация:**
    * **Композиция (Composition):** Объект-часть создается **внутри** конструктора (или иного метода) объекта-владельца.
      Владелец полностью инкапсулирует создание и, как правило, не предоставляет публичных методов для замены этой
      части. Часть реализуется как внутренний, приватный атрибут.
    * **Агрегация (Aggregation):** Объект-часть создается **вне** объекта-владельца и передается ему в качестве
      аргумента (чаще всего в конструктор). Владелец сохраняет ссылку на эту часть, но не управляет ее созданием.
      Объект-часть может быть общим (разделяемым) ресурсом.

2. **Жизненный цикл:**
    * При **композиции** жизненный цикл части жестко привязан к жизненному циклу целого. Когда объект-владелец
      удаляется (например, сборщиком мусора), удаляется и объект-часть, если на него больше нет ссылок.
    * При **агрегации** жизненные циклы независимы. Удаление владельца не влечет удаление части, так как на нее могут
      оставаться ссылки из других объектов.

3. **Для AQA:**
    * **Фикстуры в Pytest:** Композиция часто используется для создания сложных, вложенных фикстур, которые существуют
      только в рамках одной тестовой сессии или модуля и автоматически очищаются. Агрегация похожа на фикстуры с
      областью видимости `session` или `package`, которые создаются один раз и переиспользуются многими тестами.
    * **Тестовые данные:** Понимание, когда создавать новый экземпляр тестовых данных для каждого кейса (композиция), а
      когда использовать общий, предсозданный набор данных (агрегация), критично для скорости и изоляции тестов.
    * **Page Object:** Внутри Page Object может существовать композиция из элементов (например, `Button`, `InputField`),
      которые не имеют смысла вне контекста этой страницы. И агрегация — например, общий `Header` или `Footer`, которые
      могут быть переданы в несколько Page Object.

4. **Отличия от наследования:** И композиция, и агрегация — это альтернативы наследованию, предпочитаемые в современном
   дизайне ("предпочитай композицию наследованию"). Они обеспечивают большую гибкость и слабую связанность.

# ## **Senior Level**

На этом уровне мы рассматриваем композицию и агрегацию как фундаментальные паттерны управления зависимостями, памятью и
состоянием в сложных системах.

1. **Управление памятью и ссылочные циклы:**
    * При **композиции**, если используются двунаправленные ссылки (владелец знает о части, а часть хранит ссылку на
      владельца), возникает риск циклической ссылки. В Python до версии 3.4 это могло приводить к утечке памяти, так как
      сборщик мусора на основе подсчета ссылок (reference counting) не мог автоматически очистить такие объекты. Для
      решения использовались `weakref` (слабые ссылки) со стороны части на владельца. Современный сборщик мусора (GC) с
      алгоритмом обнаружения циклов справляется с этим, но `weakref` остаются важным инструментом для явного управления
      жизненным циклом и предотвращения непреднамеренного удержания объектов в памяти.
    * При **агрегации** риск циклических ссылок еще выше, так как объекты живут дольше и могут образовывать сложные
      графы зависимостей. Для тестовых фреймворков, которые создают тысячи объектов за прогон, это критично.

2. **Инверсия управления (IoC) и внедрение зависимостей (DI):** Агрегация — это механическая основа для DI. Объект не
   создает свои зависимости, а получает их извне. В продвинутых сценариях это реализуется через **IoC-контейнеры**,
   которые автоматически разрешают граф зависимостей, учитывая их жизненный цикл (синглтон, на запрос и т.д.). Для
   тестов это позволяет легко подменять реальные реализации на моки.

3. **Протоколы и абстракции:** Истинная мощь агрегации раскрывается, когда объект-владелец зависит не от конкретного
   класса части, а от абстракции (ABC или Protocol). Это превращает агрегацию в инструмент соблюдения **Dependency
   Inversion Principle (DIP)**. Владелец объявляет контракт, а часть (переданная извне) его выполняет. Это делает код
   идеально тестируемым.

4. **Для AQA (Кровь и Кишки):**
    * **Жизненный цикл тестовых контекстов:** При построении собственного тестового фреймворка senior-инженер должен
      спроектировать, как будут создаваться и уничтожаться контексты выполнения (test suites, fixtures, драйверы). Здесь
      композиция (вложенные контексты) и агрегация (разделяемые ресурсы, например, пул соединений с БД или экземпляр
      браузера) играют ключевую роль. Неправильный выбор приведет либо к утечкам памяти и состояния между тестами (
      недостаточная изоляция), либо к непомерному росту времени выполнения (избыточное создание).
    * **Мокирование в сложных графах:** Когда система представляет собой глубокий граф агрегированных объектов, простое
      мокирование одного интерфейса может быть недостаточно. Нужно понимать, какие связи являются композиционными (и,
      следовательно, их можно безопасно "вырвать" и заменить целиком), а какие — агрегационными (где замена одной части
      может повлиять на другие владельцы). Это важно для **shallow vs deep mocks**.
    * **Стратегия очистки (Tear-down):** При композиции очистка обычно каскадная и автоматическая. При агрегации —
      требуется явная стратегия: либо владелец не очищает часть, либо используется модель совместного владения с
      подсчетом ссылок, либо применяются паттерны вроде **Resource Acquisition Is Initialization (RAII)**, который в
      Python эмулируется через контекстные менеджеры (`__enter__`/`__exit__`) и `finally`-блоки.

5. **Отношение к другим паттернам:**
    * **Компоновщик (Composite)** — это структурный паттерн, часто строящийся на композиции, чтобы treat individual
      objects and compositions uniformly.
    * **Фасад (Facade)** — часто является агрегатором множества сложных подсистем, скрывая их за простым интерфейсом.
    * **Стратегия (Strategy)** и **Состояние (State)** — обычно агрегируются, так как должны быть заменяемы в runtime.

- [Содержание](#содержание)

---

# **Связность и связанность**

## **Junior Level*

Связность и связанность — это две фундаментальные концепции качества кода, которые описывают, насколько хорошо
организованы компоненты внутри модуля и насколько сильно они зависят друг от друга.

**Связность (Cohesion)** отвечает на вопрос: "Насколько хорошо элементы внутри одного модуля (класса, функции) связаны
между собой и работают для достижения одной четкой цели?". Высокая связность — это хорошо. Это означает, что модуль
делает что-то одно, целостное и понятное. Например, модуль `MathUtils`, который содержит только функции для
математических вычислений, обладает высокой связностью. А модуль `Utils`, который смешивает функции для работы со
строками, отправки email и логирования, имеет низкую связность — это "мусорная корзина", которую трудно поддерживать.

**Связанность (Coupling)** отвечает на вопрос: "Насколько сильно один модуль зависит от внутреннего устройства другого
модуля?". Низкая связанность — это хорошо. Это означает, что модули взаимодействуют через четкие, простые интерфейсы и
могут изменяться независимо друг от друга. Если же модуль "залезает" в приватные детали другого модуля, то любое
изменение в одном сломает другой — это высокая связанность (сильная связь).

Простой принцип: нужно стремиться к **высокой связности и низкой связанности**. Для QA инженера это напрямую влияет на
тестируемость: модули с высокой связностью легко тестировать изолированно, а при низкой связанности можно легко заменять
зависимости на моки.

# ## **Middle Level**

С технической точки зрения эти концепции реализуются через конкретные практики проектирования и имеют четкие индикаторы.

1. **Типы связности (от худшего к лучшему):**
    * **Случайная (Coincidental):** Элементы собраны вместе случайно (например, модуль `MiscHelpers`). Тестировать такое
      невозможно — нет единой ответственности.
    * **Логическая (Logical):** Элементы сгруппированы по категории (например, класс `DataProcessor`, который
      обрабатывает и CSV, и JSON, и XML). Тесты становятся размазанными и хрупкими.
    * **Временная (Temporal):** Элементы выполняются в одно время (например, функция `initialize_all()`, которая
      настраивает логгер, БД и кеш). Приводит к сложным фикстурам и непредсказуемым побочным эффектам в тестах.
    * **Процедурная (Procedural):** Элементы объединены последовательностью шагов (например, функция, которая читает
      файл, парсит данные и сохраняет в БД). В тестах приходится эмулировать всю последовательность.
    * **Коммуникационная (Communicational):** Элементы работают с одними и теми же данными (например, класс
      `CustomerReport`, который и вычисляет статистику, и форматирует отчет). Уже лучше, но еще есть смесь
      ответственностей.
    * **Последовательная (Sequential):** Выход одного элемента является входом для другого (конвейер). Хорошо для
      тестирования каждого шага.
    * **Функциональная (Functional):** Все элементы вносят вклад в выполнение одной четкой задачи — **идеал**. Класс
      `InvoiceCalculator` только считает, `InvoiceFormatter` только форматирует. Юнит-тесты пишутся легко и
      изолированно.

2. **Типы связанности (от худшего к лучшему):**
    * **Содержание (Content):** Модуль напрямую обращается к приватным данным или коду другого модуля (нарушение
      инкапсуляции). Тесты становятся хрупкими к любым изменениям.
    * **Общая (Common):** Модули используют глобальные данные. Для тестов это катастрофа — состояние тестов влияет друг
      на друга, параллельный запуск невозможен.
    * **Внешняя (External):** Модули зависят от внешнего формата данных или протокола. Требует сложных интеграционных
      тестов и моков.
    * **Управление (Control):** Один модуль управляет логикой другого (например, передача флагов). Усложняет
      тестирование, так как нужно проверять множество ветвлений.
    * **Структурная (Stamp):** Модуль принимает сложную структуру данных, но использует только часть полей. Создает
      скрытые зависимости и усложняет создание тестовых данных.
    * **Данных (Data):** Модули взаимодействуют через минимальный интерфейс (например, передача примитивных значений). *
      *Идеал** для тестирования — зависимости легко заглушить.

3. **Инструменты для достижения:**
    * **Принцип единственной ответственности (SRP)** ведет к высокой связности.
    * **Инверсия зависимостей (DIP)** через абстракции (ABC, Protocol) ведет к низкой связанности.
    * **Закон Деметры ("не разговаривай с незнакомцами")** снижает связанность.

# ## **Senior Level**

На этом уровне мы рассматриваем связность и связанность как системные силы, влияющие на энтропию кодовой базы, и
анализируем их через призму метапрограммирования и архитектурных паттернов.

1. **Связность и метаклассы:** Высокая связность на уровне архитектуры часто достигается через **модули** и **пакеты** с
   четкими границами ответственности. Однако Python позволяет нарушать эти границы через мощные механизмы интроспекции и
   метапрограммирования. Например, использование `__subclasses__()` для обнаружения всех наследников в кодовой базе
   создает **скрытую временную связность** — порядок инициализации модулей начинает иметь значение. Для тестов это может
   означать хрупкость и недетерминированное поведение при изменении порядка импортов.

2. **Связанность и система типов:** Введение **аннотаций типов** и **статической проверки** (mypy) формализует
   связанность на уровне контрактов. Однако чрезмерное использование конкретных типов (`List[ConcreteModel]`) вместо
   абстрактных (`Sequence[AbstractModel]`) создает **статическую связанность**, которая затрудняет подмену реализаций в
   тестах. Правильное применение **дженериков** и **протоколов** позволяет достичь "loose coupling by design".

3. **Динамическая природа Python как обоюдоострый меч:**
    * **Monkey Patching** (прямое изменение классов в рантайме) — это пиковый уровень **связанности по содержанию**. В
      тестах это иногда необходимо (например, для мокирования), но если это становится архитектурным приемом (плагины,
      расширения), то система превращается в "макаронный код", где откат изменений и понимание потока выполнения
      становятся невозможными. Ответственный senior AQA должен уметь предложить альтернативы на основе ABC, протоколов
      или механизма `__init_subclass__`.
    * **Импорт через `importlib.import_module()` и динамическое создание классов** (`type()` с тремя аргументами) могут
      использоваться для создания систем с низкой связанностью (например, plugin-архитектура). Но они же требуют сложных
      интеграционных тестов и могут маскировать проблемы связности.

4. **Циклические зависимости (Circular Dependencies):** Это крайнее проявление высокой связанности, когда модули
   импортируют друг друга. Python разрешает это до определенного предела (частично загруженные модули), но это убивает
   тестируемость и понятность. Решения:
    * **Рефакторинг:** Выделение общего абстрактного ядра (DIP).
    * **"Ленивые" импорты** внутри функций или методов. Это не решает проблему логической связанности, но обходит
      технические ограничения, создавая новые риски для тестов (импорт может упасть в неожиданный момент).
    * **Инверсия управления (IoC):** Зависимости не импортируются, а внедряются.

5. **Для AQA (Кровь, кишки и безумие):**
    * **Проектирование тестового фреймворка:** Senior AQA обязан проектировать фреймворк с максимально высокой
      связностью (отдельные модули для: управления драйверами, генерации данных, формирования отчетов, утилит) и
      минимальной связанностью между ними (общение через четкие API-интерфейсы, события или абстракции). Это позволяет
      подключать кастомные плагины, не ломая ядро.
    * **Анализ тестируемого приложения:** Понимание уровней связности/связанности позволяет **прогнозировать хрупкость
      тестов** и точки максимального риска. Класс с низкой связностью (God Object) будет иметь монструозные, медленные и
      нестабильные интеграционные тесты. Система с высокой связанностью будет требовать сложных, многоуровневых моков (
      deep mocks), которые сами по себе становятся burden для поддержки.
    * **Метрики и техдолг:** Использование инструментов статического анализа (например, radon для расчета связности) для
      выявления модулей с критически низкой связностью и предложения их рефакторинга как части **технического долга**.
      Senior AQA выступает здесь как инженер по качеству кода, а не только функционала.
    * **Тесты как индикатор архитектурных проблем:** Если для написания простого юнит-теста требуется поднимать половину
      системы (высокая связанность) или тест проверяет несвязанные вещи (низкая связность тестируемого модуля) — это
      красный флаг для архитектуры. Senior AQA должен уметь формализовать и донести это до команды разработки.

- [Содержание](#содержание)

---

# **SOLID**

## **Junior Level*

SOLID — это набор из пяти ключевых принципов проектирования в объектно-ориентированном программировании, которые
помогают создавать гибкий, поддерживаемый и расширяемый код. Каждая буква акронима представляет отдельный принцип:

**S - Single Responsibility Principle (Принцип единственной ответственности):** Каждый класс или модуль должен иметь
только одну причину для изменения. Он должен отвечать за одну конкретную задачу или ответственность.

**O - Open/Closed Principle (Принцип открытости/закрытости):** Классы должны быть открыты для расширения, но закрыты для
модификации. Это означает, что мы можем добавлять новое поведение, не меняя существующий код.

**L - Liskov Substitution Principle (Принцип подстановки Барбары Лисков):** Объекты в программе должны быть заменяемы на
экземпляры их подтипов без изменения корректности программы. Проще говоря: если что-то работает с родительским классом,
это должно работать и с любым его наследником.

**I - Interface Segregation Principle (Принцип разделения интерфейса):** Лучше иметь много специализированных
интерфейсов, чем один универсальный. Клиенты не должны зависеть от методов, которые они не используют.

**D - Dependency Inversion Principle (Принцип инверсии зависимостей):** Модули верхнего уровня не должны зависеть от
модулей нижнего уровня. Оба должны зависеть от абстракций. Абстракции не должны зависеть от деталей — детали должны
зависеть от абстракций.

Для QA инженера понимание SOLID критично, потому что код, написанный по этим принципам, гораздо легче тестировать: он
лучше изолирован, зависимости явные и заменяемые, а поведение предсказуемо.

# ## **Middle Level**

С технической точки зрения, каждый принцип SOLID реализуется через конкретные паттерны и механизмы Python.

1. **SRP (Single Responsibility):** На уровне модуля (файла .py) это означает, что модуль должен экспортировать
   логически связанный набор функций/классов. На уровне класса — класс должен иметь минимальное количество публичных
   методов, связанных одной целью. Нарушение SRP приводит к God-объектам, которые невозможно адекватно покрыть
   юнит-тестами. Метрика: если вы не можете назвать ответственность класса одним коротким предложением без союза "и" —
   принцип нарушен.

2. **OCP (Open/Closed):** В Python реализуется через:
    * **Наследование и полиморфизм:** Создание подклассов для добавления поведения.
    * **Композицию и паттерн "Стратегия":** Передача поведения через callable-объекты.
    * **Декораторы:** Обертывание функций без изменения их исходного кода.
    * **Абстрактные базовые классы (ABC):** Определение интерфейсов для расширения.
      Код, соответствующий OCP, позволяет добавлять новые тестовые сценарии и проверки без модификации ядра тестового
      фреймворка.

3. **LSP (Liskov Substitution):** Это контрактный принцип. В Python он реализуется через:
    * **Соблюдение сигнатур методов:** Подкласс не должен ужесточать предусловия (требовать больше) или ослаблять
      постусловия (обещать меньше).
    * **Сохранение инвариантов:** Состояние объекта после операций должно оставаться валидным с точки зрения базового
      класса.
    * **Исключения:** Подкласс не должен выбрасывать новые типы исключений, не являющиеся подтипами исключений базового
      класса.
      Нарушение LSP — классическая причина падения тестов при замене реализации. Для QA это означает, что моки и стабы
      должны точно следовать контракту реальных объектов.

4. **ISP (Interface Segregation):** В Python, где нет формальных интерфейсов, принцип реализуется через:
    * **Абстрактные базовые классы (ABC)** с минимальным набором абстрактных методов.
    * **Протоколы (Protocol),** которые позволяют описывать узкие, специфичные наборы методов.
    * **Миксины (Mixins)** — классы, предоставляющие конкретную функциональность.
      Следствие для тестирования: нам не нужно мокировать гигантские интерфейсы, достаточно реализовать только
      используемую часть.

5. **DIP (Dependency Inversion):** Практическая реализация:
    * **Зависимость от абстракций:** Вместо `ConcreteService` в коде указывается `AbstractService` (ABC или Protocol).
    * **Внедрение зависимостей (DI):** Зависимости передаются извне (через конструктор, сеттеры, контекст), а не
      создаются внутри класса.
    * **IoC-контейнеры** (в продвинутых случаях) для автоматического управления зависимостями.
      Это краеугольный камень тестируемости: он позволяет легко подменять реальные сервисы моками в тестах.

# ## **Senior Level**

На этом уровне мы рассматриваем SOLID как проявление более глубоких законов проектирования и анализируем их реализацию
через механизмы метапрограммирования, систему типов и динамическую природу Python.

1. **SRP и метаклассы:** Поистине строгий контроль за SRP может быть реализован через **метаклассы**. Метакласс может
   анализировать тело создаваемого класса (через неймспейс `namespace`), подсчитывая количество публичных методов, и
   выбрасывать исключение при создании класса, нарушающего принцип. Более того, можно использовать декораторы классов
   для автоматического разделения ответственности через **делегирование**, создавая прокси-объекты, которые распределяют
   вызовы методов по специализированным классам. Для QA это означает возможность создания "самотестирующихся"
   архитектурных правил.

2. **OCP и система плагинов:** Расширяемость (Open) часто достигается через **динамическое обнаружение и загрузку
   плагинов**. Механизмы:
    * `__subclasses__()` для поиска всех наследников ABC (хрупко, зависит от импортов).
    * **Entry points** в `setup.py`/`pyproject.toml` для обнаружения зарегистрированных плагинов из установленных
      пакетов.
    * **Импорт по соглашению:** Сканирование определенных директорий и динамический импорт через
      `importlib.import_module()`.
      "Закрытость" (Closed) обеспечивается стабильностью **публичного API** модуля или класса. Для тестового фреймворка
      это означает, что можно разрабатывать кастомные плагины для отчетов, форматов данных, провайдеров без модификации
      ядра.

3. **LSP и контрактное программирование:** Принцип Лисков формализует **контракты**. В Python есть инструменты для их
   явного задания:
    * `@abstractmethod` в ABC задает обязательство реализации.
    * Декораторы для предусловий/постусловий (например, библиотека `icontract` или самописные на основе `inspect` и
      `functools.wraps`).
    * **Аннотации типов** и статические анализаторы (mypy) проверяют часть контракта — совместимость типов.
      Нарушение LSP часто проявляется в **ковariantности/контравариантности** возвращаемых типов и аргументов. Если
      подкласс меняет возвращаемый тип на более узкий (например, `-> Dog` вместо `-> Animal`) — это допустимо (
      ковариантность по возврату). Но если он меняет тип аргумента на более широкий (например, принимает `Animal` вместо
      `Dog`) — это нарушает контракт (контравариантность по аргументам). Статические анализаторы могут ловить такие
      нарушения.

4. **ISP и автоматическое разделение интерфейсов:** Протоколы позволяют создавать **адаптивные интерфейсы**. Через
   механизм `__subclasshook__` можно реализовать автоматическое определение, что класс соответствует протоколу, если он
   реализует определенный набор методов. Более того, можно использовать **дескрипторы** и **метаклассы** для
   автоматической генерации "узких" прокси-интерфейсов на лету, которые предоставляют клиенту только требуемый
   функционал, скрывая остальное. Это уменьшает связанность в тестах.

5. **DIP и метапрограммирование:** Продвинутые фреймворки внедрения зависимостей используют:
    * **Интроспекцию** (`inspect.signature`) для автоматического определения зависимостей по аннотациям типов.
    * **Декораторы** для маркировки внедряемых сервисов.
    * **Контекстные менеджеры** для управления временем жизни объектов (scopes: singleton, request, transient).
    * **Асинхронное внедрение** зависимостей с использованием `async`/`await`.
      Для AQA это открывает возможность создания **супер-изолированных тестовых сред**, где все зависимости
      автоматически подменяются на контролируемые версии, включая стандартные библиотечные модули (через
      `unittest.mock.patch` на уровне импорта).

6. **SOLID и тестовая пирамида:**
    * **SRP** позволяет писать чистые, быстрые юнит-тесты.
    * **OCP** позволяет расширять тестовые фреймворки новыми видами тестов (нагрузочными, security) без переписывания.
    * **LSP** гарантирует, что тесты, написанные для базовых классов фикстур, будут работать для производных.
    * **ISP** позволяет создавать легковесные моки для конкретных тестов.
    * **DIP** — основа для изоляции тестов и подмены интеграционных точек на стабы.

7. **Антипаттерны и запахи кода, нарушающие SOLID:**
    * **Нарушение SRP:** Класс `TestHelper`, который и генерирует данные, и делает HTTP-запросы, и пишет в лог, и
      проверяет ответы.
    * **Нарушение OCP:** Монолитный класс `TestRunner` с гигантским `if-elif` по типам тестов.
    * **Нарушение LSP:** Класс `MockResponse`, который в методе `json()` выбрасывает исключение, хотя реальный
      `Response` его не бросает.
    * **Нарушение ISP:** Интерфейс `Database` с методами `connect`, `query`, `disconnect`, `backup`, `restore`, хотя
      большинству тестов нужен только `query`.
    * **Нарушение DIP:** Тестовый класс напрямую создает экземпляр `RealDatabaseConnector` вместо получения абстракции
      `DatabaseConnector`.

- [Содержание](#содержание)

---

# **Специфика ООП в Python**

Отлично, это ключевой вопрос для понимания философии языка, на котором строится работа. Вот развернутый ответ на тему
специфики ООП в Python.

## **Junior Level*

Объектно-ориентированное программирование (ООП) в Python — это не строгая доктрина, а гибкий набор инструментов,
встроенных в самую суть языка. В отличие от некоторых других языков (например, Java или C#), Python подходит к ООП с
прагматичной точки зрения: "если что-то ведет себя как объект, то это объект".

Главные особенности:

1. **Всё является объектом.** Даже числа, строки, функции и классы — это объекты. Это дает им возможность иметь атрибуты
   и методы.
2. **Динамическая природа.** Типы проверяются во время выполнения программы, а не на этапе компиляции. Это позволяет
   создавать очень гибкие конструкции.
3. **Простота синтаксиса.** Нет сложных модификаторов доступа вроде `private`, `protected`, `public` в классическом
   понимании. Вместо них используется соглашение об одном (`_`) и двух подчеркиваниях (`__`) для указания на уровень "
   защищенности".
4. **Множественное наследование.** Класс может наследоваться от нескольких родительских классов одновременно, что
   позволяет создавать сложные иерархии, но требует понимания метода разрешения порядка наследования (MRO).
5. **Магические методы (dunder methods).** Поведение объектов (например, что происходит при сложении, при обращении по
   индексу, при преобразовании к строке) определяется специальными методами, которые начинаются и заканчиваются двойным
   подчеркиванием (`__init__`, `__str__`, `__add__`). Это позволяет перегружать операторы и делать код интуитивно
   понятным.

Для AQA инженера это означает, что тесты и фреймворки на Python могут быть очень выразительными и лаконичными, а также
легко расширяемыми за счет этих динамических возможностей.

# ## **Middle Level**

На техническом уровне специфика ООП в Python раскрывается через несколько ключевых концепций и внутренних механизмов.

1. **Модель данных на основе специальных методов:** Ядро ООП в Python — это **протоколы**, определяемые дандер-методами.
   Например, протокол итератора требует методов `__iter__` и `__next__`. Это не абстрактные классы в классическом
   понимании, а соглашения. Класс, реализующий эти методы, автоматически становится итератором. Это называется **утиной
   типизацией**.

2. **Атрибуты и их поиск:** Атрибуты объекта в Python — это не просто поля в памяти. Поиск атрибута (`obj.attr`)
   проходит по сложной цепочке:
    * Сначала проверяется `obj.__dict__`.
    * Если не найдено, идет проверка в `type(obj).__dict__` (атрибуты класса).
    * Затем поиск поднимается по иерархии наследования (согласно MRO).
    * Если атрибут все еще не найден, вызывается метод `__getattr__` (если он определен).
      Этот механизм лежит в основе свойств (`@property`), дескрипторов и динамического добавления атрибутов.

3. **Дескрипторы (Descriptor Protocol):** Это один из самых мощных и скрытых механизмов. Если атрибут класса является
   объектом с методами `__get__`, `__set__` или `__delete__`, то при доступе к этому атрибуту через экземпляр вызываются
   эти методы. Именно так работают `@property`, `@classmethod`, `@staticmethod`, обычные методы и даже `__slots__`.
   Дескрипторы — это основа привязки методов к экземплярам.

4. **Метаклассы:** В Python классы тоже являются объектами. Объекты какого класса? Класса `type`. `type` — это
   метакласс, экземплярами которого являются все пользовательские классы. Пользователь может создавать свои метаклассы,
   наследуясь от `type`, чтобы перехватывать и модифицировать процесс создания класса. Это позволяет реализовать такие
   вещи, как автоматическую регистрацию подклассов, добавление атрибутов, проверку сигнатур методов или реализацию
   шаблонов вроде Singleton на уровне создания класса.

5. **Классы и экземпляры как namespaces:** Класс и экземпляр — это, по сути, словари (`__dict__`) с дополнительной
   логикой поиска. Это позволяет делать monkey-patching (динамическое изменение классов и объектов в рантайме), что
   широко используется в тестировании для мокирования, но может быть опасным в production-коде.

6. **Для AQA:** Динамическая природа ООП в Python позволяет создавать элегантные паттерны для тестирования:
    * **Фикстуры в pytest:** Используют декораторы и механизм зависимостей, которые были бы невозможны без гибкой модели
      объектов.
    * **Моки и патчи (`unittest.mock`):** Полностью полагаются на возможность подмены атрибутов объектов и модулей в
      рантайме.
    * **Плагины:** Система плагинов в pytest использует метаклассы и динамическое обнаружение для расширения
      функциональности.

# ## **Senior Level**

Здесь мы погружаемся в онтологию объектов Python, механику связывания (binding) и философию "всё является объектом".

1. **Объектная модель CPython (логический уровень):** Каждый объект в Python — это структура `PyObject` (или ее
   наследник в логическом представлении), содержащая как минимум счетчик ссылок и указатель на тип объекта. **Тип
   объекта (`PyTypeObject`)** — это тоже объект, содержащий таблицу виртуальных функций (slots), которая определяет
   поведение экземпляров. Эти слоты (например, `tp_getattr`, `tp_iter`, `tp_call`) соответствуют дандер-методам. Когда
   вы вызываете `len(obj)`, интерпретатор ищет слот `tp_len` в типе объекта. Если он найден (что означает, что класс
   определил `__len__`), вызывается связанная функция. Это делает операции над объектами невероятно быстрыми на уровне
   C.

2. **Механизм связывания методов (Method Binding):** Когда вы определяете метод в классе, он хранится в `class.__dict__`
   как обычная функция. При обращении к нему через экземпляр (`instance.method`) срабатывает **дескриптор**. Функция,
   будучи дескриптором, имеет метод `__get__`. При его вызове с экземпляром и классом в качестве аргументов, он
   возвращает **связанный метод (bound method)** — объект, который "запоминает" экземпляр и при вызове автоматически
   подставляет его первым аргументом (`self`). Если метод вызван через класс (`Class.method`), `__get__` возвращает
   несвязанную (unbound) функцию. Это не магия, а прямое следствие протокола дескрипторов.

3. **`__slots__` и экономия памяти:** Обычно каждый экземпляр хранит атрибуты в своем словаре `__dict__`, что требует
   много памяти. `__slots__` — это особый атрибут класса, который заменяет `__dict__` на статически выделенный массив
   слотов для указанных атрибутов. Это не только экономит память, но и ускоряет доступ к атрибутам. Однако это
   ограничивает динамическое добавление атрибутов и требует осторожности при наследовании. Для AQA это важно при
   тестировании высоконагруженных систем или работе с огромными массивами объектов (например, в генераторах тестовых
   данных).

4. **Метаклассы и `__init_subclass__`:** Вместо сложных метаклассов для простых задач модификации подклассов теперь
   можно использовать метод класса `__init_subclass__`. Он вызывается при инициализации каждого нового подкласса. Это
   более простая и понятная альтернатива для регистрации классов, добавления атрибутов по умолчанию или валидации.
   Однако для сложных трансформаций (изменение порядка наследования, модификация тела класса) метаклассы остаются
   незаменимы.

5. **Для AQA (Кровь, кишки и безумие):**
    * **Создание DSL (предметно-ориентированного языка) для тестов:** Используя магические методы (`__getattr__`,
      `__call__`), дескрипторы и метаклассы, можно создавать выразительные DSL. Например, тест может выглядеть как
      `page.element.should(be_visible.and_have(text="Submit"))`. Это достигается цепочкой объектов, каждый из которых
      возвращает следующий через кастомные `__getattr__` или дескрипторы.
    * **Динамическая генерация тестов:** Метакласс может сканировать тело класса во время его создания, находить
      функции, начинающиеся с `test_`, и на их основе генерировать множество реальных тест-кейсов с разными параметрами
      или именами, что используется в `pytest` и `unittest` с параметризацией.
    * **Трассировка и интроспекция в тестах:** Используя знание о том, что вызов метода — это вызов связанного объекта,
      можно создавать декораторы, которые оборачивают каждый метод класса для трассировки вызовов, замеров времени или
      автоматического логирования. Это делается путем перехвата в метаклассе или через `__getattribute__` (но осторожно,
      это очень низкоуровневый и медленный механизм).
    * **Эмуляция и патчинг на системном уровне:** Понимание, что даже импортированные модули — это объекты в
      `sys.modules`, позволяет делать глубокие патчи, например, подменять встроенные функции (`open`, `os.listdir`) для
      изоляции тестов от файловой системы. Это основа библиотек вроде `pytest-mock`.
    * **Профилирование потребления памяти тестами:** Понимание модели объектов (`__dict__`, `__slots__`, ссылочные
      циклы) позволяет писать тесты, которые не только проверяют функциональность, но и профилируют утечки памяти или
      чрезмерное потребление ресурсов в тестируемом коде, используя `tracemalloc` или `gc` модули.

- [Содержание](#содержание)

---

# **Наследование и композиция**

Отличный вопрос, затрагивающий самую суть дизайна объектно-ориентированных систем. Вот развернутый ответ.

## **Junior Level*

Наследование и композиция — это два основных способа повторного использования кода и построения отношений между классами
в ООП.

**Наследование** — это отношение "является" (is-a). Когда класс `Dog` наследует от класса `Animal`, мы говорим, что "
Собака **является** Животным". Наследование позволяет дочернему классу автоматически получать все методы и атрибуты
родительского класса, а также может переопределять или расширять его поведение.

**Композиция** — это отношение "имеет" (has-a). Когда класс `Car` содержит в себе объект класса `Engine`, мы говорим,
что "Автомобиль **имеет** Двигатель". Композиция позволяет строить сложные объекты, собирая их из более простых
компонентов.

Простое правило: **предпочитайте композицию наследованию**. Наследование создает жесткую связь между классами, тогда как
композиция дает больше гибкости и позволяет изменять поведение объекта в рантайме, заменяя его компоненты. Для AQA
инженера композиция часто предпочтительнее, так как она упрощает тестирование — компоненты можно легко подменить моками.

# ## **Middle Level**

На техническом уровне обе концепции реализуются по-разному и имеют различные последствия для архитектуры.

**Наследование в Python:**

1. **Механизм:** Используется синтаксис `class Child(Parent1, Parent2):`. Поддерживается множественное наследование.
2. **Поиск атрибутов (MRO):** Python использует C3 Linearization для построения линеаризованного списка классов (Method
   Resolution Order), который определяет порядок поиска атрибутов и методов. Его можно посмотреть через
   `ClassName.__mro__`.
3. **Вызов родительских методов:** Для явного вызова метода родительского класса используется `super()`. При
   множественном наследовании `super()` работает в соответствии с MRO, что позволяет кооперативно вызывать методы
   родителей в предсказуемом порядке.
4. **Абстрактные классы:** Для создания интерфейсов, которые обязывают наследников реализовать определенные методы,
   используются ABC (Abstract Base Classes) с декоратором `@abstractmethod`.

**Композиция в Python:**

1. **Механизм:** Реализуется путем включения объектов других классов в качестве атрибутов. Это может быть сделано в
   конструкторе (`__init__`) или через setter-методы.
2. **Динамическое поведение:** Компоненты могут быть заменены во время выполнения, что позволяет динамически менять
   поведение объекта (паттерн "Стратегия").
3. **Управление жизненным циклом:** При композиции владеющий объект обычно управляет жизненным циклом компонентов (
   создает и уничтожает их). При агрегации (более слабая форма композиции) жизненные циклы независимы.

**Сравнение для AQA:**

* **Тестируемость:** Классы, построенные через композицию, легко тестировать, так как зависимости можно заглушить или
  замокать. Наследование же часто приводит к необходимости тестировать целые иерархии классов, а мокирование
  родительских методов становится сложнее.
* **Хрупкость базового класса (Fragile Base Class):** Изменения в родительском классе могут неожиданно сломать работу
  дочерних классов, даже если они не касались напрямую переопределенных методов. Это делает наследование рискованным.
* **Взрыв иерархии:** При активном использовании наследования для добавления новой функциональности часто приходится
  создавать новые классы на каждом уровне иерархии (проблема "ромбовидного наследования"), что ведет к сложности
  поддержки.

# ## **Senior Level**

На этом уровне мы рассматриваем наследование и композицию как конкурирующие парадигмы с глубокими последствиями для
структуры кода, его эволюции и тестируемости.

**1. Наследование как нарушение инкапсуляции:**
Наследование — это самый сильный вид связи в ООП (white-box reuse). Дочерний класс получает доступ к защищенным (
`_protected`) атрибутам и методам родителя, тем самым нарушая инкапсуляцию родительского класса. Это создает скрытые
контракты: дочерний класс должен не только реализовывать публичный интерфейс, но и знать о внутренних деталях родителя.
Любое изменение этих внутренних деталей в родителе может сломать наследников. Для тестируемости это катастрофа: чтобы
понять поведение дочернего класса, нужно глубоко понимать поведение родителя, что резко увеличивает когнитивную нагрузку
и сложность изоляции.

**2. Композиция и протоколы:**
Вместо наследования для полиморфизма в Python всё чаще используется **композиция с протоколами** (Structural Subtyping).
Объект не обязан объявлять "я наследник абстрактного класса `Reader`", он должен просто реализовать метод `read()`. Это
проверяется статически (через `typing.Protocol` и mypy) или динамически (через `hasattr` или `isinstance` с
`@runtime_checkable`). Такой подход резко снижает связанность. В тестах мы можем передать любой объект, имеющий
`read()`, без необходимости строить иерархии наследования.

**3. Множественное наследование и Mixins:**
Python разрешает множественное наследование, что является мощным, но опасным инструментом. На практике его используют
для реализации **Mixins** — небольших классов, предоставляющих определенную функциональность (например, `LoggingMixin`,
`SerializationMixin`). Mixin не предназначен для самостоятельного использования, он добавляет методы в класс. Однако
если два Mixina определяют методы с одинаковыми именами, возникает конфликт, разрешаемый через MRO. Для тестового
фреймворка Mixins могут быть полезны для добавления общих утилит в тест-классы, но необходимо тщательно проектировать
их, чтобы избежать конфликтов и неявных зависимостей.

**4. Делегирование как альтернатива:**
Паттерн **Делегирование** (Delegation) — это явная форма композиции, где внешний объект (делегатор) передает запросы
внутреннему объекту (делегату). В Python его можно элегантно реализовать через метод `__getattr__`: если атрибут не
найден в самом объекте, запрос автоматически перенаправляется делегату. Это позволяет строить объекты-обертки (например,
адаптеры, прокси), которые полностью контролируют взаимодействие с внутренним объектом и обеспечивают точку для
внедрения моков в тестах.

**5. Для AQA (Кровь, кишки и безумие):**

* **Тестирование наследуемых фикстур:** При использовании наследования для построения тестовых классов (например, в
  `unittest.TestCase`) изменение в базовом классе тестов может сломать сотни наследников. Решение — использовать *
  *композицию фикстур** (как в pytest), где каждая фикстура независима и явно запрашивается в тесте. Это дает
  детерминированное управление жизненным циклом и изоляцию.
* **Мокирование унаследованных методов:** Мокировать метод, который вызывается через `super()`, крайне сложно, потому
  что `super()` является статической конструкцией, разрешаемой во время компиляции. Приходится применять тяжеловесные
  патчи (`patch.object`) на конкретный класс, что ведет к хрупким тестам. Композиция позволяет просто подменить весь
  компонент.
* **Анализ покрытия для иерархий классов:** При наследовании инструменты покрытия кода (coverage.py) могут показывать
  вводящую в заблуждение статистику: строка кода в родительском классе может быть выполнена при тестировании дочернего,
  но это не означает, что она была протестирована во всех возможных контекстах. Ответственный AQA должен уметь
  интерпретировать такие отчеты и, возможно, требовать отдельного тестирования базовых классов.
* **Динамическое наследование и метаклассы:** В очень продвинутых фреймворках классы могут динамически наследоваться от
  других с помощью `type(name, bases, dict)` или метаклассов. Это позволяет генерировать специализированные классы на
  лету (например, для каждой тестовой конфигурации). Хотя это мощно, это усложняет отладку и понимание потока
  выполнения. Тесты для такого кода должны быть максимально простыми и изолированными, чтобы не усугублять сложность.
* **Наследование от встроенных типов:** В Python можно наследоваться от `list`, `dict` и других встроенных типов. Это
  может быть полезно для добавления поведения, но чревато тонкими ошибками, если не переопределены все relevant
  специальные методы (например, могут сломаться операции копирования или сериализации). При тестировании таких классов
  необходимо проверять не только добавленное поведение, но и что все унаследованные методы работают корректно в новом
  контексте.

- [Содержание](#содержание)

---

# **Метапрограммирование**

## **Junior Level*

Метапрограммирование в Python — это техника, когда программа может создавать или модифицировать другую программу,
включая саму себя, во время выполнения. Если обычное программирование работает с данными, то метапрограммирование
работает с кодом как с данными.

Простые примеры, которые вы уже используете:

- **Декораторы** — это функции, которые меняют поведение других функций или классов.
- **Динамическое создание классов** с помощью `type()`.
- **Изменение объектов на лету** (например, добавление новых атрибутов).

Для QA инженера метапрограммирование — мощный инструмент для создания гибких тестовых фреймворков, автоматической
генерации тестов, мокирования и построения DSL (предметно-ориентированных языков) для тестирования.

# ## **Middle Level**

Технически метаprogramming в Python охватывает несколько ключевых областей:

1. **Декораторы:** Синтаксический сахар для функций высшего порядка. Декоратор принимает функцию/класс и возвращает
   модифицированную версию. Могут быть вложенными, принимать аргументы, применяться к методам. Для AQA:
   `@pytest.fixture`, `@unittest.mock.patch`, `@pytest.mark.parametrize`.

2. **Метаклассы:** Классы — это объекты типа `type`. Метакласс — это класс классов, контролирующий создание классов.
   Позволяет перехватывать момент создания класса, модифицировать его атрибуты, методы, добавлять валидацию.
   Используется в ORM (Django), валидаторах данных.

3. **Дескрипторы:** Объекты с методами `__get__`, `__set__`, `__delete__`. Лежат в основе `@property`, методов классов,
   `__slots__`. Позволяют контролировать доступ к атрибутам.

4. **Динамическое выполнение кода:** `eval()` и `exec()` выполняют строки как код Python. Опасны, но мощны для DSL или
   конфигураций.

5. **Интроспекция:** Возможность исследовать объекты во время выполнения через `dir()`, `getattr()`, `hasattr()`, модуль
   `inspect`. Основа для автоматического обнаружения тестов, dependency injection.

6. **Monkey Patching:** Динамическое изменение модулей и классов в рантайме. Основа `unittest.mock`, но требует
   осторожности.

Для AQA: pytest использует метапрограммирование для фикстур, параметризации; unittest.mock для подмены объектов; можно
создавать кастомные маркеры, системы отчетности.

# ## **Senior Level**

На этом уровне метапрограммирование — это философия проектирования систем, где код манипулирует структурой и поведением
во время выполнения.

1. **Метаклассы и `__init_subclass__`:**
    - Метакласс `ABCMeta` реализует ABC. Метаклассы могут конфликтовать при множественном наследовании — Python
      разрешает конфликты, требуя чтобы один метакласс был подклассом другого.
    - `__init_subclass__` — более простая альтернатива для модификации подклассов без полного метакласса. Вызывается при
      создании подкласса, позволяет регистрировать классы, добавлять атрибуты.

2. **Дескрипторы высшего порядка:**
    - Дескрипторы могут иметь состояние. Например, `property`, которое кэширует результат. Для этого дескриптор хранит
      данные в `instance.__dict__` под уникальным ключом (часто `_<descriptor_name>`).
    - **Неданные дескрипторы** (только `__get__`) и **данные дескрипторы** (есть `__set__`). Данные дескрипторы имеют
      приоритет над `instance.__dict__` при записи.
    - Можно создавать **ленивые дескрипторы**, которые вычисляют значение при первом обращении.

3. **Метапрограммирование с аннотациями типов:**
    - Аннотации доступны в `__annotations__`. Их можно использовать для автоматической валидации, сериализации,
      генерации схем.
    - `typing.get_type_hints()` возвращает аннотации с разрешенными forward references.
    - **Pydantic, FastAPI** используют аннотации для построения моделей и API. Для AQA — можно создавать
      само-валидирующиеся тестовые данные.

4. **Перехват создания модулей:**
    - Можно перехватывать импорт через **хуки в `sys.meta_path`**. Каждый элемент `meta_path` — это finder/loader.
      `importlib.abc.MetaPathFinder` позволяет подменять модули на лету, что используется для изоляции тестов (подмена
      `sys.path`), загрузки конфигов.

5. **Динамическое изменение MRO:**
    - MRO определяется при создании класса, но можно изменить через `__mro__` (только для чтения) или создать новый
      класс с другим порядком наследования.
    - `type.__new__` может переупорядочивать базовые классы.

6. **Для AQA (Кровь, кишки и безумие):**
    - **Генерация тестов из данных:** Метакласс, который читает YAML/JSON конфиг и для каждого элемента генерирует
      тестовый метод с уникальным именем и параметрами. Это позволяет тестам быть декларативными.
    - **Автоматические моки:** Дескриптор, который при доступе к атрибуту `api.client` автоматически создает
      `unittest.mock.Mock` с сигнатурой, соответствующей аннотациям реального класса. Это дает автодополнение в IDE и
      безопасность.
    - **Трассировка вызовов в реальном времени:** Декоратор, оборачивающий каждый метод класса, который записывает
      вызовы, аргументы, возвращаемые значения и исключения. Полезно для отладки интеграционных тестов.
    - **Инъекция фикстур на уровне метакласса:** Метакласс, который анализирует аннотации методов класса и автоматически
      внедряет фикстуры перед вызовом тестовых методов, эмулируя поведение pytest, но для кастомного фреймворка.
    - **Динамическое создание классов для изоляции:** Каждый тест запускается в экземпляре класса, который создан
      динамически с уникальным набором моков. Это гарантирует, что состояния не пересекаются даже при параллельном
      запуске.
    - **Валидация архитектуры через декораторы:** Декоратор `@enforce_interface`, который проверяет, что класс реализует
      все методы протокола и их сигнатуры соответствуют ожидаемым. Использует `inspect.signature` и `get_type_hints`.
    - **Подмена стандартной библиотеки в тестах:** Через `sys.modules` подменять модули типа `time`, `random`, `uuid` на
      детерминированные версии, чтобы тесты были воспроизводимы.
    - **Сериализация сложных объектов для снимков (snapshot testing):** Дескриптор или метакласс, который автоматически
      генерирует сериализатор для класса на основе его аннотаций и `__slots__`/`__dict__`, и сравнивает состояние
      объекта со снимком.

7. **Опасности и ограничения:**
    - **Производительность:** Метапрограммирование добавляет накладные расходы. Динамическое создание классов, глубокий
      introspection замедляют код.
    - **Читаемость:** Код с активным метапрограммированием труден для понимания и отладки. Нужна отличная документация.
    - **Совместимость:** Динамические изменения могут сломаться при обновлении Python или сторонних библиотек.
    - **Тестирование самого метакода:** Требует особого подхода, часто с использованием тех же метатехник.

- [Содержание](#содержание)

---

# **Автоматизация**

## **Junior Level*

ООП в Python кажется простым и интуитивным, но содержит несколько неочевидных "подводных камней", которые могут
приводить к трудноуловимым ошибкам, особенно в сложных тестовых фреймворках и автоматизации.

Основные подводные камни:

1. **Изменяемые атрибуты класса** - если вы определите список или словарь как атрибут класса (а не экземпляра), все
   экземпляры будут разделять одну и ту же копию этого объекта.
2. **Неявное связывание self** - методы автоматически получают `self`, но при использовании методов как callback-функций
   можно забыть его привязать.
3. **Наследование от встроенных типов** - поведение может отличаться от ожидаемого, особенно при переопределении
   методов.
4. **Множественное наследование и MRO** - порядок разрешения методов может стать неожиданным в сложных иерархиях.
5. **Магические методы и операторы** - некоторые методы вызываются неявно, и их неправильная реализация может сломать
   стандартное поведение.

Для QA инженера эти нюансы критичны, так как могут вызывать недетерминированное поведение тестов, утечки памяти и
сложные для отладки проблемы в тестовых фреймворках.

# ## **Middle Level**

Технические детали основных подводных камней:

1. **Изменяемые атрибуты класса и экземпляра:**
    - Атрибуты, объявленные на уровне класса, являются общими для всех экземпляров. Если это изменяемый объект (список,
      словарь, множество), изменение через один экземпляр влияет на все.
    - Проблема возникает из-за того, что поиск атрибутов идет сначала в `instance.__dict__`, затем в `class.__dict__`.
      При присваивании `instance.attr = value` создается запись в `instance.__dict__`, но при обращении к `class.attr`
      модифицируется общий объект.

2. **Методы и bound/unbound состояния:**
    - Метод, извлеченный через экземпляр (`instance.method`), становится bound method с привязанным `self`.
    - Метод, извлеченный через класс (`Class.method`), остается unbound function.
    - Если передать `Class.method` как callback без явного связывания, вызов завершится ошибкой из-за отсутствия `self`.

3. **Наследование от встроенных типов и `super()`:**
    - Встроенные типы (`list`, `dict`, `str`) написаны на C и могут не вызывать переопределенные методы в подклассах
      ожидаемым образом.
    - `super()` в классах, наследующих от встроенных типов, может вести себя иначе, особенно при множественном
      наследовании.
    - Пример: переопределение `__init__` в наследнике `list` требует явного вызова `super().__init__()`.

4. **Множественное наследование и алгоритм C3:**
    - MRO (Method Resolution Order) определяется алгоритмом C3, который гарантирует сохранение порядка и отсутствие
      циклов.
    - Однако сложные диаграммы наследования могут приводить к неочевидному порядку вызовов.
    - Проблема "ромбовидного наследования" может вызывать многократный вызов одного и того же метода.

5. **Магические методы, не вызываемые неявно:**
    - Некоторые магические методы вызываются только для встроенных операций, но не для соответствующих функций.
    - Например, `len(x)` вызывает `x.__len__()`, но нет гарантии, что все контейнеры реализуют `__len__`.
    - `__del__` не является деструктором в классическом понимании и вызывается не детерминировано сборщиком мусора.

6. **Проблемы с `__hash__` и `__eq__`:**
    - При переопределении `__eq__` нужно переопределять `__hash__`, иначе объект станет нехешируемым.
    - Если `__eq__` возвращает `True` для объектов разных классов, их хеш-значения должны совпадать.

7. **Для AQA:**
    - Фикстуры с состоянием на уровне класса могут приводить к взаимовлиянию тестов.
    - Моки методов родительских классов требуют понимания MRO для правильного патчинга.
    - Сериализация объектов с циклическими ссылками может приводить к переполнению стека.

# ## **Senior Level**

Глубокий анализ архитектурных и системных подводных камней:

1. **Изменяемые атрибуты класса и дескрипторы:**
    - Проблема глубже, чем кажется. Дескрипторы данных (например, `property`) имеют приоритет над `instance.__dict__`.
    - При использовании `__slots__` механизм полностью меняется: объекты не имеют `__dict__`, атрибуты хранятся в
      массиве указателей.
    - **Последствие для тестирования:** Мокирование атрибутов в классах со `__slots__` невозможно стандартными
      средствами `unittest.mock`, требуется создание нового класса.

2. **Метод `__init_subclass__` и метаклассы:**
    - `__init_subclass__` вызывается при создании каждого подкласса, но если в иерархии есть метакласс, порядок вызова
      может быть неочевидным.
    - Метаклассы могут конфликтовать при множественном наследовании, вызывая `TypeError: metaclass conflict`.
    - **Для AQA:** При создании фреймворка с плагинами через наследование, неправильное сочетание метаклассов может
      сломать загрузку плагинов.

3. **Циклические ссылки и сборка мусора:**
    - Python использует комбинированный сборщик мусора (reference counting + generational GC).
    - Циклические ссылки между объектов с `__del__` никогда не будут собраны, так как GC не может определить порядок
      удаления.
    - Слабые ссылки (`weakref`) решают проблему, но требуют осторожности: `weakref.ref(obj)` возвращает прокси, который
      может стать `None` в любой момент.
    - **Для AQA:** Тесты, создающие графы объектов с циклическими ссылками, могут вызывать утечки памяти, которые
      проявляются только при длительных прогонах.

4. **Протоколы итераторов и генераторов:**
    - Итератор, исчерпавший себя, при последующих вызовах `__next__` всегда выбрасывает `StopIteration`.
    - В генераторах `return value` преобразуется в `StopIteration(value)`, что может быть неочевидно.
    - Асинхронные генераторы имеют отдельный жизненный цикл и требуют явного закрытия через `aclose()`, иначе могут
      оставаться незавершенными.
    - **Для AQA:** Тестирование генераторов требует проверки не только выдачи значений, но и корректного завершения.
      Незакрытые асинхронные генераторы в тестах могут приводить к предупреждениям `ResourceWarning`.

5. **Дескрипторы с состоянием и кэширование:**
    - Дескриптор, хранящий состояние в себе (а не в экземпляре), будет разделять это состояние между всеми экземплярами
      класса.
    - Паттерн кэширования через дескриптор требует использования `instance.__dict__` для хранения кэша, но это может
      конфликтовать с `__slots__`.
    - **Для AQA:** Мокирование property-дескрипторов требует глубокого понимания: `@property` создает дескриптор на
      уровне класса, поэтому патчить нужно именно его.

6. **Проблемы сериализации и `__getstate__`/`__setstate__`:**
    - По умолчанию `pickle` сохраняет `instance.__dict__`. Для объектов со `__slots__` это не работает.
    - `__getstate__` должен возвращать сериализуемое представление, но если он возвращает сложный граф объектов, могут
      возникнуть циклические ссылки.
    - `__setstate__` должен корректно восстанавливать состояние, включая несериализуемые ресурсы (файлы, сокеты).
    - **Для AQA:** Сериализация используется в распределенных тестовых системах. Объекты тестовых данных и фикстур
      должны быть сериализуемыми.

7. **Наследование от `dict` и `list`:**
    - Встроенные типы оптимизированы и могут обходить переопределенные методы.
    - Например, `dict.update()` может не вызывать переопределенный `__setitem__`.
    - Решение: наследоваться от `collections.UserDict` или `collections.UserList`, которые реализованы на Python и
      правильно вызывают переопределенные методы.
    - **Для AQA:** Кастомные коллекции для тестовых данных должны наследоваться от абстрактных классов
      `collections.abc`, а не от встроенных типов.

8. **Монкеи-патчинг и его последствия:**
    - Изменение классов или модулей в runtime влияет на все потоки выполнения.
    - В асинхронном коде это может вызывать состояние гонки.
    - Патчинг через `unittest.mock.patch` использует механизм `importlib`, но не всегда очищает кэши (`sys.modules`).
    - **Для AQA:** Параллельный запуск тестов с патчингом одних и тех же модулей может приводить к недетерминированному
      поведению. Нужно использовать изоляцию на уровне процесса.

9. **Проблема хрупкого базового класса (Fragile Base Class):**
    - Изменение приватного метода (`_method`) в базовом классе может сломать наследников, которые его используют.
    - Добавление публичного метода в базовый класс может случайно переопределить метод в наследнике (если у него было то
      же имя).
    - **Для AQA:** Тестовые базовые классы должны быть максимально стабильными или использовать композицию вместо
      наследования.

10. **`__prepare__` и пространства имен класса:**
    - Метакласс может определить метод `__prepare__`, который возвращает кастомный словарь для namespace класса.
    - Это позволяет, например, автоматически регистрировать все методы, но ломает ожидания о том, что `class.__dict__`
      является обычным словарем.
    - **Для AQA:** Фреймворки, использующие такие метаклассы, могут иметь неочевидное поведение при интроспекции
      тестовых классов.

11. **Проблемы с `__del__` и контекстные менеджеры:**
    - `__del__` может быть вызван в любой момент, даже во время сборки мусора при завершении интерпретатора.
    - Если в `__del__` происходит обращение к глобальным переменным или импортированным модулям, они могут быть уже
      `None`.
    - Контекстные менеджеры (`__enter__`/`__exit__`) более предсказуемы для управления ресурсами.
    - **Для AQA:** Тестовые фикстуры, управляющие ресурсами (браузер, БД), должны использовать контекстные менеджеры, а
      не `__del__`.

12. **Аннотации типов и `__init__`:**
    - Аннотации типов сохраняются в `__annotations__`, но не влияют на проверку во время выполнения.
    - `dataclasses` и Pydantic используют аннотации для генерации `__init__`, что может конфликтовать с явно
      определенным `__init__` в родительском классе.
    - **Для AQA:** Тестовые data-классы должны быть согласованы: либо использовать `@dataclass`, либо явный `__init__`,
      но не смешивать подходы.

- [Содержание](#содержание)

---

# **Миксины**

### ## **Junior Level**

Миксины (mixins) — это специальные классы в Python, предназначенные для добавления конкретной функциональности к другим
классам через множественное наследование. Если представить основной класс как основное блюдо, то миксины — это специи
или добавки, которые придают ему дополнительные свойства и возможности.

Миксины не предназначены для самостоятельного использования — они не являются полноценными объектами, а служат для "
подмешивания" методов и атрибутов к другим классам. Например, можно создать миксин `LoggingMixin`, который добавляет
методы логирования, и использовать его в разных классах, где нужна эта функциональность.

Для QA инженера миксины полезны при создании тестовых фреймворков и утилит: можно вынести общую функциональность (
например, работу с базой данных, генерацию тестовых данных, создание отчетов) в миксины и переиспользовать их в разных
тестовых классах.

## **Middle Level**

Технически миксины — это обычные классы Python, которые следуют определенным соглашениям при использовании в
множественном наследовании.

1. **Синтаксис и использование:**
    - Миксины включаются в цепочку наследования перед основным классом.
    - Обычно имеют суффикс `Mixin` в названии для ясности.
    - Не вызывают `super().__init__()` в своем `__init__`, если не предназначены для участия в MRO (Method Resolution
      Order) инициализации.

2. **Метод разрешения порядка (MRO):**
    - При множественном наследовании Python использует алгоритм C3 для определения порядка поиска методов.
    - Миксины должны быть спроектированы так, чтобы не конфликтовать с методами основных классов и других миксинов.
    - Важно правильно располагать миксины в списке наследования: обычно миксины идут слева перед основными классами.

3. **Особенности проектирования:**
    - **Одна ответственность:** Каждый миксин должен добавлять одну четкую функциональность.
    - **Независимость:** Миксины должны быть максимально независимы от деталей реализации классов, к которым они
      подмешиваются.
    - **Гибкость:** Миксины могут определять абстрактные методы (`@abstractmethod`), которые должны быть реализованы в
      основном классе.

4. **Для AQA:**
    - **Тестовые утилиты:** Создание миксинов для общих операций: `DatabaseMixin` (для работы с БД), `APIClientMixin` (
      для HTTP-запросов), `ScreenshotMixin` (для создания скриншотов при падении теста).
    - **Расширение Page Object:** Добавление функциональности к Page Object через миксины (например, `ModalDialogMixin`
      для работы с модальными окнами).
    - **Кастомизация тестовых классов:** В `unittest.TestCase` можно использовать миксины для добавления методов
      подготовки данных, ассертов.

5. **Примеры в экосистеме Python:**
    - Django использует миксины для CBV (Class-Based Views).
    - DRF (Django REST Framework) активно использует миксины для ViewSets.
    - В тестировании: `unittest.TestCase` можно расширять миксинами.

## **Senior Level**

На этом уровне миксины рассматриваются как инструмент композиции поведения, альтернатива сложным иерархиям наследования,
с глубоким анализом их взаимодействия с механизмами Python.

1. **Миксины и линейная MRO:**
    - Алгоритм C3 гарантирует сохранение порядка и отсутствие циклов в наследовании.
    - При использовании нескольких миксинов их порядок в списке наследования критичен. Методы миксина, стоящего левее,
      имеют приоритет.
    - **Проблема diamond inheritance:** Если два миксина наследуются от одного базового класса, может возникнуть
      неочевидное поведение. C3 решает эту проблему, но важно понимать итоговый MRO.

2. **Миксины как протоколы дескрипторов:**
    - Миксины могут использовать дескрипторы для управления доступом к атрибутам. Например, миксин может добавлять
      property или управляемые атрибуты.
    - Дескрипторы в миксинах могут быть использованы для автоматической валидации данных, ленивой загрузки или
      кэширования.

3. **Динамическое подмешивание (монкей-патчинг vs наследование):**
    - В отличие от статического наследования, миксины можно "подмешивать" динамически с помощью `type()` с тремя
      аргументами:
      ```python
      DynamicClass = type('DynamicClass', (ExistingClass, SomeMixin), {})
      ```
    - Это позволяет создавать классы на лету с нужной комбинацией функциональности, что полезно в тестовых фреймворках
      для создания специализированных тестовых классов.
    - Однако такой подход усложняет отладку и анализ кода.

4. **Миксины и метаклассы:**
    - Миксины могут иметь свои метаклассы, что может привести к конфликту метаклассов при множественном наследовании.
    - Python разрешает конфликты, требуя чтобы один метакласс был подклассом другого. Если миксин использует метакласс,
      нужно убедиться в его совместимости.
    - Метаклассы миксинов могут использоваться для автоматической регистрации классов, добавления декораторов или
      изменения структуры класса.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Динамическая композиция тестовых классов:** В сложных тестовых фреймворках можно использовать фабрику классов,
      которая на основе конфигурации (например, тестируемой среды, типа браузера, нужных фич) динамически создает
      тестовый класс, подмешивая соответствующие миксины:
        - `SeleniumMixin` для UI-тестов.
        - `APITestMixin` для API-тестов.
        - `MobileTestingMixin` для мобильного тестирования.
        - `PerformanceMetricsMixin` для сбора метрик.
          Это позволяет избежать взрыва комбинаторного наследования.

    - **Миксины с зависимостями и инициализацией:** Создание миксинов, которые требуют определенной последовательности
      инициализации. Использование `__init_subclass__` для настройки класса при создании. Например, миксин может
      автоматически добавлять декораторы к методам класса, начинающимся с `test_`.

    - **Тестирование самих миксинов:** Написание тестов для миксинов — отдельная задача, так как они не предназначены
      для использования в изоляции. Стратегии:
        - Создание тестового класса-заглушки, который использует миксин.
        - Использование метаклассов для автоматического тестирования всех комбинаций миксинов.
        - Проверка, что миксин не ломает MRO основного класса.

    - **Миксины для cross-cutting concerns в тестах:**
        - `RetryMixin`: автоматически повторяет падающий тест (опасно, может маскировать проблемы).
        - `TimeoutMixin`: добавляет таймауты к тестам.
        - `IsolationMixin`: обеспечивает изоляцию тестов (создание уникальных данных, очистка состояния).
          Эти миксины могут переопределять методы выполнения тестов (`setUp`, `tearDown`, `runTest`), что требует
          глубокого понимания механизмов выполнения тестов во фреймворке.

    - **Конфликты и разрешение методов:** При использовании нескольких миксинов, определяющих методы с одинаковыми
      именами, можно реализовать механизм разрешения конфликтов:
        - Использование `super()` с явным указанием класса (но это ломает гибкость).
        - Создание миксина-медиатора, который координирует вызовы.
        - Использование декораторов для "сшивания" методов (например, вызывать оба метода).

    - **Миксины для интеграции с внешними системами:**
        - `AllureReportingMixin`: автоматически добавляет шаги и аттачменты в Allure-отчет.
        - `JiraIntegrationMixin`: создает баг-репорты в JIRA при падении теста.
        - `SlackNotificationMixin`: отправляет уведомления в Slack о начале/конце тестового прогона.
          Такие миксины часто требуют настройки (учетные данные, URL), что можно реализовать через атрибуты класса или
          конфигурационные объекты.

6. **Альтернативы миксинам в современном Python:**
    - **Композиция через классы-делегаты:** Вместо множественного наследования можно использовать композицию и
      делегирование. Это делает зависимости более явными, но требует больше кода.
    - **Протоколы и ABC:** Для определения интерфейсов лучше использовать абстрактные базовые классы или протоколы, а
      реализацию — через композицию.
    - **Декораторы классов:** Многие задачи, решаемые миксинами, можно реализовать через декораторы классов, которые
      модифицируют поведение.
    - **Хуки и плагины:** В pytest функциональность добавляется через фикстуры и плагины, что является более
      декларативным и гибким подходом, чем миксины.

7. **Опасности и антипаттерны:**
    - **God Mixin:** Миксин, который делает слишком много и становится монолитом.
    - **Неявные зависимости:** Миксин, который зависит от определенных атрибутов или методов в классе, но не декларирует
      это явно.
    - **Хрупкость MRO:** Изменение порядка миксинов может сломать работу класса.
    - **Тестируемость:** Классы с большим количеством миксинов сложно тестировать, так как приходится учитывать все
      подмешанные поведения.

- [Содержание](#содержание)

---

# **typing**

## **Junior Level*

`typing` модуль в Python предоставляет инструменты для добавления подсказок типов (type hints) в код. Это не меняет
поведение программы во время выполнения, но помогает разработчикам, IDE и статическим анализаторам понимать, какие типы
данных ожидаются.

**Optional[X]** означает, что значение может быть либо типа `X`, либо `None`. Это удобный способ сказать "этот аргумент
может быть передан, а может и нет".

**Union[X, Y, ...]** означает, что значение может быть одного из перечисленных типов. Например, `Union[int, str]` — это
либо целое число, либо строка.

**TypeVar** используется для создания обобщенных (generic) типов. Например, если у вас есть функция, которая работает со
списками любого типа, вы можете использовать `TypeVar` чтобы показать, что тип элементов входного списка и выходного
значения одинаков.

**Generic** — это способ создавать классы или функции, которые могут работать с разными типами, но сохранять информацию
о конкретном типе. Например, `List[int]` — это список целых чисел, а `List[str]` — список строк. `Generic` позволяет вам
создавать свои собственные классы, которые могут быть параметризованы типами.

Для QA инженера использование type hints делает код тестов и фреймворков более понятным, позволяет IDE предлагать
автодополнение и обнаруживать многие ошибки до запуска тестов.

# ## **Middle Level**

Технически, эти конструкции являются частью системы типизации, которая реализована в модуле `typing` и поддерживается
статическими анализаторами (mypy, pyright, PyCharm).

1. **Optional и Union:**
    - `Optional[X]` это просто сокращение для `Union[X, None]`.
    - `Union` может использоваться для любого количества типов. В Python 3.10 появился синтаксис `X | Y` как
      альтернатива `Union[X, Y]`.
    - Важно: `Optional` и `Union` не выполняют проверку типов во время выполнения. Они только для статического анализа.
    - Для проверки в runtime можно использовать `isinstance()` с кортежем типов, но это не связано напрямую с
      аннотациями.

2. **TypeVar:**
    - Создается вызовом `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`.
    - **Ограничения (constraints):** TypeVar может быть ограничен конкретными типами. Тогда он может представлять только
      один из них.
    - **Связывание (bound):** TypeVar может быть связан с базовым классом или протоколом. Тогда он представляет любой
      подтип этого базового класса.
    - **Ковариантность/контравариантность:** Позволяют выразить отношения между производными типами. Например, если `C`
      ковариантен по `T`, то `C[Dog]` является подтипом `C[Animal]` (при условии, что `Dog` подтип `Animal`). Это важно
      для корректного определения подтипов в обобщенных классах.

3. **Generic:**
    - Класс, наследующийся от `Generic[T]`, где `T` — это TypeVar, становится обобщенным.
    - Можно использовать несколько TypeVar: `Generic[T, U]`.
    - Внутри класса можно использовать `T` как обычный тип в аннотациях методов и атрибутов.
    - При наследовании от обобщенного класса можно либо конкретизировать тип (`ChildClass[int]`), либо передать TypeVar
      дальше.

4. **Для AQA:**
    - Type hints помогают документировать интерфейсы тестовых утилит и фикстур.
    - `Optional` часто используется для необязательных аргументов в функциях-фикстурах.
    - `Union` полезен, когда функция может возвращать разные типы данных в зависимости от условий (например,
      `Union[WebElement, None]` при поиске элемента).
    - `Generic` и `TypeVar` позволяют создавать гибкие, переиспользуемые компоненты тестовых фреймворков, например,
      абстрактный репозиторий для тестовых данных `Repository[T]`, где `T` — тип модели.

# ## **Senior Level**

На этом уровне мы рассматриваем систему типизации как формальную систему, которая взаимодействует с механизмами Python и
метапрограммированием.

1. **Optional и Union как алгебраические типы:**
    - В теории типов `Union` является суммой типов (sum type), а кортеж — произведением типов (product type). Это основа
      для построения сложных типов данных.
    - `Optional` — это частный случай суммы типа с единичным типом (`None`).
    - В Python 3.10 появился `TypeGuard` и `isinstance` с `Union` теперь работает лучше, но все еще есть ограничения.

2. **TypeVar и вариативность:**
    - **Инвариантность по умолчанию:** `List[T]` инвариантен: `List[Dog]` не является ни подтипом, ни надтипом
      `List[Animal]`, потому что список можно изменять. Это предотвращает ошибки типа "добавление кота в список собак".
    - **Ковариантность:** `Sequence[T]` (как и `Iterable[T]`) может быть ковариантен, потому что он только читает
      данные. Если `Dog` подтип `Animal`, то `Sequence[Dog]` подтип `Sequence[Animal]`.
    - **Контравариантность:** `Callable[[Animal], ...]` может быть контравариантен по аргументам. Функция, которая
      принимает `Animal`, может принимать и `Dog`, поэтому `Callable[[Animal], ...]` является подтипом
      `Callable[[Dog], ...]` (обратное отношение).
    - Правильное указание вариативности в пользовательских дженериках критично для корректной работы статических
      анализаторов.

3. **Generic и `__class_getitem__`:**
    - Когда вы пишете `List[int]`, это не создает новый класс. Вместо этого вызывается метод `__class_getitem__` класса
      `List`, который возвращает объект-алиас с сохраненной информацией о параметре типа.
    - Пользовательские классы могут реализовать `__class_getitem__` для поддержки такой нотации, не наследуясь от
      `Generic`. Но наследование от `Generic` автоматически предоставляет эту реализацию и другие возможности.
    - `Generic` использует метаклассы и магию, чтобы сохранить параметры типов в `__parameters__` и `__args__`.

4. **TypeVar и `NewType`:**
    - `NewType` создает непрозрачный псевдоним типа, который статически считается подтипом исходного типа, но в runtime
      это отдельный тип. Полезно для предотвращения логических ошибок (например, `UserId = NewType('UserId', int)`).
    - `TypeVar` же используется для параметризации, а не создания новых типов.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Создание типобезопасных DSL для тестов:** Используя `Generic` и `TypeVar`, можно создать DSL, который статически
      проверяет цепочки вызовов. Например, `browser.element(by.id("login")).click()` — если `element` возвращает
      `WebElement[T]`, где `T` — это тип элемента (кнопка, поле ввода), то можно гарантировать, что методы `click()` или
      `send_keys()` доступны только для соответствующих типов.
    - **Генерация тестов на основе типов:** Метакласс может анализировать аннотации типов тестовых методов и
      автоматически генерировать тесты для разных типов данных. Например, для параметризованного теста
      `test_sort[T: (int, str)]` можно сгенерировать два теста.
    - **Валидация конфигураций тестов:** Используя `TypedDict` и `Literal`, можно статически проверять конфигурационные
      файлы тестов (YAML/JSON) на соответствие схеме.
    - **Мокирование с проверкой типов:** Создание моков, которые наследуются от `Generic[T]` и проверяют, что
      возвращаемые значения соответствуют аннотациям. Можно использовать `__annotations__` реального класса для
      автоматической настройки мока.
    - **Протоколы и дженерики:** Комбинирование `Protocol` и `Generic` для описания обобщенных интерфейсов. Например,
      `class Repository[T, U]: ...` — где `T` — тип модели, `U` — тип идентификатора. Это позволяет создавать
      типобезопасные абстракции для тестовых данных.
    - **Зависимые типы (эмуляция):** Хотя Python не поддерживает зависимые типы полноценно, можно использовать `Literal`
      и `TypeVar` с bound для эмуляции простых случаев. Например, функция, которая возвращает длину списка, может быть
      аннотирована так, чтобы статический анализатор понимал, что индекс не выйдет за границы.
    - **Проблемы с рекурсивными типами:** Определение рекурсивных структур данных (например, дерева) требует
      использования строковых аннотаций (`'Tree'`) или `from __future__ import annotations`. В тестовых фреймворках это
      может возникать при описании сложных JSON-схем.
    - **Производительность:** Использование `typing` модуля может замедлить запуск программы, потому что он содержит
      много сложной логики. В продакшн-коде иногда используют `from typing import TYPE_CHECKING` для условного импорта.
      В тестах это менее критично, но стоит помнить.

6. **Ограничения и будущее:**
    - **PEP 563 (Postponed Evaluation of Annotations):** Строковые аннотации по умолчанию с Python 3.10 (в 3.7—3.9 через
      `from __future__ import annotations`). Это решает проблему forward references, но делает аннотации недоступными
      для интроспекции в runtime (нужно использовать `get_type_hints`).
    - **PEP 646 (Variadic Generics):** В Python 3.11 добавлены `TypeVarTuple` и `Unpack` для работы с произвольным
      количеством типов (например, для многомерных массивов).
    - **PEP 675 (LiteralString):** Для более точной типизации строковых литералов, полезно в SQL-запросах или командах
      shell, чтобы предотвранить injection.
    - **PEP 655 (Required/NotRequired для TypedDict):** Для указания обязательных и необязательных ключей.

- [Содержание](#содержание)

---

# **Literal, TypedDict, Protocol**

## **Junior Level**

**Literal** позволяет указать конкретное значение, которое может принимать переменная. Например,
`Literal["GET", "POST"]` означает, что параметр может быть только строкой "GET" или "POST". Это полезно для ограничения
допустимых значений.

**TypedDict** позволяет описывать структуру словаря с конкретными ключами и типами их значений. Это как схема для
словаря. Например, можно описать тип пользователя: `{"name": str, "age": int}`. Статические анализаторы будут проверять,
что словарь имеет именно такую структуру.

**Protocol** (о котором уже говорили) позволяет описывать интерфейсы через набор методов и атрибутов, которые должны
быть у объекта. Объект считается соответствующим протоколу, если у него есть все указанные методы и атрибуты, даже если
он не наследуется от протокола явно.

Для QA инженера эти инструменты помогают четко описывать форматы тестовых данных, конфигураций и ожидаемых структур
ответов, а также создавать типобезопасные моки и заглушки.

# ## **Middle Level**

Рассмотрим технические особенности каждой конструкции:

1. **Literal:**
    - Введен в PEP 586 (Python 3.8).
    - Может содержать конкретные значения: строки, числа, булевы значения, `None`.
    - Часто используется с `Union` для ограничения допустимых значений параметра.
    - Пример: `def request(method: Literal["GET", "POST", "PUT"]) -> Response: ...`
    - Статические анализаторы проверяют, что передаваемые значения входят в указанный набор.

2. **TypedDict:**
    - Введен в PEP 589 (Python 3.8).
    - Определяется через наследование от `TypedDict` или с использованием синтаксиса `TypedDict()`.
    - Поддерживает обязательные и необязательные ключи (через `total=False` и `NotRequired` в Python 3.11+).
    - Пример:
      ```python
      class User(TypedDict):
          name: str
          age: int
          email: NotRequired[str]  # Python 3.11+
      ```
    - В рантайме `TypedDict` ведет себя как обычный `dict`, но статические анализаторы используют его для проверки
      структуры.

3. **Protocol:**
    - Введен в PEP 544 (Python 3.8).
    - Определяет интерфейс через аннотации методов и атрибутов.
    - Может быть параметризован с помощью `Generic`.
    - Декоратор `@runtime_checkable` позволяет использовать `isinstance()` для проверки соответствия протоколу (но
      проверяет только наличие атрибутов, не их типы).
    - Пример:
      ```python
      class SupportsClose(Protocol):
          def close(self) -> None: ...
      ```

4. **Для AQA:**
    - **Literal:** Типизация параметров тестов (например, имена браузеров, окружения).
    - **TypedDict:** Описание форматов JSON-ответов API, конфигураций тестового окружения, фикстур.
    - **Protocol:** Определение интерфейсов для зависимостей, которые нужно мокировать. Создание абстракций для тестовых
      утилит.

# ## **Senior Level**

Глубокий анализ внутренней механики и продвинутое применение:

1. **Literal и зависимые типы:**
    - `Literal` может использоваться для уточнения типов в зависимости от других значений (эмуляция зависимых типов).
    - В сочетании с `@overload` и `TypeGuard` позволяет создавать точные аннотации для функций с ветвлением.
    - **Проблема:** `Literal` значения не считаются подтипами базового типа (`Literal["GET"]` не подтип `str`) для
      статического анализатора, что может привести к неожиданностям.
    - **Для AQA:** Использование `Literal` для параметризации тестов гарантирует, что тестовые значения принадлежат
      допустимому набору. Можно создавать перечисления тестовых сценариев.

2. **TypedDict и динамическая валидация:**
    - `TypedDict` использует метаклассы для создания классов-схем.
    - Атрибуты `__required_keys__` и `__optional_keys__` доступны для интроспекции.
    - **Проблема:** Наследование `TypedDict` работает, но требует осторожности при переопределении ключей.
    - **Интеграция с runtime-валидацией:** Библиотеки типа `pydantic` или `marshmallow` могут использовать `TypedDict`
      для генерации схем валидации.
    - **Для AQA:** Создание самодокументирующихся и валидируемых конфигураций тестового фреймворка. Автогенерация
      тестовых данных на основе схем.

3. **Protocol и метаклассы:**
    - `Protocol` наследуется от `ABCMeta`, что делает его метаклассом.
    - Декоратор `@runtime_checkable` добавляет метод `__instancecheck__`, который проверяет наличие атрибутов через
      `hasattr()`.
    - **Generic протоколы:** Позволяют создавать параметризованные интерфейсы. Например,
      `class Repository[T](Protocol): ...`.
    - **Проблема:** `@runtime_checkable` не проверяет сигнатуры методов и типы атрибутов. Это ограничение.
    - **Для AQA:** Создание библиотеки типобезопасных моков. Мок, созданный на основе протокола, будет иметь все
      необходимые методы, и IDE сможет предоставлять автодополнение.

4. **Комбинирование конструкций:**
    - Создание сложных типов: `TypedDict`, где некоторые значения имеют тип `Literal`.
    - Протоколы, требующие наличия атрибутов определенного `TypedDict` типа.
    - Пример: `class APIResponse(TypedDict): status: Literal["success", "error"]`.

5. **Ограничения и обходные пути:**
    - **Производительность:** Большое количество `TypedDict` и `Protocol` может замедлить статический анализ.
    - **Динамические ключи:** `TypedDict` не подходит для словарей с динамическими ключами. Для этого есть
      `typing.TypedDict` с `total=False`.
    - **Рекурсивные типы:** Для рекурсивных `TypedDict` или `Protocol` требуются строковые аннотации.

6. **Для AQA (Кровь, кишки и безумие):**
    - **Создание DSL для тестов:** Используя `Protocol` и `Literal`, можно построить DSL, который статически проверяет
      корректность цепочек вызовов. Например, `page.form().fill().submit()` — если `form()` возвращает объект,
      соответствующий протоколу `Form`, то доступны только методы `fill()` и `submit()`.
    - **Генерация тестов из OpenAPI/Swagger:** На основе схемы API (которая сама по себе `TypedDict`-подобна) можно
      генерировать типизированные клиенты и тесты.
    - **Валидация тестовых сценариев:** Статическая проверка, что тестовые сценарии (описанные как `TypedDict`)
      соответствуют ожидаемой структуре. Например, сценарий теста UI:
      `{"action": Literal["click", "fill"], "element": str, ...}`.
    - **Мокирование внешних сервисов:** Создание моков, которые точно соответствуют протоколу внешнего сервиса.
      Использование `TypedDict` для описания форматов запросов и ответов.
    - **Статический анализ тестовых данных:** Написание скриптов, которые с помощью `TypedDict` проверяют, что тестовые
      данные (JSON, YAML) соответствуют ожидаемой структуре. Интеграция этой проверки в CI/CD.
    - **Протоколы для плагинов:** Описание интерфейсов плагинов для тестового фреймворка через `Protocol`. Это позволяет
      сторонним плагинам быть типобезопасными.
    - **Селекторы и локаторы:** Использование `Literal` для типизации селекторов: `by: Literal["id", "xpath", "css"]`.

7. **Будущее:**
    - **PEP 655 (NotRequired/Required):** Улучшает работу с необязательными ключами в `TypedDict`.
    - **PEP 681 (Data Class Transforms):** Упрощает создание классов, похожих на `TypedDict`.
    - **PEP 692 (TypedDict для **kwargs):** Позволит типизировать `**kwargs` с помощью `TypedDict`.

- [Содержание](#содержание)

---

# **Ковариантность, контравариантность**

## **Junior Level*

Ковариантность и контравариантность — это понятия из теории типов, которые описывают, как отношения между типами (
например, "является подтипом") переносятся на более сложные типы, содержащие эти типы (например, на списки или функции).

Представьте, что у вас есть иерархия классов: `Animal` -> `Dog` (собака — это животное). Теперь рассмотрим контейнеры:

- **Ковариантность** означает, что если `Dog` является подтипом `Animal`, то `Container[Dog]` является подтипом
  `Container[Animal]`. То есть контейнер сохраняет направление отношения. Это безопасно, если контейнер только читает
  данные (например, итератор).
- **Контравариантность** означает обратное: если `Dog` является подтипом `Animal`, то `Container[Animal]` является
  подтипом `Container[Dog]`. То есть контейнер инвертирует отношение. Это имеет смысл для контейнеров, которые только
  записывают данные (например, функция, которая принимает аргументы).

Простой пример: если у вас есть функция, которая может принять любое `Animal`, то она может принять и `Dog` (
контравариантность по аргументу). Если функция возвращает `Dog`, то эта возвращаемая собака также является `Animal` (
ковариантность по возвращаемому значению).

Для QA инженера понимание этих концепций важно для создания гибких и типобезопасных тестовых фреймворков, особенно при
работе с дженериками и моками.

# ## **Middle Level**

Технически, ковариантность и контравариантность определяются в параметризованных (generic) типах. В Python они задаются
с помощью `TypeVar` с соответствующими флагами.

1. **Инвариантность (по умолчанию):**
    - Если `TypeVar('T')` объявлен без указания `covariant` или `contravariant`, то `Generic[T]` инвариантен.
    - Это значит, что `Container[Dog]` не является ни подтипом, ни надтипом `Container[Animal]`, даже если `Dog` —
      подтип `Animal`.
    - Пример: `List[T]` инвариантен, потому что список можно и читать, и изменять. Если бы он был ковариантен, то можно
      было бы присвоить `List[Dog]` переменной типа `List[Animal]` и добавить `Cat`, что привело бы к ошибке типа.

2. **Ковариантность:**
    - Объявляется как `TypeVar('T', covariant=True)`.
    - Пример: `Iterable[T]` ковариантен, потому что он только производит (yield) значения типа `T`. Если `Dog` подтип
      `Animal`, то `Iterable[Dog]` можно использовать везде, где требуется `Iterable[Animal]`.

3. **Контравариантность:**
    - Объявляется как `TypeVar('T', contravariant=True)`.
    - Пример: `Callable[[T], ...]` контравариантен по параметру `T`. Функция, которая может обработать любое `Animal`,
      может обработать и `Dog`. Поэтому `Callable[[Animal], ...]` является подтипом `Callable[[Dog], ...]`.

4. **Правила вариативности:**
    - Ковариантные типовые параметры могут использоваться только в "выходных" позициях (например, возвращаемый тип
      метода).
    - Контравариантные типовые параметры могут использоваться только во "входных" позициях (например, аргументы метода).
    - Инвариантные параметры могут использоваться и там, и там.

5. **Для AQA:**
    - При создании собственных generic-классов для тестовых фреймворков (например, `Repository[T]`) важно правильно
      указать вариативность, чтобы обеспечить типобезопасность.
    - Понимание вариативности помогает правильно аннотировать моки и стабы, особенно когда они используются в
      полиморфных контекстах.

# ## **Senior Level**

Глубокий анализ ковариантности и контравариантности в системе типов Python и их последствий для проектирования
фреймворков.

1. **Теоретические основы и Liskov Substitution Principle (LSP):**
    - Вариативность — это формальное выражение принципа подстановки Барбары Лисков для параметризованных типов.
    - Ковариантность гарантирует, что если `A` — подтип `B`, то `F[A]` — подтип `F[B]` (при условии, что `F`
      ковариантен).
    - Контравариантность гарантирует, что если `A` — подтип `B`, то `F[B]` — подтип `F[A]`.
    - Эти правила обеспечивают безопасность типов при использовании полиморфизма.

2. **Система типов Python и вариативность:**
    - Python использует структурную типизацию для протоколов, что делает вариативность еще более важной.
    - Например, протокол `SupportsLessThan[T]` может быть ковариантен по `T`, потому что если `Dog` можно сравнивать, то
      и `Animal` (если `Dog` — подтип `Animal`) можно сравнивать? На самом деле, это не всегда так, и поэтому
      вариативность должна быть определена аккуратно.
    - Статические анализаторы (mypy, pyright) проверяют вариативность и выдают ошибки, если ковариантный параметр
      появляется в контравариантной позиции (и наоборот).

3. **Вариативность и исключения:**
    - В Python исключения ковариантны. Это означает, что `Exception[Dog]` является подтипом `Exception[Animal]`. Это
      безопасно, потому что исключения обычно только читаются (их ловят и обрабатывают).
    - Однако, если бы исключения могли изменяться, это могло бы привести к проблемам. Но поскольку исключения в Python
      неизменяемы (immutable) после создания, ковариантность безопасна.

4. **Практические проблемы и обходные пути:**
    - **Проблема с изменяемыми данными:** Изменяемые контейнеры не могут быть ковариантными или контравариантными,
      потому что это привело бы к нарушению типобезопасности. Поэтому `list` инвариантен.
    - **Обходной путь:** Использование неизменяемых типов, таких как `Sequence` (ковариантен) или `Mapping` (ковариантен
      по значениям, но инвариантен по ключам).
    - **Self-тип:** Паттерм `Self` (введенный в Python 3.11) позволяет методам возвращать экземпляр текущего класса, что
      полезно для fluent-интерфейсов. `Self` является ковариантным.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Создание типобезопасных API для тестов:** При проектировании фреймворка для тестирования API можно использовать
      ковариантность для представления иерархий ответов. Например, `APIResponse[Success]` и `APIResponse[Error]` могут
      быть подтипами `APIResponse[Any]`. Это позволяет писать общие обработчики ответов.
    - **Мокирование с вариативностью:** При создании моков для generic-интерфейсов важно учитывать вариативность.
      Например, если интерфейс `Repository[T]` инвариантен, то мок для `Repository[Dog]` не может быть использован там,
      где ожидается `Repository[Animal]`, даже если `Dog` — подтип `Animal`. Это может потребовать создания отдельных
      моков для каждого типа.
    - **Параметризация тестов:** Используя вариативность, можно создавать параметризованные тесты, которые безопасно
      работают с иерархиями типов. Например, тест для `Animal` должен работать и для `Dog`, если только тест не изменяет
      объект (в этом случае требуется инвариантность).
    - **Статическая проверка тестовых сценариев:** С помощью правильной вариативности в аннотациях можно заставить
      статический анализатор проверять, что тестовые данные соответствуют ожидаемым типам, даже при использовании
      полиморфизма.
    - **Проблема с двойной вариативностью:** Некоторые структуры данных, такие как `Comparable`, могут требовать, чтобы
      тип был одновременно и ковариантным, и контравариантным. Это невозможно в большинстве систем типов. В Python это
      решается через протоколы с `Self` или через перегрузку (overloading).
    - **Использование `typing.covariant_check` и `contravariant_check`:** Эти декораторы (гипотетические) могли бы
      использоваться для проверки вариативности в runtime, но в стандартной библиотеке их нет. Однако можно создать свои
      проверки, используя `isinstance` и `issubclass`.

6. **Ограничения и будущее:**
    - **Производительность:** Проверка вариативности во время статического анализа может быть сложной и замедлять
      анализ.
    - **Динамическая природа Python:** В runtime информация о типах стирается (type erasure), поэтому вариативность
      важна только для статической проверки.
    - **PEP 484 и последующие:** Постепенное улучшение поддержки вариативности в системе типов Python.

- [Содержание](#содержание)

---

# **Pytest**

## **Junior Level*

Pytest — это современный фреймворк для тестирования в Python, который делает написание и выполнение тестов простым,
интуитивным и эффективным. В отличие от стандартного модуля unittest, pytest требует меньше шаблонного кода и предлагает
более мощные возможности.

Ключевые преимущества:

1. **Простой синтаксис:** Тесты пишутся как обычные функции с префиксом `test_`, а проверки — с помощью оператора
   `assert`. Не нужно запоминать множество методов вроде `assertEqual`, `assertTrue`.
2. **Фикстуры (Fixtures):** Механизм для подготовки и очистки тестового окружения. Фикстуры объявляются декоратором
   `@pytest.fixture` и могут использоваться в тестах путем указания их имен в параметрах функции.
3. **Параметризация:** Легко запустить один тест с разными наборами данных с помощью декоратора
   `@pytest.mark.parametrize`.
4. **Плагины:** Богатая экосистема плагинов расширяет возможности pytest (например, `pytest-cov` для измерения покрытия,
   `pytest-xdist` для параллельного запуска).
5. **Подробные отчеты:** При падении теста pytest предоставляет детальную информацию, что упрощает отладку.

Для QA инженера pytest — это основной инструмент для создания автоматизированных тестов, от простых unit-тестов до
сложных интеграционных и системных проверок.

# ## **Middle Level**

Технически pytest — это сложная система, построенная вокруг нескольких ключевых концепций:

1. **Архитектура запуска:**
    - Основная точка входа — функция `pytest.main()`, которая вызывает внутренний механизм, собирающий тесты,
      выполняющий их и формирующий отчет.
    - Процесс включает **обнаружение тестов** (сканирование файлов и каталогов по соглашениям), **сбор тестов** (
      создание объектов `Item` для каждого теста) и **выполнение** (запуск тестов с учетом фикстур и хуков).

2. **Фикстуры (Fixtures):**
    - Это функции, помеченные декоратором `@pytest.fixture`. Они могут возвращать данные или объекты, которые затем
      передаются в тестовые функции.
    - **Области видимости (scope):** Фикстуры могут быть уровня функции, класса, модуля или сессии, что определяет, как
      часто они создаются и уничтожаются.
    - **Зависимости фикстур:** Фикстуры могут зависеть от других фикстур, образуя граф зависимостей, который pytest
      разрешает автоматически.
    - **Finalizer и yield:** Фикстуры могут использовать `yield` для разделения кода установки и очистки (альтернатива
      `addfinalizer`).

3. **Хуковая система (Hook System):**
    - Pytest построен на плагинах, которые могут перехватывать и изменять его поведение через **хуки**. Хуки определены
      в `pytest.hookspec` и реализуются плагинами.
    - Примеры хуков: `pytest_collection_modifyitems` (для изменения списка тестов), `pytest_runtest_setup` (выполняется
      перед каждым тестом).
    - Эта система позволяет глубоко кастомизировать процесс тестирования.

4. **Маркеры (Markers):**
    - Декоратор `@pytest.mark` позволяет помечать тесты для выборочного запуска (например, `@pytest.mark.slow`).
    - Можно создавать собственные маркеры и использовать их для фильтрации или передачи метаданных.

5. **Параметризация:**
    - `@pytest.mark.parametrize` создает несколько тестовых случаев из одной функции. Каждый набор параметров становится
      отдельным тестовым случаем в отчете.

6. **Для AQA:**
    - **Интеграция с Allure:** Генерация детальных отчетов через `pytest-allure`.
    - **Параллельный запуск:** Использование `pytest-xdist` для ускорения выполнения тестов.
    - **Мокирование:** Хотя pytest сам не предоставляет моки, он хорошо интегрируется с `unittest.mock`.

# ## **Senior Level**

Глубокое погружение во внутреннюю механику pytest, расширяемость и интеграция с экосистемой Python.

1. **Архитектура на основе плагинов:**
    - Сам pytest реализован как набор **встроенных плагинов**. Даже базовые функции, такие как сбор тестов и выполнение,
      являются плагинами.
    - **Загрузка плагинов:** Плагины могут быть зарегистрированы через `setup.cfg`, `pyproject.toml`, аргументы
      командной строки или автоматически обнаружены через setuptools entry points.
    - **Конфликты плагинов:** При наличии нескольких плагинов, переопределяющих одни и те же хуки, порядок их выполнения
      определяется порядком загрузки и может быть критичным.

2. **Механизм фикстур: как работает внедрение зависимостей:**
    - **Разрешение зависимостей:** При обнаружении тестовой функции pytest анализирует ее сигнатуру и ищет имена
      параметров среди зарегистрированных фикстур. Затем он строит **граф зависимостей** и определяет порядок создания
      фикстур, учитывая их области видимости.
    - **Кэширование фикстур:** Фикстуры с областью видимости выше, чем `function`, кэшируются. Например, фикстура
      `session` создается один раз и переиспользуется. Это реализовано через объект `FixtureCache`.
    - **Время жизни фикстур и очистка:** При использовании `yield` в фикстуре, код после `yield` выполняется в момент
      окончания области видимости. Это не просто генератор, а специальная обработка внутри pytest, которая гарантирует
      выполнение финализатора даже при падении теста.

3. **Сбор тестов и интроспекция:**
    - **Объекты Node:** Pytest представляет тестовую сессию как дерево объектов: `Session` -> `Collection` -> `Item` (
      тест). Каждый узел соответствует каталогу, файлу, классу или функции.
    - **Плагин `pytest_pycollect`:** Отвечает за сбор тестов из Python-модулей. Он использует интроспекцию для поиска
      функций и классов, соответствующих шаблонам.
    - **Кастомизация сбора:** Можно написать плагин, который изменяет процесс сбора, например, добавляет тесты из
      не-Python файлов или динамически генерирует тесты на основе внешних данных.

4. **Хуковая система и взаимодействие с event loop:**
    - **Асинхронная поддержка:** Pytest поддерживает асинхронные тесты через плагин `pytest-asyncio`. Он предоставляет
      фикстуру `event_loop` и маркер `@pytest.mark.asyncio`.
    - **Внутренности:** Асинхронные фикстуры и тесты требуют особой обработки, так как хуки pytest сами не являются
      асинхронными. Плагин `pytest-asyncio` использует свой собственный цикл событий и оборачивает асинхронные функции.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Динамическое создание фикстур:** В продвинутых сценариях можно создавать фикстуры программно во время
      выполнения, используя `pytest.fixture` как обычную функцию и регистрируя ее через `request.addfixturedef`. Это
      позволяет создавать фикстуры на основе конфигурации или внешних данных.
    - **Кастомизация отчетов через хуки:** Хуки `pytest_runtest_makereport` и `pytest_terminal_summary` позволяют
      перехватывать результаты тестов и формировать кастомные отчеты, интегрируясь с внешними системами (например,
      TestRail, JIRA).
    - **Распределенное тестирование:** Используя `pytest-xdist`, можно распределять тесты по нескольким worker'ам. При
      этом каждая worker-нода имеет свою копию фикстур сессии, что может привести к избыточному созданию ресурсов. Для
      оптимизации можно использовать фикстуры с областью `"worker"` (только в xdist).
    - **Интеграция с контейнеризацией:** Запуск тестов в Docker-контейнерах с помощью кастомного плагина, который через
      хук `pytest_configure` поднимает инфраструктуру, а через `pytest_unconfigure` — останавливает.
    - **Мокирование на уровне импорта:** Плагины могут использовать `importlib` и `sys.modules` для подмены модулей до
      начала сбора тестов, что полезно для изоляции тестов от внешних зависимостей.
    - **Профилирование тестов:** Написание плагина, который с помощью `cProfile` или `py-spy` собирает метрики
      производительности для каждого теста и выводит топ медленных тестов.
    - **Валидация тестовых данных:** Использование хука `pytest_collection_modifyitems` для проверки, что все тесты
      имеют определенные маркеры или что параметризованные тесты используют валидные данные.
    - **Селективный запуск на основе изменений:** Плагин, который интегрируется с Git и анализирует diff, чтобы
      запускать только тесты, связанные с измененными файлами.
    - **Работа с базами данных:** Создание фикстур, которые управляют транзакциями БД, откатывают изменения после
      каждого теста (используя `rollback`) или применяют миграции для тестовой базы.

6. **Ограничения и граничные случаи:**
    - **Циклические зависимости фикстур:** Pytest обнаружит цикл и выдаст ошибку. Нужно перепроектировать фикстуры,
      введя третью, или использовать ленивую инициализацию.
    - **Параллелизм и состояние:** При использовании `pytest-xdist` нужно избегать разделяемого изменяемого состояния
      между worker'ами. Фикстуры `session` создаются в каждом worker отдельно.
    - **Моники-патчинг и фикстуры:** Если фикстура делает monkeypatch, то важно убедиться, что патч применяется и
      откатывается в правильной области видимости. Использование встроенной фикстуры `monkeypatch` решает эту проблему.
    - **Наследование фикстур в классах:** Фикстуры, определенные в родительском классе, не автоматически доступны в
      дочерних. Нужно явно объявить их или использовать миксины.

- [Содержание](#содержание)

---

# **Pytest hooks**

## **Junior Level*

Pytest hooks (хуки) — это специальные функции, которые позволяют расширять и кастомизировать поведение pytest на разных
этапах выполнения тестов. Если представить pytest как кинотеатр, то хуки — это моменты, когда можно вставить свою
рекламу или изменить сценарий: перед началом сеанса, во время показа или после его завершения.

Хуки позволяют плагинам (включая ваши собственные) вмешиваться в процесс тестирования: изменять список тестов, добавлять
дополнительную обработку перед или после каждого теста, модифицировать отчеты, интегрироваться с внешними системами. Для
QA инженера понимание хуков открывает возможность создания кастомных плагинов для специфичных нужд проекта: интеграция с
системой отчетности, подготовка тестового окружения, сбор дополнительных метрик.

# ## **Middle Level**

Технически, хуки — это часть архитектуры pytest, построенной на библиотеке `pluggy`. Это система точек расширения, где
каждая точка соответствует определенному этапу жизненного цикла тестов.

1. **Система плагинов и pluggy:**
    - Pytest сам является набором встроенных плагинов, которые регистрируют и используют хуки.
    - `pluggy` — это отдельная библиотека, реализующая механизм «хук-спецификаций» и «хук-имплементаций». Она управляет
      обнаружением, регистрацией и вызовом хуков.

2. **Типы хуков:**
    - **Хуки настройки/завершения:** `pytest_configure`, `pytest_unconfigure`. Вызываются при инициализации и завершении
      сессии.
    - **Хуки сбора тестов:** `pytest_collection_modifyitems`, `pytest_collection_finish`. Позволяют фильтровать,
      переупорядочивать или модифицировать собранные тесты.
    - **Хуки выполнения тестов:** `pytest_runtest_setup`, `pytest_runtest_call`, `pytest_runtest_teardown`. Вызываются
      соответственно перед тестом, во время выполнения теста и после.
    - **Хуки отчетов:** `pytest_runtest_makereport`, `pytest_terminal_summary`. Позволяют создавать кастомные отчеты и
      выводить информацию в терминал.
    - **Хуки вызова:** `pytest_internalerror`, `pytest_keyboard_interrupt`. Обработка внутренних ошибок и прерываний.

3. **Реализация хуков:**
    - Хуки реализуются в плагинах (отдельных модулях или классах) как функции с именами, соответствующими спецификациям.
    - Плагин регистрирует свои хуки автоматически при загрузке (через entry points) или вручную через `pytest.addhooks`.
    - Хуки могут иметь параметры, которые pytest передает в них (например, `session`, `item`, `report`).

4. **Примеры использования для AQA:**
    - **Автоматическая маркировка тестов:** Хук `pytest_collection_modifyitems` может анализировать имена тестов и
      автоматически помечать их как `@pytest.mark.slow` или `@pytest.mark.integration`.
    - **Динамическое добавление тестов:** Хук `pytest_generate_tests` позволяет генерировать параметризованные тесты на
      основе внешних данных.
    - **Кастомная отчетность:** Хук `pytest_runtest_makereport` позволяет добавлять в отчет дополнительную информацию (
      скриншоты, логи, метрики производительности).
    - **Интеграция с внешними системами:** Хуки `pytest_sessionstart` и `pytest_sessionfinish` могут отправлять
      уведомления в Slack, JIRA или обновлять дашборды.

# ## **Senior Level**

Глубокий анализ системы хуков, их взаимодействия с внутренней архитектурой pytest и продвинутые сценарии использования.

1. **Архитектура pluggy и порядок вызова:**
    - **HookspecMarker и HookimplMarker:** `pluggy` использует декораторы `@hookspec` для объявления спецификации хука (
      что он делает, какие параметры принимает) и `@hookimpl` для реализации.
    - **Приоритеты и ordering:** Реализации хуков могут иметь модификаторы `tryfirst=True` или `trylast=True`, чтобы
      управлять порядком вызова среди нескольких плагинов.
    - **Hook wrappers:** Особый тип реализации, оборачивающий вызов других реализаций. Хук-обертка получает генератор,
      который `yield`'ит результат остальных реализаций. Это позволяет выполнить код до и после основного вызова хука (
      аналогично `around` advice в AOP).
    - **Пример:** Хук `pytest_runtest_makereport` является hook wrapper, позволяя плагинам модифицировать отчет до и
      после его создания.

2. **Внутренний объект `pytest` и `config`:**
    - Объект `pytest` (или `config` в хуках) — это центральный реестр, содержащий все зарегистрированные плагины, хуки и
      состояние сессии.
    - Плагины могут добавлять свои атрибуты в `config`, чтобы делиться данными между хуками (например,
      `config.myplugin_data = {}`).
    - `config.hook` — это объект `pluggy.PluginManager`, через который происходит вызов всех хуков.

3. **Хуки и жизненный цикл тестового элемента (Item):**
    - Каждый тест (функция, метод) представляется объектом `Item`. Хуки `pytest_runtest_*` получают этот объект.
    - Внутри `Item` есть `_request` — объект, содержащий контекст выполнения, включая фикстуры.
    - Хуки могут модифицировать `Item` (например, добавлять пользовательские атрибуты через `item.user_properties`),
      которые затем могут быть использованы в отчетах.

4. **Динамическая регистрация хуков:**
    - Плагины могут регистрировать новые хуки во время выполнения через `pluggy.PluginManager.add_hookspecs`. Это
      позволяет создавать плагины, которые расширяют не только поведение, но и интерфейс pytest.
    - Однако, такой подход требует глубокого понимания архитектуры и может привести к конфликтам.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Создание DSL через хуки:** Плагин может через хук `pytest_pycollect_makeitem` перехватывать сбор тестов и
      преобразовывать функции с определенными декораторами в кастомные тестовые объекты. Например, можно создать
      синтаксис `@scenario("ID-123")`, который будет превращать функцию в тест-кейс с привязанным ID из TestRail.
    - **Распределенное выполнение тестов:** Плагин, использующий `pytest_collection_modifyitems` для разделения тестов
      на группы (sharding) и `pytest_sessionstart` для координации между несколькими инстансами runner'ов через
      распределенную очередь (Redis, RabbitMQ). Хук `pytest_runtest_protocol` может быть полностью переопределен для
      отправки теста на выполнение в удаленный worker.
    - **Адаптивное тестирование:** Плагин, который в `pytest_runtest_makereport` анализирует результаты и на лету
      изменяет план тестирования (например, при падении smoke-теста, пропускает dependent тесты). Это требует сохранения
      состояния между вызовами хуков через `config` или внешнее хранилище.
    - **Глубокая интеграция с мониторингом:** Хук `pytest_runtest_call` может оборачивать выполнение теста в span
      трассировки (OpenTelemetry), отправляя метрики длительности, а `pytest_terminal_summary` — агрегировать и выводить
      перформанс-дайджест.
    - **Валидация тестового кода:** Плагин, использующий хук `pytest_collect_file` для анализа не только Python-файлов,
      но и конфигурационных (YAML, JSON) на соответствие схеме, и падения сборки тестов при нарушении.
    - **Кастомные фикстуры на уровне плагина:** Хук `pytest_fixture_setup` позволяет перехватывать создание фикстур и
      модифицировать их поведение глобально (например, добавлять автоматическое логирование для всех фикстур с
      определенным маркером).
    - **Интроспекция и отладка:** Плагин для отладки самого pytest, который регистрирует все хуки и логирует их вызовы с
      параметрами. Это помогает понять порядок выполнения и взаимодействие плагинов в сложных конфигурациях.
    - **Динамическое управление ресурсами:** Плагин, который в `pytest_sessionstart` резервирует виртуальную машину в
      облаке, в `pytest_collection_modifyitems` фильтрует тесты, которые могут на ней выполняться, а в
      `pytest_sessionfinish` — освобождает ее. Состояние VM хранится в `config` и доступно в фикстурах через
      `request.config`.
    - **Поддержка новых языков/форматов:** Через хук `pytest_pycollect_makeitem` можно научить pytest собирать тесты из
      файлов, написанных на другом языке (например, Gherkin для BDD), преобразуя их в Python-объекты на лету.

6. **Опасности и тонкости:**
    - **Циклические зависимости и порядок:** Неправильный порядок вызова хуков может привести к неожиданному поведению.
      Например, если плагин A зависит от данных, которые подготавливает плагин B, нужно обеспечить правильный порядок
      через `tryfirst`/`trylast` или явные зависимости плагинов.
    - **Производительность:** Слишком много хуков, особенно тех, которые выполняют тяжелые операции (сетевые запросы,
      анализ файлов), могут сильно замедлить выполнение тестов.
    - **Совместимость:** При обновлении pytest сигнатуры хуков могут меняться. Плагины должны быть готовы к обратной
      совместимости или явно указывать версию pytest, которую они поддерживают.
    - **Отладка:** Ошибки внутри хуков могут быть трудноотлаживаемыми, так как они происходят глубоко внутри механизма
      pytest. Необходимо тщательное логирование и обработка исключений.

7. **Расширенные объекты в хуках:**
    - **`PytestPluginManager`:** Позволяет программно управлять плагинами (включать/выключать).
    - **`Node`:** Базовый класс для `Item`, `Collector`. Хуки часто получают узлы дерева тестов и могут перемещаться по
      нему (родитель, дети).
    - **`CallInfo`:** Объект, содержащий информацию о вызове теста: результат, исключение, длительность. Доступен в
      `pytest_runtest_makereport`.

- [Содержание](#содержание)

---

# **Kubernetes**

## **Junior Level*

Kubernetes (K8s) — это система для автоматизации развертывания, масштабирования и управления контейнеризированными
приложениями. Представьте, что у вас есть много контейнеров (как изолированных пакетов с вашим приложением), и вам нужно
управлять ими на множестве серверов. Kubernetes берет на себя эту задачу: он сам решает, где запускать контейнеры, как
распределять между ними нагрузку, как перезапускать их при сбоях и как обновлять без простоев.

Для QA инженера Kubernetes важен по нескольким причинам:

1. **Тестовые окружения:** Можно быстро создавать изолированные окружения для тестирования, которые точно повторяют
   продакшен.
2. **Масштабирование тестов:** Запускать тысячи тестов параллельно, используя возможности Kubernetes по управлению
   ресурсами.
3. **Инфраструктура для тестов:** Сами тестовые фреймворки и системы отчетности можно развертывать в Kubernetes как
   микросервисы.
4. **Тестирование в реалистичных условиях:** Тестировать приложение в той же среде, где оно будет работать.

# ## **Middle Level**

С технической точки зрения, Kubernetes состоит из нескольких ключевых компонентов, которые взаимодействуют через API.

1. **Архитектура кластера:**
    - **Control Plane (Master):** Управляющая нода, содержащая API Server, Scheduler, Controller Manager, etcd (
      хранилище конфигурации).
    - **Worker Nodes:** Ноды, на которых запускаются контейнеры. Каждая содержит kubelet (агент), kube-proxy (сетевой
      прокси) и container runtime (например, Docker).

2. **Основные объекты Kubernetes:**
    - **Pod:** Минимальная единица развертывания. Это один или несколько контейнеров, которые разделяют сеть и
      хранилище.
    - **Deployment:** Описывает желаемое состояние приложения и управляет обновлением и откатом версий.
    - **Service:** Абстракция для доступа к группе подов (обычно через балансировку нагрузки).
    - **ConfigMap и Secret:** Для управления конфигурацией и секретами.
    - **Namespace:** Виртуальный кластер внутри физического, для изоляции ресурсов.

3. **Для AQA:**
    - **Тестовые среды:** Использование Namespaces для изоляции тестовых окружений. Можно создать namespace для каждого
      тестового прогона.
    - **Запуск тестов в Pod'ах:** Тесты могут запускаться в отдельных Pod'ах как Job или CronJob. Это позволяет легко
      масштабировать и управлять выполнением тестов.
    - **Доступ к приложению:** Использование Services для доступа к тестируемому приложению, развернутому в кластере.
    - **Конфигурация тестов:** Использование ConfigMaps для передачи конфигурации тестов (например, URL приложения,
      учетные данные).

4. **Инструменты:**
    - **kubectl:** CLI для управления кластером.
    - **Helm:** Менеджер пакетов для Kubernetes, упрощающий развертывание сложных приложений.
    - **Minikube и Kind:** Инструменты для запуска локального кластера Kubernetes на машине разработчика.

# ## **Senior Level**

На этом уровне мы рассматриваем Kubernetes как сложную распределенную систему, понимаем ее внутренние механизмы и
используем для построения продвинутых тестовых инфраструктур.

1. **Control Plane и распределенный консенсус:**
    - **etcd:** Распределенное key-value хранилище, основанное на Raft consensus алгоритме. Вся конфигурация и состояние
      кластера хранятся здесь. Для тестовых кластеров важно понимать, что etcd может стать узким местом при интенсивной
      нагрузке (много изменений конфигурации, например, частые создания/удаления Pod'ов для тестов).
    - **API Server:** Центральный управляющий компонент, который валидирует и обрабатывает запросы. Все взаимодействие с
      кластером происходит через его REST API. Можно интегрировать тестовые фреймворки напрямую с API Server для
      мониторинга состояния развертываний.

2. **Scheduler и распределение нагрузки:**
    - Scheduler решает, на какой ноде запустить Pod, на основе политик, ресурсов и ограничений. Для тестовых нагрузок
      важно правильно настраивать ресурсы (requests/limits) для Pod'ов с тестами, чтобы не перегружать ноды и не влиять
      на другие приложения.
    - Можно создавать custom schedulers для специализированных сценариев, например, для запуска тестов на нодах с
      определенными характеристиками (GPU, SSD).

3. **Сеть в Kubernetes:**
    - **CNI (Container Network Interface):** Плагины, обеспечивающие сетевое взаимодействие между Pod'ами. Для
      тестирования сетевых политик (Network Policies) важно понимать, какой CNI используется в кластере.
    - **Service Mesh (Istio, Linkerd):** Надстройка над сетью Kubernetes, предоставляющая продвинутые возможности:
      трафик, наблюдение, безопасность. В тестировании можно использовать Service Mesh для управления трафиком во время
      тестов (например, перенаправление части запросов на канареечную версию).

4. **Хранилище:**
    - **Persistent Volumes (PV) и Persistent Volume Claims (PVC):** Механизм предоставления постоянного хранилища
      Pod'ам. Для тестов, которые требуют сохранения состояния (например, тесты базы данных), важно правильно
      настраивать PVC.
    - **Storage Classes:** Позволяют динамически предоставлять хранилище. Можно создать отдельный storage class для
      тестов, который использует быстрые, но недолговечные диски.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Динамическое создание тестовых окружений:** Использование операторов Kubernetes (Kubernetes Operators) для
      автоматического развертывания и управления состоянием тестовых окружений. Оператор может отслеживать custom
      resources (CRD) и создавать namespace, deployment, service и другие ресурсы для каждого тестового прогона.
    - **Распределенное выполнение тестов:** Запуск тестов как Job с параллельными Pod'ами. Использование очередей (
      например, RabbitMQ, Kafka) для координации между Pod'ами. Каждый Pod берет задание из очереди, выполняет тест и
      отправляет результат обратно.
    - **Нагрузочное тестирование в K8s:** Развертывание системы нагрузочного тестирования (например, Apache JMeter в
      кластере) как Deployment. Управление количеством Pod'ов-генераторов нагрузки в зависимости от требований теста.
      Использование Horizontal Pod Autoscaler для автоматического масштабирования генераторов нагрузки.
    - **Мониторинг и observability:** Интеграция тестовых прогонов с системой мониторинга кластера (Prometheus,
      Grafana). Сбор метрик не только с тестируемого приложения, но и с самого кластера (использование ресурсов, ошибки
      kubelet). Настройка алертинга на аномалии во время тестов.
    - **Тестирование отказоустойчивости:** Использование chaos engineering инструментов (например, Chaos Mesh, Litmus)
      для внесения сбоев в кластер во время выполнения тестов. Проверка, как приложение и тесты себя ведут при отказе
      ноды, сети или хранилища.
    - **Безопасность (Security Testing):** Использование возможностей Kubernetes для тестирования безопасности: Security
      Context, Pod Security Policies (устарели), Pod Security Standards. Запуск тестов на уязвимости контейнеров (Trivy,
      Clair) непосредственно в CI/CD пайплайне, интегрированном с Kubernetes.
    - **Миграция состояния тестов:** При тестировании stateful приложений (базы данных) использование снапшотов (
      snapshots) Persistent Volumes для быстрого восстановления состояния перед каждым тестом. Настройка Init Containers
      в Pod'ах тестов для подготовки данных.
    - **Интеграция с внешними системами:** Использование Kubernetes API для автоматического создания тестовых данных во
      внешних системах (например, в облачных сервисах AWS, GCP) через механизм External Secrets и External DNS.
    - **Оптимизация затрат:** Настройка node autoscaling (Cluster Autoscaler) для автоматического добавления и удаления
      нод в зависимости от нагрузки тестов. Использование spot/preemptible инстансов для тестовых нод, чтобы снизить
      стоимость. Гибкое управление ресурсами через ResourceQuotas и LimitRanges в namespace тестов.

6. **Проблемы и решения:**
    - **Сетевые задержки:** В распределенной среде сетевые задержки могут влиять на результаты тестов, особенно на
      производительность. Необходимо учитывать расположение Pod'ов с тестами и тестируемым приложением (желательно в
      одной зоне доступности).
    - **Очистка ресурсов:** После тестового прогона важно удалить все созданные ресурсы (namespace, persistent volumes)
      чтобы избежать утечек. Использование механизма ttl для Job или написание собственных cleanup скриптов, которые
      вызываются через хуки (pre-stop, post-start).
    - **Доступ к результатам тестов:** Сбор артефактов тестов (логи, скриншоты, отчеты) из Pod'ов. Использование sidecar
      контейнеров для отправки артефактов в объектное хранилище (S3, GCS) или в специализированную систему (
      Elasticsearch).
    - **Воспроизводимость тестовых сред:** Использование Infrastructure as Code (IaC) инструментов (Terraform,
      Crossplane) для описания кластера и GitOps (ArgoCD, Flux) для управления конфигурацией приложений. Это позволяет
      воспроизводить окружения для отладки проблем.

7. **Тренды и будущее:**
    - **Serverless поверх Kubernetes (Knative):** Запуск тестов как serverless функций, что позволяет еще более
      эффективно использовать ресурсы.
    - **eBPF:** Использование расширенных возможностей ядра Linux для наблюдения и безопасности без модификации
      приложений. Инструменты типа Cilium предоставляют мощные возможности для тестирования сетевых взаимодействий.
    - **WebAssembly (Wasm) в Kubernetes:** Возможность запуска тестов, скомпилированных в Wasm, что обеспечивает лучшую
      изоляцию и производительность.

- [Содержание](#содержание)

---

# **Пирамида тестирования**

## **Junior Level*

Пирамида тестирования — это концепция, которая визуализирует оптимальное соотношение различных типов автоматизированных
тестов в проекте. Она состоит из трех основных уровней:

1. **Unit-тесты (нижний уровень, основание пирамиды):** Тестируют отдельные компоненты системы (функции, классы) в
   полной изоляции. Их должно быть больше всего — они быстрые, дешевые в поддержке и дают мгновенную обратную связь.

2. **Интеграционные тесты (средний уровень):** Проверяют взаимодействие нескольких компонентов (модулей, сервисов, баз
   данных). Их меньше, чем unit-тестов — они медленнее, сложнее в поддержке, но проверяют критически важные
   взаимодействия.

3. **UI/E2E-тесты (верхний уровень, вершина пирамиды):** Тестируют систему с точки зрения конечного пользователя,
   проверяя полные сценарии работы. Их должно быть меньше всего — они самые медленные, хрупкие и дорогие в поддержке, но
   дают уверенность в работе системы в целом.

Цель пирамиды — создать сбалансированную стратегию тестирования: много быстрых и стабильных тестов внизу, меньше
медленных и комплексных наверху. Для QA инженера понимание этой концепции помогает планировать усилия по автоматизации,
распределять ресурсы и строить эффективный процесс тестирования.

# ## **Middle Level**

С технической точки зрения реализация каждого уровня пирамиды в Python-экосистеме имеет свои особенности:

1. **Unit-тестирование:**
    - **Инструменты:** `pytest`, `unittest`, `nose2`. Pytest стал де-факто стандартом благодаря гибкости и богатой
      экосистеме.
    - **Изоляция:** Использование моков (`unittest.mock`) для замены зависимостей. Ключевые техники: патчинг (`patch`),
      подмены (`MagicMock`, `AsyncMock`).
    - **Покрытие кода:** Инструменты `coverage.py` и `pytest-cov` для измерения покрытия.
    - **Параметризация:** Декоратор `@pytest.mark.parametrize` для запуска одного теста с разными входными данными.
    - **Важно:** Хороший unit-тест не зависит от внешних систем (БД, файловая система, сеть).

2. **Интеграционное тестирование:**
    - **Тестирование API:** Библиотеки `requests` + `pytest` для HTTP-API. Для асинхронных API — `aiohttp` или `httpx`.
    - **Тестирование БД:** Использование тестовых баз данных (например, SQLite in-memory) или механизмов транзакций с
      откатом после каждого теста. Инструменты: `pytest-django`, `factory_boy` для генерации данных.
    - **Тестирование микросервисов:** Использование тестовых дублей (test doubles) — заглушек (stubs) и моков для
      зависимых сервисов. Контейнеризация зависимостей (Docker) для запуска реальных сервисов в тестовом окружении.
    - **Фикстуры с областью видимости:** В pytest использование `@pytest.fixture(scope="module")` или
      `@pytest.fixture(scope="session")` для создания дорогих ресурсов (например, соединение с БД), которые
      переиспользуются между тестами.

3. **UI/E2E-тестирование:**
    - **Инструменты:** `Selenium WebDriver`, `Playwright`, `Cypress` (через `pytest-playwright`).
    - **Page Object Pattern:** Организация тестового кода через абстракции страниц/компонентов для уменьшения хрупкости
      и повышения переиспользуемости.
    - **Управление состоянием:** Создание и очистка тестовых данных перед/после тестов. Использование API для
      предварительной настройки состояния системы.
    - **Параллельный запуск:** Инструменты `pytest-xdist` для параллельного выполнения тестов. Для UI-тестов важно
      изолировать сессии браузера.

4. **Для AQA:**
    - **Баланс уровней:** Практическое правило: 70% unit-тестов, 20% интеграционных, 10% E2E. Но пропорции зависят от
      проекта.
    - **CI/CD интеграция:** Размещение разных уровней тестов в разных стадиях пайплайна: unit-тесты запускаются на
      каждом коммите, интеграционные — на пулл-реквестах, E2E — на релизных кандидатах.
    - **Флаки-тесты:** UI-тесты часто нестабильны. Необходимы стратегии борьбы: retry механизмы, стабилизация ожиданий (
      explicit waits), изоляция окружения.

# ## **Senior Level**

Глубокий анализ пирамиды тестирования как архитектурного паттерна, его эволюции, ограничений и интеграции с современными
практиками разработки.

1. **Эволюция и критика классической пирамиды:**
    - **"Песочные часы" или "Ромб":** Современные подходы предлагают увеличивать средний уровень (
      интеграционные/сервисные тесты) для микросервисных архитектур. Вместо пирамиды — песочные часы: много unit-тестов,
      много E2E, но акцент на контрактных тестах между сервисами.
    - **Пирамида Майка Кона:** Дополнение пирамиды ручным тестированием (исследовательское, usability) и тестами
      производительности/безопасности.
    - **Критика:** В микросервисной архитектуре unit-тесты часто дают ложное чувство безопасности, так как не проверяют
      взаимодействие сервисов. Акцент смещается на контрактное тестирование (Pact) и тестирование потребителя (
      consumer-driven contracts).

2. **Архитектурные аспекты реализации каждого уровня:**
    - **Unit-тесты и чистая архитектура:** Unit-тесты должны тестировать бизнес-логику в изоляции от инфраструктуры.
      Достигается через Dependency Injection и следование принципам SOLID. Использование `Protocol` для абстракций
      позволяет создавать моки без наследования.
    - **Интеграционные тесты и транзакции:** Для тестов БД важно использовать механизмы отката транзакций. В Django —
      `@pytest.mark.django_db(transaction=True)`. В SQLAlchemy — `session.begin_nested()` для nested transactions. Для
      NoSQL БД — создание отдельной тестовой базы на каждый тестовый прогон.
    - **E2E тесты и идемпотентность:** Каждый E2E тест должен быть идемпотентным — его повторный запуск не должен
      зависеть от предыдущих запусков. Достигается через:
        - Глобальную уникальность тестовых данных (UUID, временные метки).
        - Паттерн Test Data Builder.
        - Автоматическую очистку через хуки (например, `pytest.fixture` с `autouse=True` и `yield`).

3. **Пирамида и CI/CD:**
    - **Стратификация выполнения:** Разделение тестов на "быстрые" и "медленные". Быстрые тесты запускаются на каждом
      коммите, медленные — по расписанию или по мере необходимости. В GitLab CI/CD — `rules: changes`, в GitHub
      Actions — `paths`.
    - **Канареечный деплоймент и тестирование:** E2E-тесты выполняются на канареечном окружении перед выкатом в прод.
      Использование feature flags для управления доступностью функциональности.
    - **Тестирование в продакшене:** Практики progressive delivery: A/B тестирование, мониторинг ошибок, трассировка
      запросов. Тесты в проде — это следующий уровень после пирамиды.

4. **Для AQA (Кровь, кишки и безумие):**
    - **Динамическое определение уровня тестов:** Плагин для pytest, который анализирует зависимости теста (через
      интроспекцию фикстур) и автоматически определяет его уровень: если тест использует моки — unit; если реальную БД —
      интеграционный; если Selenium — E2E. Это позволяет автоматически распределять тесты по разным стадиям CI/CD.
    - **Автоматический баланс пирамиды:** Скрипт, который анализирует историю выполнения тестов (длительность,
      стабильность, покрытие кода) и рекомендует, какие тесты нужно перевести на другой уровень или переписать.
      Использование machine learning для предсказания хрупкости тестов.
    - **Контрактное тестирование для микросервисов:** Внедрение Pact или аналогичных инструментов. Проблема: поддержание
      актуальности контрактов. Решение: автоматическая генерация контрактов из аннотаций типов (TypeScript/OpenAPI для
      фронтенда, `pydantic` для бэкенда) и их валидация в CI.
    - **Тестирование event-driven архитектур:** Для систем на базе Kafka/RabbitMQ unit-тесты бессмысленны. Акцент на
      интеграционное тестирование с тестовым брокером (например, `testcontainers` для запуска Kafka в Docker) и проверка
      корректности обработки событий в различных сценариях.
    - **Перформанс-тестирование как часть пирамиды:** Интеграция нагрузочных тестов (Locust, k6) в CI/CD. Запуск базовых
      нагрузочных тестов на каждую сборку, расширенных — перед релизом. Анализ degradation по метрикам (памяти, CPU,
      latency).
    - **Security-тестирование:** Статический анализ (SAST) — на уровне unit-тестов (каждый коммит). Динамический
      анализ (DAST) — на уровне интеграционных тестов. Penetration testing — на уровне E2E.
    - **Метрики качества тестов:**
        - **Unit-уровень:** Code coverage (branch coverage > 80%), цикломатическая сложность, мутационное тестирование (
          mutmut).
        - **Интеграционный уровень:** Time to failure (как быстро тест обнаруживает регрессию), стабильность (percentage
          of flaky tests).
        - **E2E-уровень:** Business risk coverage (какие бизнес-сценарии покрыты), cost per test (время выполнения *
          стоимость инфраструктуры).
    - **Пирамида для ML-систем:** Особый вызов. Unit-тесты для функций предобработки данных, интеграционные для проверки
      цепочек пайплайнов, E2E для проверки качества модели на тестовых датасетах. Добавляется специфичный уровень —
      тестирование данных (data validation, drift detection).

5. **Антипаттерны и темная сторона:**
    - **Перевернутая пирамида:** Много медленных E2E-тестов, мало unit-тестов. Признак: CI/CD пайплайны выполняются
      часами, разработчики не запускают тесты локально.
    - **Ложные unit-тесты:** Тесты, которые используют реальную БД или сетевые вызовы, но называются unit-тестами.
      Следствие: медленные "unit-тесты", которые падают при отсутствии сети.
    - **Хрупкие интеграционные тесты:** Тесты, которые зависят от специфичного состояния данных. Падают при запуске в
      неправильном порядке или на неподготовленном окружении.
    - **Over-mocking:** Избыточное использование моков, когда тест проверяет не поведение системы, а то, как вызываются
      моки. Тест становится хрупким к рефакторингу.

6. **Будущее пирамиды:**
    - **Shift-left и shift-right:** Тестирование смещается влево (в процесс разработки) и вправо (в прод). Пирамида
      становится объемной фигурой.
    - **AI-assisted testing:** Использование LLM для генерации тестовых случаев, выявления паттернов в падающих тестах,
      предложения оптимизаций.
    - **Serverless и FaaS:** Для бессерверных архитектур классическая пирамида не подходит. Акцент на тестировании
      функций в изоляции (unit) и тестировании оркестрации (интеграционное).

- [Содержание](#содержание)

---

# **Виды тестирования**

## **Junior Level*

Виды тестирования — это различные подходы и методы проверки программного обеспечения, каждый из которых решает
конкретные задачи и имеет свою область применения. Основные виды:

1. **Функциональное тестирование** — проверяет, что система работает в соответствии с требованиями (что она делает).
2. **Нефункциональное тестирование** — проверяет, как система работает (производительность, безопасность, надежность).
3. **Модульное тестирование (Unit)** — тестирование отдельных компонентов кода (функций, классов) в изоляции.
4. **Интеграционное тестирование** — проверка взаимодействия между компонентами, модулями или системами.
5. **Системное тестирование (End-to-End)** — тестирование полного рабочего потока приложения от начала до конца.
6. **Регрессионное тестирование** — проверка, что новые изменения не сломали существующую функциональность.
7. **Дымовое тестирование (Smoke)** — быстрая проверка основных функций системы после сборки.
8. **Приемочное тестирование (Acceptance)** — проверка соответствия системы бизнес-требованиям.

Для QA инженера понимание этих видов помогает выбирать правильные подходы для разных ситуаций: что тестировать
автоматически, а что вручную, как распределять ресурсы и строить стратегию тестирования.

# ## **Middle Level**

С технической точки зрения каждый вид тестирования в Python-экосистеме реализуется через конкретные инструменты и
практики:

1. **Функциональное тестирование:**
    - **API-тестирование:** Использование `requests`, `httpx`, `aiohttp` для HTTP-запросов. Фреймворки: `pytest` с
      плагинами `pytest-httpx`, `pytest-asyncio`.
    - **UI-тестирование:** `Selenium WebDriver`, `Playwright`, `Cypress` через Python-биндинги. Паттерн Page Object для
      структурирования кода.
    - **Тестирование бизнес-логики:** Модульные и интеграционные тесты с использованием моков (`unittest.mock`) и
      стабов.

2. **Нефункциональное тестирование:**
    - **Нагрузочное тестирование:** `locust` (кодовая нагрузка), `k6` (через subprocess), `JMeter` (через
      `jmeter-python`).
    - **Тестирование безопасности:** Статические анализаторы (`bandit`, `safety`), динамические (`OWASP ZAP` API),
      проверка зависимостей (`dependabot`, `renovate`).
    - **Тестирование доступности (a11y):** `axe-core` через `selenium` или `playwright`.

3. **Модульное тестирование (Unit):**
    - **Изоляция:** Использование `unittest.mock.patch`, `MagicMock`, `AsyncMock` для подмены зависимостей.
    - **Параметризация:** `@pytest.mark.parametrize` для тестирования с разными входными данными.
    - **Property-based тестирование:** `hypothesis` для генерации тестовых данных и проверки инвариантов.

4. **Интеграционное тестирование:**
    - **Тестирование с БД:** Использование тестовых БД (SQLite in-memory), транзакций с откатом, фикстур для данных.
    - **Тестирование микросервисов:** `docker-compose` для поднятия зависимостей, `testcontainers` для управления
      контейнерами из кода.
    - **Контрактное тестирование:** `pact-python` для проверки совместимости между потребителем и поставщиком API.

5. **Регрессионное тестирование:**
    - **Тест-сьюты:** Организация тестов по тегам (`@pytest.mark.regression`) для выборочного запуска.
    - **Анализ покрытия:** `pytest-cov` для отслеживания покрытия измененного кода.

6. **Приемочное тестирование:**
    - **BDD-подход:** `behave`, `pytest-bdd` для тестирования на основе пользовательских сценариев (Gherkin).
    - **Автоматизация сценариев:** Комбинация API и UI-тестов для проверки полных пользовательских сценариев.

7. **Тестирование в CI/CD:**
    - **Стратификация тестов:** Разделение на быстрые (unit) и медленные (UI, нагрузочные) с разными триггерами запуска.
    - **Параллельный запуск:** `pytest-xdist` для ускорения выполнения.

# ## **Senior Level**

На этом уровне мы рассматриваем виды тестирования как систему взаимосвязанных практик, интегрированных в процесс
разработки и архитектуру приложения.

1. **Архитектурные аспекты тестирования:**
    - **Тестируемость как свойство архитектуры:** Принципы SOLID, Dependency Injection, использование абстракций (ABC,
      Protocol) для упрощения тестирования. Хорошо спроектированная система допускает легкую изоляцию компонентов для
      модульного тестирования и четкие контракты для интеграционного.
    - **Гексагональная архитектура (Ports & Adapters):** Позволяет тестировать бизнес-логику без инфраструктуры через
      in-memory адаптеры.
    - **CQRS и Event Sourcing:** Требует специализированных подходов к тестированию: проверка корректности обработки
      команд, проекций и событий.

2. **Продвинутые техники тестирования:**
    - **Мутационное тестирование:** `mutmut` для оценки качества unit-тестов. Мутанты (малые изменения в коде) должны
      обнаруживаться тестами.
    - **Фаззинг (Fuzz testing):** `atheris` (на основе libFuzzer) для поиска уязвимостей путем генерации случайных
      входных данных.
    - **Дифференциальное тестирование:** Сравнение поведения двух реализаций (старой и новой) на одном наборе данных для
      обнаружения регрессий.
    - **Тестирование вероятностных систем:** Системы с рандомным поведением или машинным обучением требуют
      статистических методов проверки (доверительные интервалы, p-value).

3. **Тестирование распределенных систем:**
    - **Тестирование в условиях сетевых проблем:** Использование инструментов chaos engineering (`chaostoolkit`,
      `pytest-chaos`) для внесения сбоев (задержки, обрывы соединений).
    - **Проверка идемпотентности и консистентности:** В системах с eventual consistency тесты должны учитывать временные
      задержки и возможные конфликты.
    - **Трассировка запросов:** Интеграция с OpenTelemetry для отслеживания выполнения запроса через несколько сервисов
      и проверки корректности работы цепочек.

4. **Мета-тестирование (тестирование тестов):**
    - **Валидация тестового кода:** Статический анализ тестов с помощью `pylint`, `flake8`, `mypy` для поддержания
      качества.
    - **Тестирование фикстур:** Проверка, что фикстуры корректно создают и очищают ресурсы, не оставляют побочных
      эффектов.
    - **Измерение эффективности тестов:** Метрики: процент обнаружения дефектов, время до обнаружения, стабильность
      тестов (flakiness score).

5. **Для AQA (Кровь, кишки и безумие):**
    - **Динамическая генерация тестов:** Использование метаклассов и декораторов для создания тестов на лету на основе:
        - Конфигурационных файлов (YAML/JSON описывающих тест-кейсы).
        - Моделей данных (`pydantic` схемы для генерации тестовых данных).
        - Контрактов API (OpenAPI/Swagger спецификации для генерации тестов валидации запросов/ответов).
    - **Адаптивное тестирование:** Система, которая анализирует результаты предыдущих запусков и:
        - Повышает приоритет тестов, которые чаще ломались.
        - Динамически формирует регрессионный набор на основе анализа изменений кода (например, через `git diff` и
          анализ зависимостей).
    - **Тестирование времени выполнения (Runtime testing):** Использование декораторов, которые проверяют инварианты во
      время выполнения продакшен-кода (аналогично `assert`, но с сбором статистики и без прерывания работы). Например,
      проверка типов в рантайме с помощью `typeguard`, валидация бизнес-правил.
    - **Интеграция с мониторингом:** Тесты, которые проверяют не только функциональность, но и метрики мониторинга (
      Prometheus, Grafana). Например, после деплоя запускается тест, который создает нагрузку и проверяет, что latency
      не выросла, а ошибок нет.
    - **Тестирование миграций:** Автоматическая проверка миграций БД:
        - Тест на откат (rollback) миграции.
        - Проверка, что миграция не блокирует таблицы на продолжительное время в продакшене (через анализ плана
          выполнения).
        - Генерация тестовых данных, которые покрывают edge cases миграции.
    - **Тестирование resilience:** Автоматизированные сценарии восстановления после сбоев:
        - Симуляция отказов диска, сети, OOM killer.
        - Проверка, что система переключается на резервные сервера, перезапускает упавшие процессы, сохраняет данные.
    - **ML-тестирование:** Для систем с машинным обучением:
        - Тестирование качества моделей на валидационных датасетах.
        - Проверка на смещение (bias) и дрейф данных (data drift).
        - Интеграционные тесты для полного пайплайна ML: от данных до предсказания.

6. **Организационные и процессные аспекты:**
    - **Тестирование в условиях непрерывной поставки:** При частых деплоях (десятки в день) полный регресс невозможен.
      Стратегии:
        - Канареечный деплоймент с автоматическим откатом при падении ключевых тестов.
        - Feature flags для изоляции новой функциональности и её постепенного включения.
    - **Тестирование legacy систем:** Подходы для работы с кодом без тестов:
        - Characterization tests (тесты, которые описывают текущее поведение системы).
        - Golden master testing (сохранение выходных данных системы для будущего сравнения).
    - **Экономика тестирования:** Расчет ROI автоматизации, оптимизация стоимости тестов (инфраструктура, время
      выполнения, поддержка).

7. **Эволюция видов тестирования:**
    - **Shift-left:** Перенос тестирования на ранние этапы (статический анализ, проверка типов, линтинг в pre-commit).
    - **Shift-right:** Тестирование в продакшене (A/B тесты, мониторинг, canary releases).
    - **Тестирование как код (Testing as Code):** Инфраструктура тестирования управляется через код (тестовые сценарии,
      конфигурации, окружения), что позволяет применять практики разработки: ревью, версионирование, модульность.

- [Содержание](#содержание)

---

# **Техники тест дизайна**

## **Junior Level*

Техники проектирования тестов (Test Design Techniques) — это структурированные методы создания тестовых случаев, которые
помогают эффективно и полно проверить систему. Они отвечают на вопрос: "Как придумать хорошие тесты?" Вместо случайного
перебора тестовых данных эти техники предлагают системный подход.

Основные техники:

1. **Эквивалентное разделение (Equivalence Partitioning):** Разделение входных данных на группы (классы
   эквивалентности), в которых система должна вести себя одинаково. Например, для поля "возраст" можно выделить группы:
   отрицательные числа (невалидные), 0-17 (несовершеннолетние), 18-65 (взрослые), больше 65 (пенсионеры). Достаточно
   протестировать по одному значению из каждой группы.

2. **Анализ граничных значений (Boundary Value Analysis):** Тестирование на границах разделов. Ошибки часто возникают на
   краях допустимых диапазонов. Для возраста 18-65 граничные значения: 17, 18, 19 и 64, 65, 66.

3. **Таблица принятия решений (Decision Table Testing):** Используется, когда логика системы зависит от комбинации
   условий. Создается таблица, где перечислены все условия и соответствующие им действия. Каждая строка — это тестовый
   сценарий.

4. **Тестирование состояний и переходов (State Transition Testing):** Применяется для систем, которые имеют конечное
   число состояний и переходов между ними (например, банкомат: ввод карты -> ввод PIN -> выбор операции). Тестируются
   валидные и невалидные переходы.

5. **Тестирование сценариев использования (Use Case Testing):** Тестирование на основе пользовательских сценариев,
   описывающих взаимодействие пользователя с системой для достижения цели.

Для QA инженера владение этими техниками позволяет создавать тесты, которые с большей вероятностью найдут дефекты,
избегая избыточного тестирования.

# ## **Middle Level**

С технической точки зрения, применение этих техник в автоматизированном тестировании на Python имеет свои особенности.

1. **Эквивалентное разделение и анализ граничных значений:**
    - **Параметризация тестов:** В pytest с помощью `@pytest.mark.parametrize` легко реализовать проверку нескольких
      значений из одного класса эквивалентности.
    - **Генерация данных:** Для проверки граничных значений можно использовать `itertools.chain` для объединения списков
      значений или создавать фикстуры, возвращающие наборы данных.
    - **Пример:** Тестирование валидатора возраста:
      ```python
      @pytest.mark.parametrize('age, expected', [
          (-5, False),  # невалидный класс
          (0, False),   # граница
          (10, False),  # внутри класса
          (17, False),  # граница
          (18, True),   # граница
          (30, True),   # внутри класса
          (65, True),   # граница
          (66, False)   # граница
      ])
      def test_age_validation(age, expected):
          assert validate_age(age) == expected
      ```

2. **Таблица принятия решений:**
    - **Реализация через data-driven тесты:** Таблицу можно хранить в CSV, JSON или прямо в коде как список словарей.
      Затем итерироваться по строкам и запускать тест для каждой комбинации.
    - **Использование `pytest` и `pandas`:** Загрузить таблицу из Excel/CSV, преобразовать в список параметров.
    - **Пример:** Тестирование логики скидок, зависящей от статуса клиента и суммы покупки.

3. **Тестирование состояний и переходов:**
    - **Моделирование состояний:** Можно использовать библиотеку `state_machine` или реализовать простой конечный
      автомат своими силами.
    - **Проверка переходов:** Создавать тесты, которые имитируют последовательность событий и проверяют текущее
      состояние системы.
    - **Инструменты:** `pytest` с фикстурами для управления состоянием между шагами.

4. **Тестирование сценариев использования:**
    - **BDD-подход:** Инструменты `behave` или `pytest-bdd` позволяют описывать сценарии на языке, близком к
      естественному (Gherkin), и привязывать их к коду.
    - **Page Object Pattern:** Для UI-тестов Page Object хорошо ложится на сценарии использования, инкапсулируя
      взаимодействие с элементами страницы.

5. **Дополнительные техники:**
    - **Попарное тестирование (Pairwise Testing):** Использование инструментов `allpairspy` или `pict` для генерации
      комбинаций параметров, которые покрывают все пары значений. Это сильно сокращает количество тестов.
    - **Предугадывание ошибок (Error Guessing):** Опытный тестировщик на основе знаний о системе и типичных ошибках
      создает тесты. Автоматизировать сложно, но можно накапливать "шаблоны ошибок" и проверять их в регрессионных
      тестах.

6. **Для AQA:**
    - **Автоматизация техник:** Создание утилит, которые генерируют тестовые данные на основе техник эквивалентного
      разделения и граничных значений.
    - **Интеграция в CI/CD:** Параметризованные тесты могут запускаться на каждый коммит, обеспечивая быстрое получение
      обратной связи.
    - **Отчетность:** При использовании параметризации pytest генерирует отдельные записи для каждого набора данных, что
      упрощает анализ падений.

# ## **Senior Level**

На этом уровне техники проектирования тестов рассматриваются как фундаментальные принципы, интегрированные в процесс
разработки и архитектуру тестовой системы.

1. **Формальные основы и связь с теорией:**
    - **Теория множеств и логика:** Эквивалентное разделение и анализ граничных значений основаны на разбиении множества
      входных данных. Таблицы решений — это представление булевой логики.
    - **Конечные автоматы:** Тестирование переходов между состояниями основано на теории автоматов и может быть
      формально верифицировано с помощью model checking.
    - **Комбинаторика:** Попарное тестирование использует ортогональные массивы и покрывающие массивы (covering arrays)
      для минимизации количества комбинаций.

2. **Автоматическая генерация тестовых случаев:**
    - **Property-based testing:** Библиотека `hypothesis` не просто генерирует случайные данные, а использует техники
      эквивалентного разделения и граничных значений "под капотом". Она строит стратегии генерации данных (strategies),
      которые можно кастомизировать.
    - **Модельное тестирование (Model-based testing):** Создание формальной модели системы (например, на языке Python) и
      автоматическая генерация тестовых последовательностей, покрывающих все состояния и переходы. Инструменты:
      `GraphWalker` (можно интегрировать через API).
    - **Символьное выполнение (Symbolic execution):** Хотя в Python это сложно из-за динамической типизации, существуют
      инструменты, которые пытаются анализировать код и генерировать тесты, покрывающие все пути выполнения (например,
      `pyexz3` на основе Z3).

3. **Динамическое применение техник в зависимости от контекста:**
    - **Адаптивное тестирование:** Система, которая анализирует покрытие кода и результаты предыдущих тестов, чтобы
      решить, какие техники применять дальше. Например, если обнаруживается много ошибок на границах, автоматически
      увеличивается количество тестов на граничные значения.
    - **Машинное обучение для дизайна тестов:** Использование ML для предсказания, какие комбинации параметров с
      наибольшей вероятностью приведут к ошибке, на основе истории дефектов. Обучение модели на исторических данных и
      генерация тестовых случаев, которые "похожи" на те, что находили ошибки ранее.

4. **Интеграция техник в тестовые фреймворки:**
    - **Кастомные маркеры и хуки в pytest:** Создание плагина, который добавляет маркеры для различных техник (например,
      `@pytest.mark.boundary_values`). Хуки могут использоваться для сбора метрик: сколько тестов используют каждую
      технику, насколько они эффективны (количество найденных багов).
    - **Расширение pytest для поддержки таблиц решений:** Плагин, который позволяет загружать таблицы решений из файлов
      и преобразовывать их в параметризованные тесты автоматически.
    - **Генерация тестов из спецификаций:** Интеграция с инструментами спецификации (OpenAPI, Swagger) для
      автоматического создания тестов, использующих техники эквивалентного разделения (валидные/невалидные данные) для
      каждого поля.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Мета-тестирование техник:** Написание тестов, которые проверяют, что техники проектирования тестов применяются
      корректно. Например, тест, который проверяет, что для каждого параметризованного теста есть хотя бы один случай из
      каждого класса эквивалентности. Это можно сделать через анализ AST тестовых функций.
    - **Динамический анализ кода для определения границ:** Инструмент, который анализирует исходный код тестируемого
      приложения (через `ast` или `inspect`), находит условия (if-elif-else, сравнения) и автоматически определяет
      граничные значения для параметров. Затем генерирует тесты для этих границ.
    - **Фаззинг с обратной связью (coverage-guided fuzzing):** Инструменты типа `atheris` (на основе libFuzzer)
      используют информацию о покрытии кода для динамической корректировки входных данных, чтобы достичь новых ветвей
      кода. Это автоматическое применение анализа граничных значений и эквивалентного разделения в реальном времени.
    - **Тестирование вероятностных и недетерминированных систем:** Применение статистических методов для определения,
      достаточно ли тестовых случаев для обнаружения ошибок с заданной вероятностью. Использование техник, таких как
      Monte Carlo тестирование.
    - **Интеграция с формальной верификацией:** Для критичных систем (авионика, медицинское ПО) техники дизайна тестов
      могут комбинироваться с формальными методами. Например, использование контрактов (assertions, pre/post-conditions)
      и автоматической генерации тестов из формальных спецификаций с помощью инструментов, подобных `Daikon` (для Python
      можно использовать `icontract`).

6. **Проблемы и ограничения:**
    - **Проклятие размерности:** При большом количестве параметров даже попарное тестирование может генерировать много
      комбинаций. Необходимо использовать эвристики для приоритизации.
    - **Динамическая природа Python:** Статический анализ кода для определения границ сложен из-за динамической
      типизации и метапрограммирования. Часто приходится полагаться на аннотации типов и документацию.
    - **Сложность тестирования состояний:** В системах с большим количеством состояний и переходов полное покрытие может
      быть недостижимо. Приходится выбирать наиболее важные пути (например, на основе использования в продакшене).

7. **Будущее техник проектирования тестов:**
    - **AI-assisted test design:** Использование LLM (например, GPT) для предложения тестовых случаев на основе описания
      функциональности. Обучение моделей на больших наборах кода и тестов.
    - **Тестирование на основе моделей (MBT) в CI/CD:** Интеграция MBT в конвейер непрерывной интеграции, где модель
      системы постоянно обновляется и тесты генерируются автоматически при каждом изменении.
    - **Самоадаптирующиеся тестовые системы:** Фреймворки, которые обучаются на ходу, корректируя применяемые техники в
      зависимости от обнаруженных дефектов и изменений в системе.

- [Содержание](#содержание)

---

# **Метрики тестирования**

## **Junior Level*

Метрики тестирования — это количественные показатели, которые помогают измерить и оценить различные аспекты процесса
тестирования и качества продукта. Они отвечают на вопросы: "Насколько хорошо мы тестируем?", "Каково качество нашего
кода?", "Эффективны ли наши тесты?".

Основные метрики:

- **Покрытие кода (Code Coverage):** Какой процент кода выполняется во время тестов. Измеряется в процентах по строкам,
  ветвям, функциям.
- **Количество дефектов:** Сколько багов найдено, сколько исправлено, скорость их закрытия.
- **Время выполнения тестов:** Как долго работает тестовый набор.
- **Стабильность тестов (Flakiness):** Как часто тесты падают не из-за багов в коде, а по случайным причинам (например,
  проблемы с сетью).
- **Стоимость дефекта:** Сколько стоит найти и исправить баг на разных этапах (чем раньше, тем дешевле).

Метрики помогают принимать обоснованные решения: куда направить усилия по тестированию, когда можно выпускать релиз,
какие тесты нужно улучшить.

# ## **Middle Level**

С технической точки зрения метрики в Python-экосистеме тестирования собираются и анализируются с помощью конкретных
инструментов и практик.

1. **Метрики покрытия кода:**
    - **Инструменты:** `coverage.py` — стандартный инструмент для измерения покрытия. Интегрируется с pytest через
      `pytest-cov`.
    - **Типы покрытия:**
        - **Line coverage:** Процент выполненных строк.
        - **Branch coverage:** Процент пройденных ветвей в условиях (if/else).
        - **Function coverage:** Процент вызванных функций.
        - **Condition coverage:** Процент комбинаций условий в сложных булевых выражениях.
    - **Интеграция в CI/CD:** Генерация отчетов в формате XML/HTML, интеграция с сервисами (Codecov, Coveralls).

2. **Метрики качества тестов:**
    - **Mutation score (Мутационное тестирование):** `mutmut` внедряет мелкие изменения (мутации) в код и проверяет,
      обнаружат ли их тесты. Процент убитых мутаций — показатель эффективности тестов.
    - **Стабильность тестов (Flakiness):** Анализ истории запусков тестов. Если тест иногда проходит, иногда падает при
      тех же условиях — он нестабилен. Инструменты: `pytest-flakefinder`, кастомные скрипты анализа Jenkins/Allure
      отчетов.
    - **Время выполнения:** `pytest` с флагом `--durations` показывает самые медленные тесты. `pytest-xdist` для
      параллельного запуска, но нужно учитывать накладные расходы.

3. **Метрики дефектов:**
    - **Плотность дефектов (Defect Density):** Количество багов на тысячу строк кода (KLOC).
    - **Эффективность тестирования (Test Effectiveness):** Процент дефектов, найденных тестами, от общего числа
      дефектов (включая найденные пользователями).
    - **Время жизни дефекта (Defect Age):** Среднее время от создания бага до его закрытия.

4. **Метрики процесса:**
    - **Скорость выполнения тестов:** Сколько тестов выполняется в минуту/час.
    - **Автоматизация:** Процент автоматизированных тестов от общего числа.
    - **Стоимость:** Затраты на инфраструктуру тестирования (вычислительные ресурсы, лицензии инструментов).

5. **Инструменты для сбора метрик:**
    - **Allure TestOps / ReportPortal:** Системы для хранения результатов тестов, анализа метрик.
    - **Prometheus + Grafana:** Для мониторинга производительности тестовой инфраструктуры и самого приложения во время
      тестов.
    - **Кастомные скрипты на Python:** Анализ логов, парсинг отчетов, вычисление метрик.

# ## **Senior Level**

На этом уровне метрики рассматриваются как часть системы измерения качества (Quality Measurement System), которая
интегрирована в процесс разработки, архитектуру и бизнес-цели.

1. **Архитектурные метрики и тестируемость:**
    - **Связность (Cohesion) и связанность (Coupling):** Можно измерять через статические анализаторы (`radon`,
      `mccabe`). Высокая связность и низкая связанность упрощают тестирование.
    - **Цикломатическая сложность:** Количество независимых путей в коде. Высокая сложность (>10) указывает на код,
      трудный для тестирования. Инструменты: `mccabe`, `pylint`.
    - **Индекс поддерживаемости (Maintainability Index):** Комбинированная метрика, учитывающая сложность, объем кода,
      комментарии. Позволяет прогнозировать, насколько сложно будет изменять и тестировать код.

2. **Продвинутые метрики покрытия:**
    - **Semantic Coverage:** Покрытие не только синтаксических конструкций, но и семантики (например, все возможные
      состояния объекта). Частично достигается property-based тестированием (`hypothesis`).
    - **Data Coverage:** Покрытие различных комбинаций входных данных. Использование техник попарного тестирования (
      `allpairspy`) для оценки полноты тестовых данных.
    - **Temporal Coverage:** Для асинхронных и параллельных систем — покрытие различных порядков выполнения и состояний
      гонки. Инструменты: `pytest-asyncio` с детерминированным планировщиком.

3. **Мета-метрики (метрики метрик):**
    - **Полезность метрики:** Оценивается через корреляцию с бизнес-показателями (например, влияет ли покрытие кода на
      количество инцидентов в проде).
    - **Стоимость сбора:** Сколько ресурсов (время, вычисления) уходит на сбор и анализ метрики.
    - **Стабильность метрики:** Насколько метрика подвержена случайным колебаниям. Для важных метрик нужно вычислять
      доверительные интервалы.

4. **Прогнозные метрики и ML:**
    - **Предсказание дефектов:** Модели машинного обучения, которые на основе исторических данных (изменения кода,
      сложность, покрытие тестами) предсказывают, где могут появиться баги. Использование `scikit-learn` для построения
      моделей.
    - **Рекомендательные системы для тестов:** Анализ истории падений тестов и изменений кода для предложения, какие
      тесты нужно запустить при конкретном изменении (test impact analysis).
    - **Аномалии в метриках:** Автоматическое обнаружение аномалий в метриках (например, внезапное падение покрытия,
      рост времени выполнения тестов). Инструменты: `Prophet` от Facebook для временных рядов.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Динамическое вычисление метрик во время выполнения тестов:** Плагин для pytest, который с помощью хуков (
      `pytest_runtest_protocol`) собирает метрики в реальном времени: использование памяти, CPU, количество исключений,
      даже покрытие кода на лету (через `sys.settrace` или инструментирование байткода).
    - **Метрики для распределенных тестов:** При использовании `pytest-xdist` или запуске тестов в Kubernetes:
        - **Распределение тестов по нодам:** Равномерность загрузки.
        - **Сетевые задержки:** Время обмена данными между master и worker.
        - **Стоимость инфраструктуры:** Затраты на облачные инстансы во время тестового прогона.
    - **Метрики тестовых данных:**
        - **Разнообразие данных:** Энтропия тестовых данных (сколько различных сценариев покрывают).
        - **Реалистичность:** Насколько тестовые данные близки к продакшен-данным (можно измерять через статистические
          тесты — распределения, корреляции).
    - **Интеграция с бизнес-метриками:**
        - **Влияние тестирования на бизнес-показатели:** Корреляция между метриками тестирования (покрытие, количество
          найденных багов) и бизнес-метриками (коэффициент оттока пользователей, средний чек).
        - **ROI автоматизации:** Расчет возврата инвестиций: (экономия на ручном тестировании - затраты на разработку и
          поддержку автотестов) / затраты.
    - **Метрики для тестирования AI/ML систем:**
        - **Coverage для моделей:** Покрытие пространства признаков (feature space) тестовыми данными.
        - **Смещение (Bias) тестовых данных:** Насколько тестовые данные репрезентативны для всех групп пользователей.
        - **Дрейф данных (Data Drift):** Изменение распределения входных данных со временем и его влияние на качество
          модели.

6. **Визуализация и дашборды:**
    - **Grafana дашборды:** Отображение метрик в реальном времени: покрытие кода, скорость тестов, стабильность.
    - **Интерактивные отчеты:** `plotly` или `bokeh` для создания интерактивных графиков, позволяющих "копать" в данные.
    - **Heatmaps:** Визуализация покрытия кода на карте исходного кода — какие файлы/функции плохо покрыты.

7. **Ограничения и опасности метрик:**
    - **Охватка метрик (Goodhart's law):** "Когда метрика становится целью, она перестает быть хорошей метрикой".
      Пример: разработчики начинают писать бессмысленные тесты только чтобы повысить покрытие.
    - **Ложные корреляции:** Две метрики могут коррелировать, но не иметь причинно-следственной связи.
    - **Контекстная зависимость:** Одинаковые значения метрик могут означать разное в разных проектах (например, 80%
      покрытия для CRUD API и для ядра СУБД).
    - **Перегруженность метриками:** Слишком много метрик приводит к "аналитическому параличу". Нужно выбирать ключевые
      индикаторы (KPIs).

8. **Будущее метрик:**
    - **AI-ассистированный анализ:** LLM для генерации инсайтов из метрик, автоматического написания пояснений к
      изменениям.
    - **Прогнозное тестирование:** Системы, которые на основе метрик предсказывают, какие области кода станут
      проблемными в будущем, и предлагают превентивные тесты.
    - **Интеграция с DORA-метриками:** Связь метрик тестирования с DevOps Research and Assessment метриками (частота
      деплоев, время восстановления и т.д.).

- [Содержание](#содержание)

---

# **Техники тест-дизайна**

## **Junior Level**

Техники тест-дизайна — это систематические методы создания тестовых случаев, которые помогают эффективно проверять
программное обеспечение, находя максимальное количество дефектов при минимальном количестве тестов. Они основаны на
логике, математике и понимании работы системы.

Основные техники:

1. **Эквивалентное разбиение (Equivalence Partitioning):** Разделение входных данных на группы, которые система должна
   обрабатывать одинаково. Достаточно протестировать по одному значению из каждой группы.
2. **Анализ граничных значений (Boundary Value Analysis):** Тестирование значений на границах этих групп, где чаще всего
   возникают ошибки.
3. **Причина-Следствие (Cause-Effect Graphing):** Создание графа, связывающего входные условия (причины) с выходными
   действиями (следствиями), затем преобразование в таблицу решений.
4. **Таблица решений (Decision Table):** Систематическое представление комбинаций условий и соответствующих действий.
5. **Диаграмма переходов состояний (State Transition):** Тестирование систем, которые могут находиться в разных
   состояниях и переходить между ними.
6. **Попарное тестирование (Pairwise Testing):** Проверка всех возможных пар значений параметров вместо полного перебора
   комбинаций.

Для QA инженера владение этими техниками позволяет создавать осмысленные, полные и эффективные тесты вместо случайного
подбора проверок.

## **Middle Level**

С технической точки зрения применение этих техник в автоматизированном тестировании на Python требует специфических
подходов и инструментов:

1. **Эквивалентное разбиение и граничные значения:**
    - **Реализация в pytest:** Параметризация через `@pytest.mark.parametrize` с явным перечислением классов
      эквивалентности и граничных значений.
    - **Генерация данных:** Использование `itertools` или кастомных генераторов для создания тестовых данных.
    - **Пример:** Для функции, принимающей возраст от 18 до 65, тестируем значения: 17 (ниже границы), 18 (нижняя
      граница), 30 (внутри), 65 (верхняя граница), 66 (выше границы).

2. **Таблицы решений:**
    - **Data-driven подход:** Хранение таблиц в CSV, JSON, YAML или Excel. Использование `pytest` с фикстурой, которая
      загружает таблицу и преобразует в параметры.
    - **Библиотеки:** `pandas` для работы со сложными таблицами.
    - **Пример:** Таблица с условиями "статус пользователя" и "сумма заказа" и соответствующими скидками.

3. **Диаграмма переходов состояний:**
    - **Моделирование состояний:** Использование конечных автоматов (`transitions`, `automaton` библиотеки) или
      собственной реализации.
    - **Тестирование последовательностей:** Создание тестов, которые проверяют валидные и невалидные переходы между
      состояниями.
    - **Пример:** Тестирование жизненного цикла заказа: создан -> оплачен -> отправлен -> доставлен.

4. **Попарное тестирование:**
    - **Инструменты:** `allpairspy`, `pairwise` для генерации комбинаций.
    - **Интеграция с pytest:** Генерация параметров для тестов на лету.
    - **Пример:** Система с 10 параметрами, каждый из которых может принимать 10 значений. Полный перебор — 10^10
      комбинаций, попарное тестирование — порядка 100-200 комбинаций.

5. **Причина-Следствие:**
    - **Преобразование в таблицу решений:** После построения графа преобразование в таблицу и реализация как data-driven
      тесты.
    - **Инструменты:** Часто делается вручную или с помощью специализированных инструментов.

6. **Дополнительные техники:**
    - **Use Case Testing:** Реализация через BDD (`behave`, `pytest-bdd`).
    - **Error Guessing:** Накопление шаблонов ошибок и создание тестов на их основе.
    - **Exploratory Testing:** Хотя это ручная техника, можно автоматизировать некоторые аспекты через скрипты, которые
      выполняют случайные действия.

## **Senior Level**

На этом уровне техники тест-дизайна рассматриваются как формальные методы, интегрированные в процесс разработки, с
применением метапрограммирования, статического анализа и машинного обучения.

1. **Формальные методы и автоматическая дедукция:**
    - **Symbolic Execution:** Хотя в Python сложно реализовать полное символьное выполнение, инструменты вроде `pyexz3`
      пытаются анализировать пути выполнения и генерировать тестовые данные для покрытия всех ветвей.
    - **Теоретико-множественная основа:** Эквивалентное разбиение — это разбиение множества входных данных на классы
      эквивалентности (отношение эквивалентности). Граничные значения — это элементы на границах этих классов.
    - **Логическая полнота:** Таблицы решений обеспечивают проверку всех комбинаций условий, что соответствует полному
      покрытию по условиям (condition coverage).

2. **Автоматическая генерация тестов на основе техник:**
    - **Статический анализ кода для определения границ:** Инструмент, который анализирует AST кода, находит сравнения (
      `<`, `>`, `<=`, `>=`, `==`) и автоматически определяет граничные значения для параметров.
    - **Property-based тестирование как обобщение:** Библиотека `hypothesis` использует техники эквивалентного разбиения
      и граничных значений внутри стратегий генерации данных. Можно создавать кастомные стратегии, отражающие
      специфичные для домена разбиения.
    - **Модель-ориентированное тестирование (MBT):** Создание формальной модели системы (например, на языке `pydantic`
      или с помощью специализированных инструментов) и автоматическая генерация тестовых последовательностей,
      покрывающих все состояния и переходы.

3. **Динамическое применение техник в зависимости от контекста:**
    - **Адаптивный тест-дизайн:** Система, которая анализирует результаты предыдущих тестов (покрытие, найденные
      дефекты) и выбирает, какие техники применять дальше. Например, если в модуле много граничных ошибок — увеличивает
      количество тестов на граничные значения.
    - **Machine Learning для оптимизации комбинаций:** Использование ML для предсказания, какие комбинации параметров в
      попарном тестировании наиболее критичны, на основе исторических данных о дефектах.
    - **Генерация тестов на основе мутаций:** Инструменты мутационного тестирования (`mutmut`) могут быть расширены для
      генерации тестов, убивающих конкретные мутации, что косвенно применяет технику "предугадывания ошибок".

4. **Интеграция в CI/CD и DevOps:**
    - **Динамическое определение тестового набора:** На основе изменений кода (`git diff`) автоматическое определение,
      какие техники тест-дизайна применить к измененным модулям. Например, если изменено условие — генерировать тесты на
      граничные значения.
    - **Приоритизация тестов на основе риска:** Комбинирование техник тест-дизайна с анализом рисков. Более рискованные
      модули тестируются с использованием большего количества техник и более тщательно.
    - **Тестирование в продакшене (Production):** Применение техник тест-дизайна к данным из продакшена (
      анонимизированным) для создания более репрезентативных тестовых сценариев.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Мета-тестирование техник тест-дизайна:** Написание тестов, которые проверяют, что техники тест-дизайна
      применяются корректно. Например, тест, который анализирует параметризованные тесты и проверяет, что в них
      присутствуют значения из всех классов эквивалентности. Это можно сделать через анализ AST и декораторов pytest.
    - **Автоматическое построение диаграмм переходов состояний:** Инструмент, который путем статического анализа кода (
      поиск паттернов состояний) или динамического анализа (трассировка выполнения) строит диаграмму переходов, а затем
      генерирует тесты для покрытия всех переходов.
    - **Комбинаторные взрывы и их обход:** Для систем с огромным количеством параметров (например, конфигурация облачной
      инфраструктуры) использование продвинутых комбинаторных техник: ортогональные массивы, покрывающие массивы (
      covering arrays), t-wise тестирование (где t > 2). Интеграция с инструментами вроде `ACTS` (NIST).
    - **Тест-дизайн для вероятностных систем:** Для систем с недетерминированным поведением (например, рекомендательные
      системы, AI) использование статистических техник: проверка распределений выходных данных, доверительные интервалы,
      A/B тестирование в автоматическом режиме.
    - **Фаззинг с интеллектуальной генерацией данных:** Инструменты вроде `atheris` (на основе libFuzzer) могут быть
      дополнены знаниями о домене: подсказки о граничных значениях и классах эквивалентности для более эффективного
      поиска краевых случаев.
    - **Тест-дизайн для безопасности:** Применение техник тест-дизайна для поиска уязвимостей. Например, анализ
      граничных значений для буферов (переполнение), таблицы решений для контроля доступа.
    - **Визуализация и анализ покрытия техник:** Дашборды, которые показывают, какие техники тест-дизайна применялись к
      каждому модулю, и как это коррелирует с количеством найденных дефектов. Использование графовых баз данных для
      хранения отношений между тестами, техниками и дефектами.

6. **Ограничения и эволюция:**
    - **Контекстная зависимость:** Эффективность техник зависит от типа системы (например, для ML-моделей техники должны
      быть другими).
    - **Human-in-the-loop:** Некоторые техники (как Error Guessing) требуют экспертизы, которую сложно формализовать.
    - **Эволюция с развитием AI:** Возможность использования LLM для предложения тестовых случаев на основе
      естественного описания функциональности.

- [Содержание](#содержание)

---

# **Автоматизация тестирования**

## **Junior Level*

Автоматизация тестирования — это процесс использования специальных инструментов и скриптов для выполнения тестов,
проверки результатов и сравнения фактического поведения системы с ожидаемым без непосредственного участия человека.
Вместо того чтобы вручную кликать по интерфейсу или проверять API, мы пишем код, который делает это за нас.

Основные преимущества:

- **Скорость:** Автоматические тесты выполняются гораздо быстрее ручных.
- **Повторяемость:** Тесты можно запускать сколько угодно раз с одинаковой точностью.
- **Раннее обнаружение ошибок:** Автотесты можно запускать при каждом изменении кода, быстро находя регрессии.
- **Освобождение времени тестировщиков:** Позволяет сосредоточиться на сложных, исследовательских и нефункциональных
  тестах.

Для QA инженера автоматизация — это ключевой навык, который позволяет масштабировать тестирование, интегрировать его в
процесс разработки (CI/CD) и повышать общее качество продукта.

# ## **Middle Level**

С технической точки зрения автоматизация тестирования в Python-экосистеме охватывает несколько уровней и требует знания
конкретных инструментов и подходов.

1. **Уровни автоматизации (по пирамиде тестирования):**
    - **Unit-тесты:** Автоматизация с помощью `pytest`, `unittest`. Использование моков (`unittest.mock`) для изоляции.
    - **Интеграционные тесты:** Автоматизация проверки API (`requests`, `httpx`), взаимодействия с БД (транзакции,
      фикстуры), внешними сервисами.
    - **UI-тесты:** Автоматизация веб-интерфейсов через `Selenium WebDriver`, `Playwright`, `Cypress`. Использование
      паттерна Page Object.
    - **Нагрузочные тесты:** Автоматизация с помощью `locust`, `k6`, `JMeter`.

2. **Ключевые принципы автоматизации:**
    - **Поддерживаемость:** Код автотестов должен быть чистым, хорошо организованным и легко изменяемым.
    - **Стабильность:** Тесты должны быть надежными, не должны давать ложные срабатывания (flaky tests).
    - **Изоляция:** Тесты не должны зависеть друг от друга и от внешнего состояния.
    - **Информативность:** При падении тест должен давать четкое сообщение о том, что пошло не так.

3. **Инструменты и фреймворки:**
    - **Основной фреймворк:** `pytest` — де-факто стандарт для написания тестов в Python благодаря простоте, фикстурам и
      плагинам.
    - **Управление зависимостями:** `pip`, `poetry`, `pipenv`.
    - **CI/CD интеграция:** `Jenkins`, `GitLab CI`, `GitHub Actions`, `CircleCI` для автоматического запуска тестов.
    - **Управление тестовыми данными:** `factory_boy`, `Faker` для генерации данных.
    - **Отчетность:** `Allure`, `pytest-html`, `ReportPortal`.

4. **Паттерны автоматизации:**
    - **Page Object:** Для UI-тестов, инкапсулирует работу с элементами страницы.
    - **Screenplay:** Более современная альтернатива Page Object, фокусируется на взаимодействии пользователя с
      системой.
    - **Data-Driven Testing:** Отделение тестовых данных от логики теста (например, хранение данных в JSON, CSV).
    - **Keyword-Driven Testing:** Использование ключевых слов для описания тестовых шагов (часто используется в Robot
      Framework).

5. **Для AQA:**
    - **Выбор правильного уровня автоматизации:** Не все нужно автоматизировать. Критерии: частота выполнения, важность,
      стабильность функционала, сложность ручного тестирования.
    - **Баланс автоматизации:** Соотношение времени на написание автотестов и время на ручное тестирование. Обычно
      20-30% времени на автоматизацию, остальное — на исследовательское тестирование и анализ.
    - **Интеграция в процесс разработки:** Автотесты должны запускаться при каждом коммите (unit-тесты) и
      пулл-реквесте (интеграционные), а также ночью (полный регресс).

# ## **Senior Level**

На этом уровне автоматизация рассматривается как стратегическая дисциплина, интегрированная в архитектуру, процессы и
культуру разработки.

1. **Архитектура автоматизированной тестовой системы:**
    - **Многослойная архитектура:** Разделение на уровни: слой тестовых сценариев (тест-кейсы), слой бизнес-логики (Page
      Objects, API клиенты), слой утилит (хелперы, генераторы данных), слой драйверов (Selenium, requests).
    - **Принципы проектирования:** Применение SOLID, DRY, KISS к коду автотестов. Использование паттернов
      проектирования (Фабрика, Стратегия, Команда) для повышения гибкости и поддерживаемости.
    - **Микросервисная архитектура для тестов:** В больших распределенных системах сама тестовая система может быть
      построена как набор микросервисов: оркестратор тестов, генератор данных, сборщик отчетов, система нотификаций.

2. **Динамическая и адаптивная автоматизация:**
    - **Генерация тестов на лету:** Использование метапрограммирования (`type`, декораторы, метаклассы) для создания
      тестов в runtime на основе конфигурации или данных.
    - **Самоадаптирующиеся тесты:** Тесты, которые анализируют окружение и подстраиваются под него (например,
      определяют, какая версия API доступна, и выбирают соответствующую стратегию).
    - **Интеллектуальные тестовые данные:** Система, которая анализирует продакшен-данные (с соблюдением анонимности) и
      генерирует репрезентативные тестовые данные, сохраняя распределения и корреляции.

3. **Автоматизация в контексте DevOps и SRE:**
    - **Тестирование как часть инфраструктуры:** Инфраструктурные тесты (например, с помощью `terraform test` или
      `inspec`) для проверки конфигурации окружений.
    - **Тестирование надежности (Reliability Testing):** Автоматизация тестов на отказоустойчивость (chaos engineering)
      с помощью инструментов `chaostoolkit`, `Litmus`.
    - **Тестирование в продакшене:** Автоматизация canary-релизов, A/B тестов, проверки метрик мониторинга (Prometheus,
      Grafana) после деплоя.

4. **AI и ML в автоматизации тестирования:**
    - **Генерация тестовых сценариев:** Использование LLM (например, GPT) для создания тест-кейсов на основе
      пользовательских историй или документации.
    - **Визуальная валидация:** Компьютерное зрение (`opencv`, `pytesseract`) для сравнения скриншотов, поиска элементов
      на основе изображений.
    - **Предсказание падений тестов:** ML-модели, которые на основе истории выполнения предсказывают, какие тесты
      вероятнее упадут при данном изменении кода.
    - **Автоматический анализ root cause:** Система, которая анализирует падение теста, логи и изменения кода, и
      предлагает вероятную причину.

5. **Для AQA (Кровь, кишки и безумие):**
    - **Мета-автоматизация:** Автоматизация процесса создания автотестов. Например, инструмент, который по OpenAPI
      спецификации генерирует базовые API-тесты, или по HTML-странице — скелет Page Object.
    - **Распределенное выполнение тестов в облаке:** Система, которая динамически запускает тесты в облачных средах (AWS
      Lambda, Google Cloud Functions) для экономии ресурсов и ускорения выполнения. Использование Kubernetes для
      оркестрации тысяч тестовых подов.
    - **Тестирование времени выполнения (Runtime Verification):** Внедрение агентов в продакшен-код, которые в реальном
      времени проверяют инварианты, контракты и бизнес-правила, отправляя алерты при нарушениях. Это форма "
      перманентного" автоматизированного тестирования.
    - **Эмуляция пользовательского поведения:** Создание "виртуальных пользователей", которые не просто следуют жестким
      сценариям, а используют reinforcement learning для изучения интерфейса и нахождения путей к ошибкам.
    - **Автоматизация исследования (Exploratory Testing Automation):** Инструменты, которые комбинируют случайные
      действия с эвристиками для исследования приложения и обнаружения неочевидных дефектов. Например, комбинация
      фаззинга для UI.
    - **Тестирование безопасности (Security Testing Automation):** Интеграция статических (SAST) и динамических (DAST)
      анализаторов в пайплайн, автоматическое сканирование зависимостей на уязвимости, автоматическое тестирование на
      инъекции и другие атаки.
    - **Перформанс-тестирование как код:** Описание нагрузочных тестов в виде кода (например, на Python с `locust`),
      который версионируется, ревьюится и запускается автоматически. Автоматический анализ результатов и сравнение с
      baseline.

6. **Экономика и ROI автоматизации:**
    - **Расчет ROI:** Формулы для оценки возврата инвестиций в автоматизацию. Учет факторов: время на разработку и
      поддержку, сокращение времени ручного тестирования, уменьшение стоимости дефектов, найденных позже.
    - **Оптимизация стоимости выполнения:** Использование spot-инстансов в облаке, отключение тестовых окружений когда
      они не используются, кэширование зависимостей.
    - **Приоритизация автоматизации:** Матрица принятия решений: что автоматизировать в первую очередь на основе частоты
      изменений, бизнес-критичности, сложности ручного тестирования.

7. **Культурные и организационные аспекты:**
    - **Shift-Left и Shift-Right:** Автоматизация тестирования на всех этапах: от статического анализа кода до
      мониторинга в проде.
    - **Collaboration между разработчиками и QA:** Разработчики пишут unit-тесты, QA фокусируется на интеграционных и
      E2E. Использование pull request ревью для кода автотестов.
    - **Обучение и менторинг:** Создание внутренних библиотек, шаблонов, проведение воркшопов для распространения лучших
      практик автоматизации.

8. **Будущее автоматизации:**
    - **Автономные тестовые системы:** Системы, которые сами определяют, что тестировать, генерируют тесты, выполняют
      их, анализируют результаты и вносят изменения в код (самоисцеляющиеся тесты).
    - **Тестирование в метавселенных и AR/VR:** Новые инструменты для автоматизации тестирования 3D-интерфейсов и
      иммерсивных сред.
    - **Квантовое тестирование:** С появлением квантовых компьютеров потребуются новые подходы к автоматизации
      тестирования квантовых алгоритмов и программ.

- [Содержание](#содержание)

---

Лягушка
