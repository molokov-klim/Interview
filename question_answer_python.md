# Собеседование Python AQA

# Содержание

## Базовые знания

- [Типы данных](#типы-данных)
- [*args и **kwargs](#args-и-kwargs)
- [Хеш-таблица](#хеш-таблица)
- [Встроенные функции](#встроенные-функции)
- [Контекстные менеджеры (with)](#контекстные-менеджеры)
- [Генераторы и итераторы](#генераторы-и-итераторы)
- [Декораторы и замыкания](#декораторы-и-замыкания)
- [GIL (Global Interpreter Lock)](#gil-global-interpreter-lock)
- [Изменение списка во время итерации](#изменение-списка-во-время-итерации)
- [Области видимости](#области-видимости)
- [Lambda-функции](#lambda-функции)
- [Comprehensions и генераторные выражения](#comprehensions-и-генераторные-выражения)
- [copy() и deepcopy()](#copy-и-deepcopy)
- [Асинхронность](#асинхронность)
- [Многопоточность](#многопоточность)
- [Мультипроцессинг](#мультипроцессинг)
- [Dataclass](#dataclass)
- [Enum](#enum)
- [Garbage Collector (сборщик мусора)](#garbage-collector-сборщик-мусора)
- [Сложность кода](#сложность-кода)

## РАЗДЕЛ ООП

- [ООП](#ооп)
- [Абстракция (ООП)](#абстракция)
- [Инкапсуляция (ООП)](#инкапсуляция)
- [Наследование (ООП)](#наследование)
- [Полиморфизм (ООП)](#полиморфизм)
- [Diamond Problem](#diamond-problem)
- [Магические методы](#магические-методы)
- [Инвариантность и ковариантность](#инвариантность-и-ковариантность)
- [Декораторы классов и методов](#декораторы-классов-и-методов)
- [Множественное наследование и MRO](#множественное-наследование-и-mro)
- [ABC (Abstract Base Classes)](#abc)
- [Протокол (Protocol)](#протокол-protocol)
- [Паттерны проектирования](#паттерны-проектирования)
- [Композиция и агрегация](#композиция-и-агрегация)
- [Связность и связанность](#связность-и-связанность)
- [SOLID](#solid)
- [Специфика ООП в Python](#специфика-ооп-в-python)
- [Наследование и композиция](#наследование-и-композиция)
- [Метапрограммирование](#метапрограммирование)
- [Миксины](#миксины)

## Типизация

1. [typing: Optional, Union, TypeVar, Generic](#typing)
2. [Literal, TypedDict, Protocol](#literal-typeddict-protocol)
3. [Ковариантность, контравариантность](#ковариантность-контравариантность)

## Инструменты

1. [pytest](#pytest)
2. [pytest hooks](#pytest-hooks)
3. [Kubernetes](#kubernetes)

## Теория тестирования

1. [Пирамида тестирования](#пирамида-тестирования)
2. [Виды тестирования](#виды-тестирования)
3. [Метрики тестирования](#метрики-тестирования)
4. [Техники тест дизайна](#техники-тест-дизайна)
5. [Автоматизация](#автоматизация)

- [Содержание](#содержание)

---

# **Типы данных**

## **Junior Level**

Типы данных в Python делятся на **изменяемые** (mutable) и **неизменяемые** (immutable).

**Простые типы:**

- Числа: `int`, `float`, `complex`
- Строки: `str`
- Логические значения: `bool`
- Специальный тип: `NoneType` (единственное значение `None`)

**Коллекционные типы:**

- `list` — изменяемая упорядоченная коллекция
- `tuple` — неизменяемая упорядоченная коллекция
- `dict` — изменяемая коллекция пар «ключ-значение» (упорядоченная с Python 3.7)
- `set` / `frozenset` — изменяемое и неизменяемое множества уникальных элементов
- `bytes` / `bytearray` — неизменяемая и изменяемая последовательности байтов

## **Middle Level**

1. **Ключевые различия мутабельности:**
    - **Изменяемые:** `list`, `dict`, `set`, `bytearray`, пользовательские классы. Можно модифицировать после создания.
    - **Неизменяемые:** `int`, `float`, `str`, `bytes`, `tuple`, `frozenset`, `bool`, `NoneType`. Любая операция создаёт
      новый объект.

2. **Практические следствия:**
    - Передача изменяемых объектов в функции позволяет модифицировать оригинал
    - Только неизменяемые объекты могут быть ключами словаря (требуется хэшируемость)
    - Мутабельность влияет на потокобезопасность и кэширование

3. **Специфика типов:**
    - `None` — синглтон, обозначающий отсутствие значения
    - `bool` — подкласс `int`, значения `True` и `False` — синглтоны
    - `tuple` — неизменяем, но может содержать изменяемые элементы
    - `set`/`frozenset` — хранят только хэшируемые элементы, реализация аналогична словарям без значений

## **Senior Level**

В CPython все данные — это объекты с единым базовым заголовком `PyObject`, а конкретные типы (int, list, dict и т.д.)
реализованы как структуры C, начинающиеся с этого заголовка или его расширения `PyVarObject` для переменного размера.
Типы сами являются объектами (`type` — метакласс), и поведение каждого типа задаётся через таблицу слотов
`PyTypeObject`.

## Базовая объектная модель

- Любой объект в CPython представляется как `PyObject`:
    - поля: указатель на тип (`ob_type`) и счётчик ссылок (`ob_refcnt`).
- Для объектов переменного размера (строки, списки, кортежи и т.п.) используется `PyVarObject`, который добавляет поле
  `ob_size` — логический размер (например, количество элементов).

Следствие: любой указатель на объект можно привести к `PyObject*`/`PyVarObject*`, а дальше через `ob_type` понять, как
его интерпретировать.

## Тип как объект: PyTypeObject

- Каждый тип (включая встроенные, пользовательские классы и `type` itself) представлен структурой `PyTypeObject`.
- В `PyTypeObject` есть:
    - метаданные (имя, размер базовой части, размер элемента для var‑объектов, флаги);
    - набор «слотов» — указателей на функции, реализующие операции: арифметика, сравнение, итерация, индексирование,
      доступ к атрибутам и т.д. (`tp_as_number`, `tp_as_sequence`, `tp_as_mapping`, `tp_repr`, `tp_hash`, `tp_call` и
      др.).

За счёт этого один и тот же байт‑код (`BINARY_ADD`, `LOAD_ATTR`, `FOR_ITER` и т.п.) работает с разными типами, просто
дергая разные функции из их `PyTypeObject`.

## PyObject vs PyVarObject и контейнеры

- Непеременного размера (пример: `int`):
    - структура типа `PyLongObject` начинается с `PyVarObject`, но фактические данные — массив «цифр» фиксированной
      длины, встроенный после заголовка; размер хранится в `ob_size`.
- Переменного размера (пример: `tuple`, `bytes`):
    - структура содержит `PyVarObject` + «открытый массив» элементов/байт в конце структуры; аллоцируется на один блок
      памяти сразу с нужным количеством элементов.

Списки (`list`) обычно содержат внутри указатель на отдельный C‑массив `PyObject*` (динамически растёт/сжимается), а
размер и capacity лежат в структуре списка.

## Категории встроенных типов (уровень Python)

С точки зрения языка стандартные типы делятся на:

- Числовые: `int`, `float`, `complex`.
- Последовательности: `list`, `tuple`, `range`, `str`, `bytes`, `bytearray`.
- Отображения: `dict`.
- Множества: `set`, `frozenset`.
- Прочие: `bool`, `NoneType`, пользовательские классы, функции, генераторы, файлы, исключения и т.д.

Внутри CPython эта классификация отражается в том, какие слоты реализованы: числам заполняют `tp_as_number`,
последовательностям — `tp_as_sequence`, отображениям — `tp_as_mapping`.

## type, object и метаклассы

- `type` — это и тип объектов (метакласс), и обычный объект:
    - `type.__bases__ == (object,)`, у `object` базовый тип — `type`;
    - `isinstance(type, object)` и `isinstance(object, type)` обе истина.
- Создание класса (через `class` или `type(...)`) по сути вызывает конструктор `type`, который формирует `PyTypeObject`,
  заполняет слоты и регистрирует новый тип.


- [Содержание](#содержание)

---

# **args и kwargs**

## **Junior Level**

`*args` и `**kwargs` — это специальные синтаксические конструкции в Python, позволяющие функциям принимать произвольное
количество аргументов.

**`*args`** (от слова "arguments") собирает все **позиционные аргументы**, переданные функции сверх явно объявленных, в
**кортеж**. Это полезно, когда количество передаваемых аргументов заранее неизвестно.

**`**kwargs`** (от "keyword arguments") собирает все **именованные аргументы** (ключ=значение), которые не были явно
перечислены в параметрах функции, в **словарь**.

Также символы `*` и `**` используются при **вызове** функции для распаковки коллекций:

- `*` распаковывает итерируемый объект (список, кортеж) в позиционные аргументы
- `**` распаковывает словарь в именованные аргументы

Этот механизм — основа для создания гибких API, декораторов и функций-обёрток.

## **Middle Level**

1. **Строгий порядок параметров в определении функции**:
   ```
   def f(a, b, *args, c=None, d=None, **kwargs)
   ```
   Порядок следования:
    - Позиционные параметры (a, b)
    - `*args` — собирает избыточные позиционные аргументы
    - Keyword-only аргументы (c, d) — после `*args` все параметры требуют явного указания имени
    - `**kwargs` — собирает избыточные именованные аргументы

2. **Внутреннее представление и особенности**:
    - При передаче словаря в `**kwargs` ключи **должны быть строками**
    - Дублирование имен аргументов при распаковке вызывает `TypeError`
    - `**kwargs` сохраняет порядок аргументов (с Python 3.6)
    - Метод `__getitem__` объекта используется при распаковке через `**`, что позволяет распаковывать любые
      mapping-объекты

3. **Принцип работы распаковки**:
   Когда вызывается `func(*[1, 2, 3])`, интерпретатор:
    - Создаёт итерируемый объект
    - Распаковывает его элементы в отдельные позиционные аргументы
    - Внутри функции эти аргументы доступны через кортеж `args`

## **Senior Level**

В CPython `*args` и `**kwargs` — это синтаксический сахар на уровне байт-кода, который компилируется в специальные
инструкции для упаковки/распаковки аргументов при вызове и определении функций. Реализация происходит в `ceval.c` (eval
loop) и `Python/call.c` (функциональные вызовы), где позиционные аргументы собираются в `tuple`, а именованные — в
`dict`.

## Компиляция в байт-код

- `*args` в определении функции (`def f(*args)`):
    - компилятор генерирует `MAKE_FUNCTION` с флагом `METH_VARARGS`, а внутри — `LOAD_FAST` для доступа к локальной
      переменной `args` (tuple всех позиционных аргументов после фиксированных).
- `**kwargs` (`def f(**kwargs)`):
    - аналогично, `METH_VARKEYWORDS`, `kwargs` становится `dict` с именованными аргументами.
- При вызове `f(*tup, **dct)`:
    - `*tup` распаковывается в `UNPACK_EX` или `BUILD_TUPLE_UNPACK_WITH_CALL`;
    - `**dct` — в `BUILD_MAP_UNPACK_WITH_CALL`, байт-код мержит словари аргументов.

Компилятор (`compile.c`) создаёт `arguments` структуру в `code object` с `argcount`, `kwonlyargcount`, указателями на
имена аргументов.

## Вызов функции в C (PyObject_Call)

- В `PyCFunction_CALL` (для C-функций) или `do_call` (Python-функций):
    - фиксированные позиционные аргументы берутся из `args` tuple;
    - `*args` — остаток tuple после фиксированных (slice от `argcount`);
    - именованные сначала мержатся в один `kwargs` dict (с приоритетом вызова над дефолтами);
    - `**kwargs` — весь этот dict присваивается локальной переменной.
- Порядок разрешения: positional < *args < keyword-only < **kwargs.

Если аргументов больше `va_list` лимита — аллокация `PyTuple_New()` и `PyDict_New()`.

## Локальные переменные и frame

- В `PyFrameObject` (call frame):
    - `args` — PyTupleObject с позиционными (refcount +1);
    - `kwargs` — PyDictObject (слабая ссылка, если пустой — NULL).
- Доступ через `LOAD_FAST 0` (для f0=args) или `LOAD_FAST 1` (kwargs).
- При выходе frame: `decref` аргументов, чтобы избежать утечек.

## Расширенные случаи (3.8+)

- `/` (positional-only): аргументы слева от `/` не видны в `inspect.signature`, но упаковываются в `*args`. PEP 570.
- `*` (keyword-only): после `*` — только kwargs, `*args` пустой.
- Сигнатура: `inspect` читает `co_varnames`, `co_argcount` из code object.

## Шпаргалка для собеса

- `*args` → tuple всех extra positional args (slice `args[argcount:]`).
- `**kwargs` → dict всех kw args (merged from call и defaults).
- Байт-код: `MAKE_FUNCTION` flags + `CALL_FUNCTION_KW`/`CALL_FUNCTION_EX`.
- C-логика: `PyEval_EvalFrameEx` парсит `co_flags`, аллоцирует locals.
- Edge cases: `f(*[], **{})`, nested unpacking, error "too many args".


- [Содержание](#содержание)

---

# *Хеш-таблица*

## **Junior Level**

Хеш-таблица — это структура данных, обеспечивающая амортизированную сложность O(1) для операций поиска, вставки и
удаления за счёт использования хеш-функции, преобразующей ключ в индекс массива. В Python она лежит в основе словарей (
dict) и множеств (set). Ключевыми особенностями являются требование хешируемости (неизменяемости) ключей, сохранение
порядка вставки и автоматическое увеличение размера при достижении определённого коэффициента заполнения.

Представьте библиотеку, где номер полки вычисляется по названию книги по определённому правилу (например, первая буква).
Это правило — **хеш-функция**. Она преобразует ключ в число-индекс. В идеале вы находите элемент за O(1) время.

**Коллизии** (когда разным ключам соответствует один индекс) решаются разными способами. Например, на «полке» может быть
список пар «ключ-значение», и вы ищете среди них по полному ключу.

В Python ключ словаря или элемент множества должен быть **хешируемым** (неизменяемым) объектом.

## **Middle Level**

В Python хеш-таблицы используют **открытую адресацию** с **квадратичным зондированием** для разрешения коллизий.

**Ключевые аспекты:**

1. **Хешируемость:** Объект хешируем, если:
    * Имеет метод `__hash__`, возвращающий целое число.
    * Имеет метод `__eq__` для сравнения.
    * Выполняется условие: `a == b` ⇒ `hash(a) == hash(b)`.
    * Неизменяемые типы (int, str, tuple, frozenset) хешируемы по умолчанию.
2. **Размер таблицы:** Всегда является степенью двойки, что позволяет вычислять индекс через битовую маску:
   `index = hash(key) & (table_size - 1)`.
3. **Коэффициент загрузки (load factor):** При заполнении ~2/3 таблица увеличивается вдвое (**rehashing**), что является
   амортизированной операцией O(n).
4. **Удаление элементов:** Элемент помечается как **dummy** (удалённый слот), чтобы не разрывать цепочки зондирования.
5. **Сохранение порядка:** Начиная с Python 3.7, порядок вставки в словаре гарантирован. Это достигается отдельным
   массивом записей (ключ-значение), который сохраняет порядок.

## **Senior Level**

В CPython `dict` и `set` реализованы как специализированные хеш‑таблицы с открытой адресацией и плотным массивом
записей; с Python 3.6+ реализация также гарантирует порядок вставки. Ниже — то, что обычно хотят услышать на собесе по
Core Python.

## Базовая структура

- Внутри словаря есть объект `PyDictObject`, который хранит:
    - указатель на структуру ключей `PyDictKeysObject` (хеш‑таблица);
    - массив значений (отдельный для экономии памяти при split‑table режиме).
- `PyDictKeysObject` содержит:
    - `dk_indices` — «разреженный» массив индексов (таблица хешей);
    - `dk_entries` — «плотный» массив записей `<hash, key, value_or_index>`.

Идея: индексы — это хеш‑таблица, а реальные записи лежат в плотном массиве, который удобно итерировать и в котором
сохраняется порядок вставки.

## Как вычисляется индекс по ключу

- У каждого ключа берётся хеш через `tp_hash` конкретного типа (для `int`, `str`, `tuple` и т.п. свои реализации).
- Начальный слот: \( i_0 = hash(key) \& mask \), где `mask = table_size - 1` (размер таблицы — степень двойки).
- Если слот занят другим ключом, используется открытая адресация с псевдослучайным шагом (модифицированный quadratic
  probing, чтобы хорошо перемешивать коллизии).

Ключевые моменты для собеса: словарь не использует separate chaining, в каждом слоте максимум один ключ; коллизии
решаются поиском другого слота по детерминированной, но сложной последовательности.

## Коллизии, пустые и удалённые слоты

- Слот может быть:
    - никогда не использованным (EMPTY);
    - занят (используется запись `<hash, key, ...>`);
    - помечен как удалённый (DUMMY).
- При поиске:
    - если видим EMPTY — поиск можно остановить, ключа нет;
    - если видим DUMMY — продолжаем поиск (это «дырка» в цепочке пробирования);
    - если хеш совпал и `==` дало `True` — нашли ключ.

Удаление помечает слот как DUMMY, чтобы не ломать цепочку пробирования других ключей, которые «перепрыгивали» через этот
слот при вставке.

## Вставка, рост и сжатие

- Вставка:
    - считаем хеш, ищем позицию по схеме open addressing;
    - если находим свой ключ — обновляем значение;
    - если находим EMPTY или DUMMY — используем этот слот, создаём новую запись в `dk_entries`.
- Увеличение таблицы:
    - когда нагрузка (заполненность + DUMMY) становится выше порога, словарь перераспределяется: создаётся новая
      `dk_indices` большего размера и все записи из `dk_entries` пере‑rehashиваются по новой маске.
- DUMMY‑слоты частично «сжимаются» при ресайзе: при создании новой таблицы удалённые слоты не переносятся.

Амортизированная сложность операций остаётся близкой к \(O(1)\) при разумной загрузке таблицы.

## Порядок вставки и layout с 3.6+

- С Python 3.6+ `dict` стал «compact dict»:
    - `dk_indices` хранит только индексы в `dk_entries` (или спецзначения EMPTY/DUMMY);
    - `dk_entries` — плотный массив записей в порядке вставки.
- Удаление:
    - слот в `dk_indices` помечается как DUMMY;
    - элемент в `dk_entries` физически остаётся, но пропускается при итерации.
- Новая вставка почти всегда добавляет запись в конец `dk_entries`, что сохраняет порядок, а в `dk_indices` записывается
  индекс на неё.

Важно: порядок — семантика уровня языка (гарантия начиная с 3.7), но реализация — побочный эффект compact dict.

## Требования к ключам (hashable / equality)

- Ключ должен быть хешируемым:
    - неизменяемый (логически), чтобы хеш не менялся за время жизни в словаре;
    - должен иметь согласованные `__hash__` и `__eq__`: если `a == b`, то `hash(a) == hash(b)`.
- Для пользовательских классов:
    - можно переопределить `__hash__` и `__eq__` так, чтобы использовать только неизменяемые атрибуты.

Знание этого на собесе часто проверяют вопросами «что будет, если изменить объект, который является ключом словаря».

## Краткая шпаргалка для ответа

Можно сформулировать устно примерно так (своими словами):

- `dict` — это хеш‑таблица с открытой адресацией, где есть разреженный массив индексов и плотный массив записей
  `<hash, key, value>`.
- Индекс вычисляется как `hash(key) & (size-1)` и при коллизиях используется пробирование с шагом, зависящим от
  хеша.
- У слотов есть состояния EMPTY/USED/DUMMY; DUMMY остаются после удаления, пока не произойдёт перераспределение.
- С 3.6+ `dict` сохраняет порядок вставки, потому что записи хранятся в плотном массиве в порядке добавления.
- Ключи должны быть хешируемыми, с корректно согласованными `__hash__` и `__eq__`.


- [Содержание](#содержание)

---

# *Встроенные функции*

## **Junior Level**

Встроенные функции — это базовый набор функций Python, доступных без импорта, так как они находятся в автоматически
загружаемом модуле `builtins`. Они охватывают основные операции языка: преобразование типов, математические вычисления,
работу с коллекциями, ввод-вывод и интроспекцию. Будучи частью ядра языка, эти функции реализованы максимально
эффективно и имеют стандартизированное поведение.

Эти функции охватывают основные операции: работу с типами данных (`str()`, `int()`, `list()`), математические
вычисления (`abs()`, `round()`, `sum()`), преобразования (`len()`, `sorted()`, `reversed()`), ввод-вывод (`print()`,
`input()`), итерации (`range()`, `enumerate()`, `zip()`), проверки (`isinstance()`, `hasattr()`), и другие
фундаментальные операции.

Важно понимать, что это не просто функции, а часть ядра языка. Они реализованы максимально эффективно и их поведение
стандартизировано.

## **Middle Level**

1. **Пространство имен `builtins`**: Все встроенные функции находятся в модуле `builtins`, который автоматически
   импортируется при запуске интерпретатора. Можно получить прямой доступ через `import builtins`. Переопределение
   функций в этом модуле (что крайне не рекомендуется) повлияет на всю программу.

2. **Категории встроенных функций**:

**Конструкторы типов (приведение и создание объектов):**

* `int()` (создает целое число из числа или строки)
* `float()` (создает число с плавающей точкой)
* `complex()` (создает комплексное число)
* `str()` (создает строковое представление объекта)
* `bytes()` (создает неизменяемую байтовую последовательность)
* `bytearray()` (создает изменяемую байтовую последовательность)
* `memoryview()` (создает "представление памяти" объекта для эффективного доступа без копирования)
* `bool()` (возвращает логическое значение объекта)
* `list()` (создает список)
* `tuple()` (создает кортеж)
* `range()` (создает неизменяемую последовательность чисел)
* `dict()` (создает словарь)
* `set()` (создает изменяемое множество)
* `frozenset()` (создает неизменяемое множество)
* `object()` (создает новый базовый объект — корень иерархии классов)

**Математические операции и числа:**

* `abs()` (возвращает абсолютное значение числа)
* `pow(x, y)` (возводит x в степень y, эквивалентно `x**y`)
* `divmod(a, b)` (возвращает частное и остаток от деления a на b как кортеж)
* `round()` (округляет число до заданной точности)
* `sum()` (суммирует элементы итерируемого объекта)
* `min()` (находит наименьший элемент)
* `max()` (находит наибольший элемент)
* `hex()` (преобразует целое число в шестнадцатеричную строку)
* `oct()` (преобразует целое число в восьмеричную строку)
* `bin()` (преобразует целое число в двоичную строку)

**Преобразования и проверки типов:**

* `ascii()` (возвращает строку, содержащую только ASCII-символы, не-ASCII экранируются)
* `repr()` (возвращает официальное строковое представление объекта, часто пригодное для `eval()`)
* `format(value, spec)` (форматирует значение по спецификации)
* `ord()` (возвращает код Unicode для заданного символа)
* `chr()` (возвращает символ Unicode по его коду)
* `hash()` (возвращает хеш-значение объекта)
* `type()` (возвращает тип объекта или создает новый класс)
* `isinstance()` (проверяет, является ли объект экземпляром класса или кортежа классов)
* `issubclass()` (проверяет, является ли класс подклассом другого класса)
* `callable()` (проверяет, можно ли вызвать объект как функцию)
* `len()` (возвращает длину (количество элементов) объекта)

**Работа с последовательностями и итерируемыми объектами:**

* `sorted()` (возвращает новый отсортированный список из итерируемого объекта)
* `reversed()` (возвращает обратный итератор)
* `enumerate()` (возвращает итератор, генерирующий пары (индекс, элемент))
* `zip()` (комбинирует элементы нескольких итераций в кортежи)
* `filter(func, iterable)` (фильтрует элементы, оставляя только те, для которых `func` возвращает `True`)
* `map(func, iterable)` (применяет функцию к каждому элементу итерируемого объекта)
* `all()` (возвращает `True`, если все элементы итерируемого объекта истинны)
* `any()` (возвращает `True`, если хотя бы один элемент итерируемого объекта истинен)
* `slice()` (создает объект среза для извлечения части последовательности)

**Итераторы и генераторы:**

* `iter()` (возвращает итератор для объекта)
* `next()` (возвращает следующий элемент итератора)

**Ввод-вывод и операции с файлами:**

* `print()` (выводит объекты в текстовый поток, обычно на экран)
* `input()` (считывает строку из стандартного ввода)
* `open()` (открывает файл и возвращает файловый объект)

**Компиляция и выполнение кода:**

* `eval()` (выполняет строку с кодом Python и возвращает результат)
* `exec()` (выполняет динамически созданный код Python)
* `compile()` (компилирует исходный код в объект кода или AST)

**Интроспекция и работа с атрибутами (Reflection):**

* `dir()` (возвращает список имен в текущей локальной области видимости или атрибутов объекта)
* `vars()` (возвращает словарь `__dict__` объекта или локальной области)
* `globals()` (возвращает словарь текущей глобальной области видимости)
* `locals()` (возвращает словарь текущей локальной области видимости)
* `getattr()` (возвращает значение атрибута объекта по его имени)
* `setattr()` (устанавливает значение атрибута объекта)
* `delattr()` (удаляет атрибут объекта)
* `hasattr()` (проверяет наличие атрибута у объекта)
* `id()` (возвращает "идентификатор" объекта — его уникальный адрес в памяти)

**Работа с классами и объектно-ориентированное программирование:**

* `property()` (создает свойство (property) — управляемый атрибут с геттером/сеттером)
* `classmethod()` (преобразует метод в метод класса (принимает `cls` вместо `self`))
* `staticmethod()` (преобразует метод в статический метод (не принимает `self` или `cls`))
* `super()` (возвращает прокси-объект, который делегирует вызовы методов родительскому классу)

**Разное и системные функции:**

* `__import__()` (низкоуровневая функция, которая реализует оператор `import`)
* `breakpoint()` (вызывает отладчик (по умолчанию pdb) в месте вызова)
* `help()` (запускает встроенную интерактивную справочную систему)
* `memoryview()` (см. конструкторы)
* `hash()` (см. преобразования и проверки)

3. **Особенности поведения**:

* `sorted()` всегда возвращает новый список, тогда как метод `list.sort()` изменяет список на месте
* `reversed()` возвращает итератор, а не список
* `map()` и `filter()` в Python 3 возвращают итераторы, а не списки (как было в Python 2)
* `range()` тоже возвращает специальный объект, а не список
* `open()` является фабрикой, возвращающей файловый объект с разным поведением в зависимости от режима

4. **Функции высшего порядка**: `map()`, `filter()`, `sorted()` принимают функции в качестве аргументов. Это делает их
   мощным инструментом для функционального программирования.

## **Senior Level (CPython, байткод и системные вызовы)**

## Реализация встроенных функций в CPython

Встроенные функции (built-in functions) в CPython реализованы преимущественно на языке C и располагаются в файле
`Python/bltinmodule.c`. Эти функции доступны без импорта и являются частью глобального пространства имен `builtins`.

### Архитектура и расположение

Большинство встроенных функций находятся в `Python/bltinmodule.c`, но есть исключения:

- **Встроенные типы** (`str`, `dict`, `list`, `int`) имеют собственные C-файлы в директории `Objects/`
- **Встроенные типы**: файлы с именами `Objects/<builtin>object.c` (например, `listobject.c`, `dictobject.c`)
- **Специальные модули**: `sys` находится в `Python/sysmodule.c`, `marshal` в `Python/marshal.c`

### Механизм работы

Встроенные функции реализуются как C-функции, которые регистрируются в таблице методов модуля `builtins`. При вызове
функции из Python:

1. **Интерпретатор CPython** ищет имя функции в глобальном пространстве имен `__builtins__`
2. **Вызов C-функции**: происходит через Python C API, где каждая функция принимает указатели на PyObject
3. **Обработка аргументов**: используются функции `PyArg_ParseTuple()` или `PyArg_ParseTupleAndKeywords()` для
   извлечения параметров
4. **Возврат результата**: C-функция возвращает новый `PyObject*` или `NULL` при ошибке

### Примеры реализации

Функции типа `len()`, `abs()`, `print()` реализованы как статические C-функции в `bltinmodule.c` с сигнатурой типа
`static PyObject* builtin_len(PyObject *self, PyObject *v)`. Функции `eval()` и `exec()` имеют доступ к контролю
пространства имен через параметр `__builtins__`, что позволяет ограничивать доступные функции в исполняемом коде.

### Важные детали для собеседования

- Функция `id()` в CPython возвращает адрес объекта в памяти
- Встроенные функции кэшируются в глобальном словаре и не требуют поиска в `sys.path`
- Типы данных наследуются от базового `PyObject` и используют единую систему подсчета ссылок для управления памятью


- [Содержание](#содержание)

---

# *Контекстные менеджеры*

# *Контекстные менеджеры*

## **Junior Level**

Контекстные менеджеры в Python — это специальные объекты, которые позволяют управлять ресурсами и выполнять настройку и
очистку до и после выполнения блока кода. Они используются с оператором `with`, который гарантирует правильное
приобретение и освобождение ресурсов, даже если в блоке кода произошла ошибка. Это делает их идеальными для работы с
файлами, сетевыми соединениями, транзакциями баз данных и блокировками.
Основная цель контекстного менеджера — безопасное управление ресурсами, требующими явного закрытия или очистки.
Классический пример — работа с файлами:

```python
with open('file.txt') as f:
    data = f.read()
# Файл гарантированно закрыт, даже если при чтении возникло исключение
```

Без использования `with` пришлось бы оборачивать операции в `try...finally`:

```python
f = open('file.txt')
try:
    data = f.read()
finally:
    f.close()
```

Контекстный менеджер инкапсулирует эту логику, делая код чище и безопаснее.

## **Middle Level**

### 1. **Протокол контекстного менеджера**

Любой объект становится контекстным менеджером, если реализует два специальных метода:

- `__enter__(self)` — вызывается при входе в блок `with`. Возвращаемое значение присваивается переменной после `as`.
- `__exit__(self, exc_type, exc_value, traceback)` — вызывается при выходе из блока `with`. Получает информацию об
  исключении (аргументы будут `None`, если исключения не было). Если метод возвращает `True`, исключение считается
  обработанным и не пробрасывается дальше.

### 2. **Способы создания**

**Через класс:**

```python
class MyContextManager:
    def __enter__(self):
        print("Выделение ресурса")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        print("Освобождение ресурса")
        # Если вернуть True, исключение будет подавлено
        return False


with MyContextManager() as cm:
    print("Работа внутри контекста")
```

**С помощью `contextlib.contextmanager` и генератора:**

```python
from contextlib import contextmanager


@contextmanager
def my_context():
    print("Выделение ресурса")
    yield "ресурс"  # значение для as
    print("Освобождение ресурса")


with my_context() as value:
    print(f"Работа с {value}")
```

Генератор должен содержать ровно один `yield`. Код до `yield` выполняется как `__enter__`, после — как `__exit__`.

### 3. **Готовые менеджеры из `contextlib`**

- `closing(thing)` — гарантирует вызов `thing.close()`.
- `suppress(*exceptions)` — подавляет указанные исключения в блоке.
- `nullcontext(enter_result)` — полезен для подстановки заглушки в тестах.
- `ExitStack` — для управления динамическим набором контекстов.

### 4. **Вложенность и группировка**

Контекстные менеджеры можно использовать группами:

```python
with open('a.txt') as f1, open('b.txt', 'w') as f2:
    f2.write(f1.read())
```

Порядок выхода из контекстов обратен порядку входа (LIFO). Исключение в `__enter__` приведет к тому, что `__exit__` не
будет вызван для этого менеджера, но уже вошедшие менеджеры получат вызов `__exit__`.

### 5. **Обработка исключений — детали**

Метод `__exit__` получает три аргумента об исключении:

- `exc_type`: класс исключения.
- `exc_value`: экземпляр исключения.
- `traceback`: объект трассировки.
  Если исключения не было, все они равны `None`. Возврат `True` подавляет исключение. Исключение, возникшее *внутри*
  `__exit__`, заменяет исходное (если оно было).

## **Senior Level**

## Реализация контекстных менеджеров в CPython

Контекстные менеджеры в CPython реализуются через специальный протокол, состоящий из двух магических методов:
`__enter__()` и `__exit__()`. Этот механизм используется для автоматического управления ресурсами даже при возникновении
исключений.

### Протокол контекстного менеджера

Класс становится контекстным менеджером при реализации двух методов:

- **`__enter__(self)`**: вызывается при входе в блок `with`, выполняет инициализацию ресурса и возвращает объект (часто
  `self` или другой ресурс)
- **`__exit__(self, exc_type, exc_value, traceback)`**: вызывается при выходе из блока `with`, освобождает ресурсы
  независимо от наличия исключений

### Механизм работы на уровне байткода

При компиляции конструкции `with` CPython генерирует специальные опкоды для управления блоком контекста:

1. **`SETUP_FINALLY`**: помещает блок `try-finally` в стек блоков, указывая на обработчик выхода
2. **Вызов `__enter__`**: происходит через стандартный механизм вызова методов
3. **`POP_BLOCK`**: удаляет блок из стека при нормальном завершении
4. **Автоматический вызов `__exit__`**: гарантируется даже при исключениях благодаря блоку `finally`

### Обработка исключений

Метод `__exit__` получает три аргумента, описывающих исключение (если оно возникло): `exc_type`, `exc_value`,
`traceback`. Возвращаемое значение определяет поведение:

- **`True`**: исключение подавляется (не распространяется дальше)
- **`False` или `None`**: исключение пробрасывается выше

### Альтернативные способы создания

CPython предоставляет модуль `contextlib` для упрощения создания контекстных менеджеров:

- **Декоратор `@contextmanager`**: позволяет создавать менеджеры из генераторов, где код до `yield` выполняется в
  `__enter__`, а после — в `__exit__`
- **`ExitStack`**: управляет динамическим стеком контекстов, полезен для вложенных менеджеров

### Важные детали для собеседования

Метод `__enter__` не обязан возвращать `self` — может возвращать любой объект или `None`. При использовании
`threading.local()` контекстные менеджеры становятся потокобезопасными, так как каждый поток получает собственный стек
контекстов. Метод `enter_context()` класса `ExitStack` позволяет программно входить в контекст и регистрировать его
выход.

- [Содержание](#содержание)

---

# *Декораторы и замыкания*

## **Junior Level**

Когда я объясняю декораторы новичкам, я обычно говорю: представьте, что у вас есть функция — допустим, она просто
складывает два числа. И вдруг вам понадобилось каждый раз, когда её вызывают, записывать в лог, что произошел вызов.
Можно, конечно, пойти и добавить в саму функцию строку с `print`, но что если таких функций много? Или если вы хотите то
включать логирование, то выключать?

Вот здесь и появляются декораторы. По сути, декоратор — это функция, которая принимает вашу функцию, оборачивает её в
дополнительную логику и возвращает новую, улучшенную версию. А синтаксис с собачкой (`@`) — это просто красивый способ
сказать Python: "Эй, примени этот декоратор к следующей функции".

А замыкания — это то, что делает декораторы возможными. Если очень просто: замыкание — это функция внутри функции,
которая помнит переменные из внешней функции даже после того, как та завершилась. Как будто у неё есть память. Например,
можно создать функцию-счетчик, которая будет помнить, сколько раз её вызвали.

## **Middle Level**

Когда мы говорим о декораторах на более глубоком уровне, важно понимать несколько ключевых моментов. Во-первых,
декоратор — это по сути паттерн проектирования, который позволяет добавлять поведение к функциям или методам, не изменяя
их исходный код. Это мощный инструмент для разделения ответственности.

Если декоратору нужно передать параметры — например, указать уровень логирования или количество повторов — структура
усложняется. Мы получаем как бы "фабрику декораторов": функция принимает параметры и возвращает уже сам декоратор,
который будет применен к целевой функции.

Когда вы видите несколько декораторов над одной функцией, важно помнить, что они применяются снизу вверх. Тот декоратор,
что ближе к функции, сработает первым, затем следующий и так далее. Это похоже на матрешку — каждый декоратор добавляет
свой слой обертки.

Один из самых важных моментов, который часто упускают — это сохранение метаданных функции. Когда декоратор создает новую
функцию-обертку, она теряет оригинальное имя, документацию и другие атрибуты. Именно поэтому мы всегда используем
`@wraps` из модуля `functools` — он копирует эти метаданные, что критически важно для отладки и работы многих
инструментов.

Декоратором может быть не только функция, но и класс. Для этого класс должен реализовать метод `__call__`, который
делает экземпляр вызываемым. Классы-декораторы особенно удобны, когда нужно сохранять какое-то состояние между
вызовами — например, вести счетчик или кэшировать результаты.

Теперь о замыканиях и областях видимости. Python использует правило LEGB: сначала ищет переменную локально, затем в
замыкающих функциях, потом глобально и наконец во встроенных именах. Замыкания работают благодаря доступу к внешним
областям. Но если вы хотите изменить переменную из замыкания, а не просто прочитать её, нужно использовать ключевое
слово `nonlocal`. Без него Python создаст новую локальную переменную, что обычно приводит к ошибкам.

На практике декораторы и замыкания находят огромное применение. В тестировании, например, `@pytest.mark.parametrize`
позволяет запускать один тест с разными данными, а `@unittest.mock.patch` временно подменяет объекты для изоляции
тестов. Встроенные декораторы Python вроде `@property`, `@classmethod` и `@staticmethod` активно используются в ООП.

Но важно помнить и об ограничениях. Декораторы добавляют накладные расходы — каждый вызов проходит через дополнительную
функцию. В критически важных по производительности местах это может иметь значение. Также цепочки декораторов могут
усложнять отладку, делая стек вызовов очень глубоким.

По сути, декораторы и замыкания — это инструменты, которые помогают нам писать более чистый, модульный и
переиспользуемый код. Они позволяют отделять сквозную функциональность вроде логирования, кэширования или проверки прав
от основной бизнес-логики, что соответствует принципам хорошего проектирования.

## **Senior Level (CPython, байткод и системные структуры)**

## Реализация декораторов и замыканий в CPython

Декораторы и замыкания в CPython построены на фундаментальной особенности Python: функции — это объекты первого класса,
которые можно передавать, возвращать и модифицировать. На уровне интерпретатора это реализуется через специальные
структуры для хранения свободных переменных и механизм создания closure-объектов.

### Замыкания: механизм и структуры данных

Замыкание (closure) возникает, когда внутренняя функция захватывает переменные из внешней области видимости, которая уже
завершила своё выполнение. В CPython это реализовано через:

- **Ячейки (cell objects)**: специальные объекты типа `PyCellObject`, которые хранят ссылки на переменные из внешней
  области видимости.
- **Атрибут `__closure__`**: кортеж ячеек, который хранится в объекте функции и содержит захваченные переменные.
- **Код-объект (`PyCodeObject`)**: содержит атрибуты `co_freevars` (имена свободных переменных для внутренней функции) и
  `co_cellvars` (имена переменных, которые используются вложенными функциями).

Когда внутренняя функция обращается к переменной из внешней области, байт-код использует специальные инструкции
`LOAD_DEREF` и `STORE_DEREF` для работы с ячейками, а не обычные `LOAD_FAST`/`STORE_FAST`. Это позволяет нескольким
функциям разделять доступ к одной переменной даже после завершения внешней функции.

### Декораторы: синтаксический сахар и протокол

Декоратор — это функция, которая принимает другую функцию и возвращает модифицированную версию (обычно обёртку).
Синтаксис `@decorator` перед определением функции — это синтаксический сахар, который компилятор разворачивает в вызов
декоратора:

```python
@decorator
def func():
    pass
# Эквивалентно: func = decorator(func)
```

На уровне байт-кода компилятор CPython:

1. Создаёт функцию `func` как обычный объект.
2. Загружает декоратор на стек (`LOAD_NAME` или `LOAD_GLOBAL`).
3. Вызывает декоратор с функцией как аргументом (`CALL_FUNCTION`).
4. Связывает результат с именем `func` (`STORE_NAME` или `STORE_GLOBAL`).

Декораторы могут быть вложенными: `@dec1 @dec2 def func()` применяются справа налево, то есть
`func = dec1(dec2(func))`.

### Декораторы-классы и магический метод `__call__`

Помимо функций-декораторов, CPython поддерживает классы-декораторы через магический метод `__call__`. Класс становится
вызываемым (callable), если определён метод `__call__(self, *args, **kwargs)`:

- В `__init__` сохраняется ссылка на декорируемую функцию.
- При вызове декорированной функции срабатывает `__call__`, который может добавить логику до/после вызова оригинальной
  функции.

Пример структуры:

```python
class Decorator:
    def __init__(self, func):
        self.__func = func

    def __call__(self, *args, **kwargs):
        # Логика до вызова
        result = self.__func(*args, **kwargs)
        # Логика после вызова
        return result
```

Такой подход удобен, когда декоратор должен сохранять состояние между вызовами, избегая вложенных функций.[3][2]

### Параметризованные декораторы

Декоратор может сам принимать параметры, становясь фабрикой декораторов — функцией, которая возвращает декоратор. Схема
работы:

```python
@decorator_factory(arg1, arg2)
def func():
    pass
# Эквивалентно: func = decorator_factory(arg1, arg2)(func)
```

Компилятор CPython обрабатывает это так:

1. Вызывает `decorator_factory(arg1, arg2)` — получает декоратор.
2. Вызывает полученный декоратор с `func` — получает обёрнутую функцию.
3. Связывает результат с именем `func`.

Такой трёхуровневый механизм (фабрика → декоратор → обёртка) часто реализуется через вложенные замыкания, где каждая
функция захватывает параметры из внешней области.

### Встроенные декораторы и дескрипторы

CPython предоставляет встроенные декораторы для методов классов:

- **`@staticmethod`**: метод не получает неявный первый аргумент (`self` или `cls`), реализован через дескриптор
  `PyStaticMethod_Type`.
- **`@classmethod`**: метод получает класс как первый аргумент (`cls`), реализован через дескриптор
  `PyClassMethod_Type`.
- **`@property`**: превращает метод в управляемый атрибут через дескрипторный протокол (`__get__`, `__set__`,
  `__delete__`).

Дескрипторы — это классы, определяющие один или более методов `__get__`, `__set__`, `__delete__`, которые перехватывают
доступ к атрибутам. Декораторы типа `@property` создают объекты-дескрипторы, что позволяет контролировать чтение/запись
атрибутов на уровне интерпретатора.

### Практические аспекты для собеседования

Важно понимать, что декораторы применяются **один раз** в момент определения функции, а не при каждом вызове. Функция
`functools.wraps` используется для копирования метаданных (`__name__`, `__doc__`, `__module__`) из оригинальной функции
в обёртку, что важно для отладки и интроспекции. Замыкания создают дополнительный overhead: каждая ячейка — это
отдельный `PyObject` с подсчётом ссылок, но это позволяет корректно работать с областями видимости и сборкой мусора.
Декораторы-классы удобнее функций, когда нужно хранить состояние или использовать наследование для создания семейств
декораторов.

- [Содержание](#содержание)

---

# **GIL (Global Interpreter Lock)**

## **Junior Level**

Когда я объясняю GIL новичкам, я часто использую такую аналогию. Представьте, что у вас есть большой офис с несколькими
сотрудниками (это потоки) и всего один принтер (это интерпретатор Python). Все сотрудники могут готовить документы
одновременно, но печатать они могут только по очереди — когда принтер свободен. GIL — это как очередь к этому
единственному принтеру.

Технически GIL — это глобальная блокировка в CPython (стандартной реализации Python), которая позволяет выполнять только
одному потоку Python-кода за раз, даже если у вас многоядерный процессор. Это значит, что для задач, которые сильно
нагружают процессор (например, сложные вычисления), многопоточность в Python не даст ускорения. Все потоки будут по
очереди работать на одном ядре.

Но есть важный нюанс. Во время операций ввода-вывода — когда программа ждет ответа от сети, читает файл или общается с
базой данных — поток освобождает GIL, позволяя другим потокам работать. Поэтому для I/O-задач (например, веб-серверов)
многопоточность всё равно полезна.

## **Middle Level**

Если говорить более глубоко, то GIL — это компромисс между простотой и производительностью. CPython использует подсчёт
ссылок для управления памятью, и эта система не является потокобезопасной. Вместо того чтобы делать каждую операцию с
памятью атомарной (что сильно замедлило бы выполнение), разработчики Python ввели одну общую блокировку для всего
интерпретатора.

Механизм работы GIL довольно интересен. Поток удерживает GIL не бесконечно — есть несколько условий, при которых
происходит переключение:

1. **Через определённое количество тиков байт-кода** (обычно каждые 100 инструкций)
2. **При операциях ввода-вывода** — когда поток уходит в ожидание
3. **При явном освобождении** в C-расширениях

Это создаёт некоторые проблемы. Например, если у вас есть поток, который выполняет долгие вычисления без операций
ввода-вывода, он может долго не отдавать GIL, и другие потоки будут простаивать. Это называется "голоданием" потоков.

На практике это означает, что для CPU-интенсивных задач нужно использовать другие подходы. Самый распространённый — *
*многопроцессорность** через модуль `multiprocessing`. Каждый процесс получает свой интерпретатор Python со своим GIL, и
они действительно могут работать параллельно на разных ядрах.

Есть и другие способы обойти ограничения GIL. **Асинхронное программирование** с `asyncio` позволяет эффективно работать
с I/O-задачами в одном потоке. **C-расширения** могут временно освобождать GIL во время вычислительных операций — так
работают библиотеки типа NumPy и SciPy. И существуют альтернативные реализации Python, такие как Jython или IronPython,
где GIL вообще отсутствует, но они имеют свои ограничения.

Важно понимать, что GIL — это особенность именно CPython, и у него есть свои причины для существования. Он упрощает
реализацию интерпретатора и делает более предсказуемой работу с памятью. Для многих реальных задач — веб-серверов,
скриптов автоматизации, работы с базами данных — GIL не является узким местом. Проблемы возникают в основном в научных
вычислениях и высоконагруженных вычислительных задачах, где как раз и используются специализированные библиотеки и
подходы.

Понимание GIL помогает выбрать правильную архитектуру для приложения. Если задача
CPU-интенсивная — смотрим в сторону многопроцессорности или выноса вычислений в C-расширения. Если I/O-интенсивная —
можно использовать потоки, асинхронное программирование или комбинацию подходов. GIL — это не приговор, а особенность,
которую нужно учитывать при проектировании.

## **Senior Level (CPython, байткод и системные вызовы)**

## Реализация GIL (Global Interpreter Lock) в CPython

GIL (Global Interpreter Lock) — это мьютекс (mutex), блокирующий одновременное выполнение нескольких потоков в CPython.
Только один поток может владеть GIL в каждый момент времени, что делает любую многопоточную Python-программу фактически
однопоточной при выполнении байткода.

### Причины появления GIL

GIL был введён для упрощения управления памятью CPython и обеспечения интеграции с C-расширениями. Система подсчёта
ссылок (reference counting) в CPython не является потокобезопасной: без защиты несколько потоков могут одновременно
изменять счётчик ссылок объекта, что приводит к утечкам памяти или преждевременному удалению объектов.

Альтернативой GIL могла бы быть установка блокировок на каждый объект Python, но это привело бы к:

- Значительному overhead на множественные блокировки/разблокировки при каждой операции
- Высокому риску взаимоблокировок (deadlocks)
- Снижению производительности однопоточных программ

Вместо этого CPython использует единую глобальную блокировку интерпретатора, что исключает взаимоблокировки и
минимизирует влияние на производительность однопоточного кода.

### Механизм работы GIL

Блокировка GIL действует на уровне байткода: любой поток должен захватить GIL перед выполнением инструкций
интерпретатора. Основные принципы:

- **Захват GIL перед выполнением**: поток получает блокировку, выполняет определённое количество байткодовых инструкций,
  затем освобождает её
- **Переключение потоков по тикам**: интерпретатор проверяет счётчик тиков после выполнения N инструкций (или через
  определённый интервал времени в Python 3.2+) и переключается на другой поток
- **Освобождение при I/O операциях**: при блокирующих операциях (чтение файла, сетевые запросы) поток освобождает GIL,
  позволяя другим потокам работать

В современных версиях (Python 3.2+) переключение потоков происходит не по числу инструкций, а по таймеру (по умолчанию
каждые 5 мс), что улучшает отзывчивость.

### Влияние на производительность

GIL критически влияет на **CPU-bound** задачи — вычислительно интенсивные операции, выполняющиеся в чистом Python-коде.
Многопоточность не даёт ускорения, а может даже замедлить выполнение из-за накладных расходов на переключение контекста
между потоками.

Для **I/O-bound** задач (работа с сетью, файлами, базами данных) GIL не является проблемой, так как потоки освобождают
блокировку во время ожидания операций ввода-вывода. Другие потоки могут использовать это время для выполнения своего
кода.

### Способы обхода GIL

Существует несколько стратегий работы с ограничениями GIL:

**Многопроцессность (`multiprocessing`)**: каждый процесс имеет собственный интерпретатор и GIL, что позволяет реально
параллельное выполнение на разных ядрах CPU. Недостаток — больший overhead на создание процессов и межпроцессную
коммуникацию.

**C-расширения с освобождением GIL**: библиотеки типа NumPy, pandas используют макрос `Py_BEGIN_ALLOW_THREADS` /
`Py_END_ALLOW_THREADS` для освобождения GIL во время выполнения вычислений в C-коде. Это позволяет другим Python-потокам
работать параллельно.

**Асинхронное программирование (`asyncio`)**: для I/O-bound задач cooperative multitasking через корутины эффективнее
многопоточности, так как избегает накладных расходов на переключение потоков и не страдает от GIL.

### Эволюция GIL в современных версиях

**Python 3.12 (PEP 684)** вводит возможность создания субинтерпретаторов с собственным GIL через C API функцию
`Py_NewInterpreterFromConfig()`. Каждый субинтерпретатор получает независимый GIL, что позволяет полноценно использовать
многоядерные процессоры без создания отдельных процессов.

**Python 3.13 (PEP 703)** представляет экспериментальную возможность полного отключения GIL при компиляции с флагом
`--disable-gil`. Проверить состояние GIL можно через `sys._is_gil_enabled()`. C-расширения должны явно указывать
поддержку режима без GIL через слот `Py_mod_gil` или функцию `PyUnstable_Module_SetGIL()`.

### Практические рекомендации для интервью

Важно понимать, что GIL — это не проблема языка Python, а специфика реализации интерпретатора CPython. Альтернативные
интерпретаторы (Jython, IronPython) не имеют GIL. При обсуждении многопоточности следует различать CPU-bound и I/O-bound
задачи: для первых используйте `multiprocessing` или C-расширения, для вторых достаточно `threading` или `asyncio`.
Глобальные переменные в многопоточном коде могут приводить к гонкам данных даже при наличии GIL, так как операции типа
`counter += 1` не являются атомарными на уровне байткода.

- [Содержание](#содержание)

---

# **Изменение списка во время итерации**

Изменять список во время итерации по нему опасно, так как это нарушает внутреннюю логику работы итератора и может
привести к непредсказуемому поведению, например, к пропуску элементов или ошибкам. Это происходит потому, что итератор
хранит текущий индекс, а при добавлении или удалении элементов индексы сдвигаются. Для безопасной модификации следует
итерировать по копии списка или использовать подход с созданием нового списка.

## **Junior Level**

Изменение списка во время итерации по нему — это опасная операция, которая может привести к непредсказуемому поведению,
ошибкам или даже падению программы. Когда вы перебираете элементы списка в цикле `for` и одновременно добавляете или
удаляете элементы из этого же списка, вы нарушаете внутреннюю логику работы итератора.

Представьте, что вы читаете книгу и кто-то одновременно вырывает из нее страницы или вклеивает новые. Вы можете
пропустить страницы или прочитать одну страницу дважды. То же самое происходит с итератором списка: он отслеживает
текущую позицию, но если список изменяется, эта позиция может стать невалидной.

На практике это приводит к таким проблемам:

- Пропуск элементов
- Обработка одного элемента дважды
- Бесконечные циклы
- `IndexError` или `RuntimeError`

## **Middle Level**

1. **Механизм итерации**: Когда вы создаете цикл `for item in my_list:`, Python вызывает `iter(my_list)`, который
   возвращает объект-итератор. Этот итератор хранит ссылку на список и текущий индекс. При каждой итерации он вызывает
   `__next__()`, который возвращает элемент по текущему индексу и увеличивает индекс.

2. **Что происходит при изменении**:

- **Удаление текущего или предыдущих элементов**: Итератор уже прошел эти индексы, но при удалении все последующие
  элементы сдвигаются. Это может привести к пропуску элемента, который сдвинулся на место удаленного.
- **Удаление последующих элементов**: Менее проблематично, но может вызвать `IndexError`, если итератор попытается
  обратиться к индексу за пределами нового размера списка.
- **Добавление элементов в конец**: Может привести к бесконечному циклу, если условие выхода зависит от размера списка.
- **Вставка элементов перед текущей позицией**: Итератор может обработать новый элемент, который был вставлен перед
  текущей позицией.

3. **Защита в Python**: Начиная с Python 3.7, при обнаружении изменения размера списка во время итерации бросается
   `RuntimeError`:

```
RuntimeError: dictionary changed size during iteration
```

Но для списков такой защиты **нет** (она есть только для словарей и множеств). Со списками Python ведет себя более
снисходительно, что делает ошибки более коварными.

4. **Безопасные альтернативы**:

- Итерация по копии списка: `for item in list(my_list):`
- Создание нового списка через list comprehension
- Использование `while` с ручным управлением индексом
- Сбор элементов для удаления в отдельный список и удаление после итерации

## **Senior Level (Байткод, память и грабли компилятора)**

1. **Байткод итерации по списку**:

Рассмотрим простой цикл:

```python
for item in my_list:
    process(item)
```

Байткод:

```
SETUP_LOOP               L3
LOAD_FAST                0 (my_list)
GET_ITER
L1: FOR_ITER              L2
STORE_FAST               1 (item)
LOAD_FAST                2 (process)
LOAD_FAST                1 (item)
CALL_FUNCTION            1
POP_TOP
JUMP_ABSOLUTE           L1
L2: POP_BLOCK
L3: ...
```

Ключевая инструкция — `FOR_ITER`. Она вызывает `listiter_next()` у итератора списка.

2. **Структура list iterator в CPython**:

В `Include/listobject.h`:

```c
typedef struct {
PyObject_HEAD
Py_ssize_t it_index;      // Текущий индекс
PyListObject *it_seq;     // Ссылка на исходный список
} listiterobject;
```

Итератор хранит:

- `it_index`: текущую позицию (от 0 до `ob_size-1`)
- `it_seq`: указатель на `PyListObject` исходного списка

3. **Функция `listiter_next()`** (в `Objects/listobject.c`):

```c
static PyObject *
listiter_next(listiterobject *it)
{
PyListObject *seq = it->it_seq;
PyObject *item;

if (seq == NULL)
   return NULL;

// Проверяем, не вышел ли индекс за границы
if (it->it_index < PyList_GET_SIZE(seq)) {
   item = PyList_GET_ITEM(seq, it->it_index);
   ++it->it_index;  // Увеличиваем индекс ДО возврата элемента
   Py_INCREF(item);
   return item;
}

// Итерация завершена
it->it_seq = NULL;
Py_DECREF(seq);
return NULL;
}
```

4. **Критическая проблема**: Индекс увеличивается (`++it->it_index`) **после** получения элемента, но **до** того как
   элемент будет обработан в теле цикла. Если в теле цикла удалить элемент из списка, структура списка изменится, но
   индекс уже указывает на следующую позицию.

5. **Пример опасного сценария**:

Список: `[A, B, C, D]`

- Итератор на элементе B (индекс=1)
- Удаляем B в теле цикла
- Список становится: `[A, C, D]`
- Индекс увеличивается до 2
- Следующая итерация вернет D, пропустив C!

6. **Динамический массив и сдвиги памяти**:

Список в CPython — это динамический массив `PyObject**`. При удалении элемента:

```c
// Objects/listobject.c: list_ass_slice()
memmove(&item[i+1], &item[i], (n - i) * sizeof(PyObject *));
```

`memmove` сдвигает все элементы после удаленного на одну позицию влево. Итератор продолжает работать со старым индексом,
который теперь указывает на другой элемент.

7. **Глубокий грабель: изменение через `__setitem__`**:

Даже если не менять размер, но заменить элемент:

```python
for i, item in enumerate(my_list):
    my_list[i] = transform(item)  # Может вызвать неожиданное поведение
```

Это безопаснее, но если `transform()` вызывает `__del__` у старого объекта, который имеет сайд-эффекты, могут быть
проблемы.

8. **Параллельная модификация из разных потоков**:

Если один поток итерирует список, а другой его модифицирует, возникает **data race**. Даже с GIL это может привести к
повреждению памяти, потому что операции не атомарны на уровне байткода.

9. **Коллекции с защитой от модификации**:

В отличие от списков, словари и множества с Python 3.7+ имеют детектор модификаций:

```c
// Objects/dictobject.c
if (d->ma_used != ep->me_used) {
PyErr_SetString(PyExc_RuntimeError,
               "dictionary changed size during iteration");
return NULL;
}
```

Список такой защиты не имеет, так как проверка на каждую итерацию замедлила бы самый частый случай.

10. **Оптимизации компилятора Python 3.11+**:

В Python 3.11 появилась специализация байткода для циклов. Цикл `for` может быть скомпилирован в более эффективную
форму, но это не меняет фундаментальной проблемы с итератором.

11. **Наиболее опасный паттерн для AQA**:

```python
# Тест, который иногда проходит, иногда нет
def test_flaky():
    items = []


for i in range(5):
    items.append(i)

removed = []
for item in items:
    if item % 2 == 0:
        items.remove(item)  # Удаление во время итерации!
        removed.append(item)

# Результат зависит от фазы луны
assert removed == [0, 2, 4]
```

Такой тест будет вести себя нестабильно (flaky test).

12. **Правильные паттерны для тестирования**:

- **Копирование**: `for item in list(original):`
- **Обратная итерация**: `for i in range(len(lst)-1, -1, -1):`
- **Filter-подход**: `new_list = [x for x in lst if condition(x)]`
- **While с ручным индексом**:

```python
i = 0
while i < len(lst):
    if condition(lst[i]):
        del lst[i]
    else:
        i += 1
```

13. **Отладка и диагностика**:

Для поиска таких ошибок в тестируемом коде:

- Использовать `sys.settrace()` для отслеживания модификаций списка
- Создать wrapper-класс для списка, который бросает исключение при модификации во время итерации
- Использовать статический анализ (mypy, pylint с соответствующими правилами)

14. **Особый случай: `enumerate()`**:

`enumerate()` создает итератор, который возвращает пары (индекс, элемент). Индекс отражает позицию **на момент создания
итератора**, а не текущее состояние списка.

```python
for i, item in enumerate(lst):
    del lst[i]  # Удалит не тот элемент!
```

После удаления индексы смещаются, но `enumerate` продолжает выдавать старые индексы.

- [Содержание](#содержание)

---

### **Области видимости**

**Junior Level**

Представьте себе область видимости в Python как систему комнат в доме. Переменные, созданные в одной комнате (например,
в функции), не всегда видны в другой. Это фундаментальное правило помогает организовывать код и избегать конфликтов
имён.

Когда вы обращаетесь к переменной, Python ищет её, последовательно проверяя четыре «комнаты» или уровня. Это правило
удобно запомнить как акроним **LEGB**:

* **Local (Локальная):** Сначала интерпретатор смотрит внутри текущей функции — это её внутренняя, локальная комната.
* **Enclosing (Охватывающая):** Если функция вложена в другую, Python проверяет «комнаты» внешних функций.
* **Global (Глобальная):** Затем поиск переходит на уровень всего модуля (вашего файла с кодом).
* **Built-in (Встроенная):** В самом конце проверяются встроенные функции и типы языка, такие как `print` или `len`.

Ключевой момент: простое использование (чтение) переменной из внешней области обычно работает, а вот её **изменение** —
нет. Если внутри функции вы попытаетесь присвоить новое значение переменной с внешнего уровня, Python по умолчанию
создаст новую локальную переменную с тем же именем, оставив внешнюю неизменной. Чтобы явно сказать интерпретатору, что
вы хотите работать с уже существующей внешней переменной, используются специальные инструкции:

* `global` — позволяет изменять переменную, объявленную на уровне модуля (глобально).
* `nonlocal` — используется во вложенных функциях и указывает, что переменная принадлежит области видимости ближайшей
  внешней функции (но не глобальной).

**Middle Level**

Углубляясь, стоит понимать, что каждая область видимости связана со своим **пространством имён**. По сути, это словарь,
который связывает имя объекта (например, переменной) с самим объектом в памяти. Время жизни этих пространств разное:
локальное пространство функции рождается при её вызове и обычно исчезает после завершения, в то время как глобальное
пространство модуля существует всё время его загрузки.

Инструкции `global` и `nonlocal` — это инструменты для управления связью с этими пространствами имён. Важно, что
`nonlocal` появилось в Python 3 именно для решения задач с вложенными областями, позволяя изменять переменные не
локально и не глобально, а именно в охватывающей функции.

Эта механика лежит в основе **замыканий** — мощного приёма, когда внутренняя функция «запоминает» (сохраняет ссылку на)
окружение, в котором она была создана, даже после того, как внешняя функция завершила работу. Благодаря этому,
переменные из внешней области могут жить дольше, чем сама функция, их создавшая.

Современный Python (начиная с версии 3) также изолирует области видимости внутри **генераторных выражений, списковых
включений (comprehensions)** и аналогичных конструкций. Это означает, что переменная, объявленная внутри такого
выражения, не «просачивается» наружу, предотвращая неожиданные изменения в вашем коде.

Когда речь заходит о **классах**, их тело во время создания формирует своё временное пространство имён, которое затем
превращается в атрибут `__dict__` класса. Методы экземпляра получают доступ к данным через первый аргумент `self`,
который ссылается на конкретный экземпляр, обеспечивая чёткое разделение между атрибутами класса и атрибутами его
объектов.

Наконец, важно помнить о контексте выполнения модуля. При запуске скрипта напрямую его глобальное пространство имён
получает специальное имя `__main__`. Каждый импортированный модуль живёт в своём собственном изолированном глобальном
пространстве, что предотвращает коллизии имён между разными частями программы и способствует созданию чистой, модульной
архитектуры.

## **Senior Level**

В CPython области видимости реализуются через работу компилятора (symbol table → code object) и интерпретатора (фреймы,
массив localsplus, инструкции `LOAD_*` / `STORE_*`). Ниже — только «подкапотные» детали, без общеобразовательной
воды.

## Анализ областей видимости на этапе компиляции

На этапе компиляции исходник проходит через построение таблиц символов (`symtable`), где каждому идентификатору
назначается тип и область видимости. Для этого компилятор анализирует, где имя читается, где в него пишут, есть ли
`global`/`nonlocal`, используется ли имя во вложенных функциях и т.д.

Результат — для каждого блока (module / function / class / comprehension) строится объект symbol table, где имена
помечаются флагами: local, global, free, cell и т.п. Эти данные мигрируют в `PyCodeObject`:

- `co_varnames` — локальные переменные «быстрого доступа» (fast locals).
- `co_freevars` — имена, захваченные из внешних областей (free vars).
- `co_cellvars` — локальные переменные, которые становятся частью замыкания (cell vars).
- `co_names` — остальные имена (глобалы, атрибуты и пр.).

По этим маркерам компилятор выбирает конкретные опкоды: `LOAD_FAST`, `LOAD_GLOBAL`, `LOAD_DEREF`, `LOAD_NAME`,
`STORE_FAST`, `STORE_GLOBAL`, `STORE_DEREF` и т.д.

## Фрейм и физическое хранение локалов

При вызове функции на уровне C создаётся `PyFrameObject`, который содержит: код-объект, указатель на предыдущий фрейм,
стэк значений и структуру `localsplus`.

В `localsplus` в современных версиях CPython хранятся:

- «Быстрые» локальные переменные (`co_varnames`) в фиксированном диапазоне слотов.
- Ячейки для `cellvars`/`freevars` (closure) в отдельной части массива.

`f_locals` во фрейме — это *не* основной storage локалов, а «ленивый» словарь, синхронизируемый с `localsplus` через
функции вроде `_PyFrame_FastToLocalsWithError()`. В обычном исполнении доступ к локалам идёт по индексам массива, а не
через `dict`; словарь создаётся только по запросу (например, при обращении к `frame.f_locals` или вызове `locals()`),
что важно для производительности.

Глобалы и builtins в фрейме представлены отдельными словарями:

- `f_globals` — `dict` модуля (его `__dict__`).
- `f_builtins` — `builtins.__dict__`.

## Разрешение имён: байткод и алгоритм

Тип опкода для чтения/записи имени зависит от классификации символа компилятором:

- Локальные fast‑vars → `LOAD_FAST` / `STORE_FAST` с индексом в `co_varnames`.
- Имя, точно не локальное (не помечено как local/cell/free) → `LOAD_GLOBAL` / `STORE_GLOBAL` с индексом в
  `co_names`.
- Переменные из замыканий → `LOAD_DEREF` / `STORE_DEREF` с индексом в массиве cell/free.
- В некоторых контекстах (класс, `exec`, модуль) используется `LOAD_NAME`, который реализует общий поиск по
  locals/globals/builtins.

Под капотом опкоды делают следующее:

### `LOAD_FAST i`

- Берёт значение из `localsplus[i]` без словарей и дополнительных проверок, кроме проверки на `NULL` (в случае
  неинициализированной переменной).
- Это самый дешёвый вид доступа к имени (один массивный индекс + push на value stack).

### `LOAD_GLOBAL i`

- Получает `PyObject *name = GETITEM(co_names, i)`.
- Делает lookup в `f_globals` (словарь модуля); если не нашли, делает lookup в `f_builtins`.
- Если нигде не найдено — выбрасывает `NameError`.

Ключевой момент: `LOAD_GLOBAL` *вообще не смотрит* в локальные fast‑vars, потому что компилятор заранее решил, что имя
не может быть локальным.

### `LOAD_NAME i`

- Используется в ситуациях, где локалы не являются обычными fast‑vars (например, тело модуля, класс, `exec`).
- Реализует «полное» LEGB-подобное поведение: сначала ищет в локальном словаре (`f_locals`), затем в `f_globals`, затем
  в `f_builtins`.
- В отличие от `LOAD_GLOBAL`, действительно заглядывает в локальные маппинги.

### `LOAD_DEREF i`

- Используется для свободных/ячеечных переменных (enclosing scopes).
- Получает `PyCellObject *cell` из секции cell/free в `localsplus` и читает `cell->ob_ref`.
- Вложенная функция получает указатель на ту же ячейку, так что изменения через `STORE_DEREF` отражаются во внешнем
  замыкании.

### Запись: `STORE_FAST`, `STORE_GLOBAL`, `STORE_DEREF`

- `STORE_FAST i` просто кладёт объект в слот `localsplus[i]` (инкрементируя refcount), при этом область видимости — уже
  решённый факт (локальная).
- `STORE_GLOBAL i` делает запись в `f_globals` по ключу `co_names[i]`.
- `STORE_DEREF i` обновляет `cell->ob_ref` в массиве closure.

Поскольку область видимости (local/global/nonlocal) зашита в выбор опкода и индекс, на этапе выполнения *нет* логики
анализа `global` или `nonlocal` — это всё уже отработало на уровне компилятора.

## Влияние `global` и `nonlocal` на байткод

При разборе функции компилятор строит таблицу символов и отмечает имена из директив `global` как принадлежащие
глобальному namespace модуля.

- Для таких имён в теле функции не создаются fast‑slots (`co_varnames`), и для чтения/записи генерируются
  `LOAD_GLOBAL` / `STORE_GLOBAL`.
- Для имён из `nonlocal` компилятор ищет их в внешних блоках; найденные помечаются как `free` в текущем блоке и как
  `cell` в enclosing блоке.
- В результате внутренний код использует `LOAD_DEREF` / `STORE_DEREF`, а во внешней функции переменная перемещается из
  `co_varnames` в `co_cellvars` и хранится в `PyCellObject`.

Если имя используется и для чтения, и для записи без директив, оно помечается как локальное и компилятор генерирует
`LOAD_FAST` / `STORE_FAST`; попытка чтения до присваивания на рантайме приводит к `UnboundLocalError`, потому что слот
есть, но там `NULL`.

## Отдельные области видимости: модуль, класс, exec

### Модульный уровень

- Тело модуля исполняется как код-объект, у которого `f_locals` и `f_globals` указывают на один и тот же dict (namespace
  модуля).
- Доступ к именам чаще всего идёт через `LOAD_NAME` / `STORE_NAME`, которые работают поверх `dict`, а не
  fast‑locals.

### Тело класса

- Тело `class` исполняется с отдельным локальным mapping (не список fast‑vars), обычно обычный `dict` или объект,
  переданный через `__prepare__`.
- Имена в теле класса читаются/пишутся через `LOAD_NAME` / `STORE_NAME` и живут в `locals` этого блока, а после
  завершения создаётся объект класса на основе полученного mapping.

### `exec` / `eval`

- Для `exec(source, globals, locals)` интерпретатор подготавливает фрейм, где `f_globals` и `f_locals` указывают на
  заданные mapping‑объекты, и опкоды `LOAD_NAME` / `STORE_NAME` работают через протокол Mapping API.
- Это даёт возможность иметь «нестандартные» области видимости, реализованные не через dict, но интерпретатор всё равно
  опирается на общую машинерию опкодов `*_NAME`.

## Интерактивный и отладочный доступ к scope

- `frame.f_locals` в Python‑коде формируется вызовом `_PyFrame_FastToLocalsWithError()`, который копирует содержимое
  `localsplus` в словарь и кеширует его внутри `frame`.
- Обратная синхронизация (из dict обратно в fast‑locals) — через `_PyFrame_LocalsToFast()`; этим пользуются `exec`,
  дебаггеры, `locals()` в некоторых режимах.
- Обсуждения в PEP‑558 / PEP‑667 и issue `PyFrame_GetVar` как раз касаются того, как безопасно и консистентно
  публиковать view на fast‑locals, не ломая производительность.

Таким образом, «области видимости» в CPython — это результат статического анализа symbol table (решение, в какой
таблице/ячейке живёт имя и какой опкод его обслуживает) плюс конкретная структура памяти (`localsplus` +
`cell/free vars` + `f_globals`/`f_builtins`) и реализация опкодов, выполняющих lookup/запись по уже выбранному уровню
scope.

- [Содержание](#содержание)

---

# **Lambda-функции**

Lambda-функции в Python — это анонимные функции, определяемые с помощью ключевого слова `lambda` и предназначенные для
выполнения простых операций в одну строку. Они могут содержать только одно выражение, которое автоматически
возвращается, и часто используются в функциях высшего порядка, таких как `map()`, `filter()` и `sorted()`. Несмотря на
ограничения, лямбды поддерживают замыкания и аргументы по умолчанию.

## **Junior Level**

Lambda-функции в Python — это анонимные (безымянные) функции, которые могут содержать только одно выражение и возвращают
результат его вычисления. Они создаются с помощью ключевого слова `lambda` и часто используются для простых операций,
когда не хочется определять полноценную функцию через `def`.

Представьте, что вам нужно быстро выполнить простое преобразование данных, например, удвоить каждое число в списке.
Вместо того чтобы писать отдельную функцию, можно использовать lambda: `map(lambda x: x * 2, numbers)`. Lambda-функции
особенно полезны в сочетании с функциями высшего порядка: `sorted()`, `filter()`, `map()` и другими.

Основные ограничения lambda:

- Может содержать только одно выражение
- Не может содержать операторы (только выражения)
- Не может иметь документацию
- Обычно помещается в одну строку

## **Middle Level**

1. **Синтаксис и семантика**: `lambda arguments: expression`

- `arguments` — ноль или более аргументов через запятую
- `expression` — одно выражение, результат которого возвращается
- Неявный `return`: результат выражения автоматически возвращается

2. **Замыкания и захват переменных**: Lambda может захватывать переменные из окружающей области видимости (closure). Это
   делает их мощным инструментом для создания фабрик функций на лету.

3. **Сравнение с обычными функциями**:

- Lambda создает объект функции, но без имени (`__name__ == '<lambda>'`)
- Не может содержать аннотации типов (до Python 3.12, где появилась поддержка синтаксиса, но с ограничениями)
- Не может быть декорирована обычным синтаксисом декораторов
- Всегда возвращает один результат

4. **Использование с функциями высшего порядка**:

- `sorted(items, key=lambda x: x[1])` — сортировка по второму элементу
- `filter(lambda x: x > 0, values)` — фильтрация положительных чисел
- `map(lambda x, y: x + y, list1, list2)` — поэлементное сложение
- `functools.reduce(lambda acc, x: acc * x, numbers)` — аккумуляция

5. **Пространства имен и аргументы по умолчанию**:
   Lambda может иметь аргументы со значениями по умолчанию: `lambda x, y=10: x + y`
   Важно: значения по умолчанию вычисляются в момент **определения** lambda, а не вызова.

6. **Распространенная ошибка с замыканиями**:

```python
funcs = [lambda: i for i in range(3)]
# Все функции вернут 2!
```

Причина: все lambda захватывают переменную `i`, а не ее значение на момент создания. Значение берется в момент
вызова, когда цикл уже завершился.

## **Senior Level**

1. **Компиляция lambda в байткод**:

Lambda-функция компилируется почти так же, как обычная функция. Рассмотрим:

```python
f = lambda x: x * 2
```

Байткод для этого выражения:

```
LOAD_CONST               <code object <lambda> at 0x...>
LOAD_CONST               '<lambda>'
MAKE_FUNCTION            0
STORE_FAST               f
```

Ключевые моменты:

- Объект кода lambda создается на этапе **компиляции** и помещается в константы родительского кода
- `MAKE_FUNCTION` создает объект функции во время **выполнения**
- Имя функции всегда `'<lambda>'`

2. **Объект кода lambda**:

В CPython объект `PyCodeObject` для lambda почти идентичен объекту кода обычной функции:

```c
typedef struct {
   PyObject_HEAD
   // ...
   PyObject *co_code;        // Байткод
   PyObject *co_consts;      // Константы
   PyObject *co_names;       // Имена глобальных переменных
   PyObject *co_varnames;    // Имена локальных переменных
   PyObject *co_freevars;    // Свободные переменные (для замыканий)
   PyObject *co_cellvars;    // Ячейки (cell variables)
   // ...
} PyCodeObject;
```

Отличие от обычной функции: `co_flags` не имеет флага `CO_NEWLOCALS`? На самом деле, lambda тоже создает локальное
пространство имен.

3. **Байткод внутри lambda**:

Для `lambda x: x * 2`:

```
0 LOAD_FAST                0 (x)
2 LOAD_CONST               1 (2)
4 BINARY_MULTIPLY
6 RETURN_VALUE
```

Важно: lambda использует `LOAD_FAST` для доступа к аргументам, как и обычная функция. Это быстрее, чем `LOAD_NAME`
или `LOAD_GLOBAL`.

4. **Замыкания в lambda**:

Когда lambda захватывает переменные из внешней области видимости:

```python
def outer(n):
    return lambda x: x * n
```

Байткод для lambda:

```
0 LOAD_DEREF               0 (n)   # Загружает из замыкания
2 LOAD_FAST                0 (x)
4 BINARY_MULTIPLY
6 RETURN_VALUE
```

Lambda хранит ссылки на cell objects в `__closure__`, как и обычная функция.

5. **Создание функции: `MAKE_FUNCTION`**:

Инструкция `MAKE_FUNCTION` (код 0x84 в Python 3.10+):

- Берет объект кода из стека
- Создает объект функции `PyFunctionObject`
- Устанавливает `__name__`, `__doc__`, `__qualname__`
- Для lambda `__name__` устанавливается в `'<lambda>'`

В CPython `PyFunction_New`:

```c
PyObject *
PyFunction_New(PyObject *code, PyObject *globals)
{
   PyFunctionObject *op = PyObject_GC_New(PyFunctionObject, 
                                          &PyFunction_Type);
   op->func_code = code;
   op->func_globals = globals;
   op->func_defaults = NULL;
   op->func_kwdefaults = NULL;
   op->func_closure = NULL;
   op->func_doc = NULL;
   op->func_name = ((PyCodeObject *)code)->co_name;
   // ...
}
```

6. **Проблема с аргументами по умолчанию**:

Рассмотрим:

```python
funcs = [lambda x, i=i: x + i for i in range(3)]
```

Здесь `i=i` создает **локальную** переменную в лямбде, которая инициализируется значением `i` на момент создания. В
байткоде:

```
# Для каждой lambda в цикле
LOAD_FAST                0 (i)    # Текущее значение i
BUILD_TUPLE              1
LOAD_CONST               <code>
LOAD_CONST               '<lambda>'
MAKE_FUNCTION            1        # 1 = есть defaults
```

Аргументы по умолчанию хранятся в `func_defaults` как кортеж.

7. **Производительность lambda vs def**:

- **Создание**: Lambda создается во время выполнения, как и функция через `def`. Нет разницы в производительности
  создания.
- **Вызов**: Вызов lambda идентичен вызову обычной функции. Одинаковое количество байткод-инструкций.
- **Чтение**: Lambda не имеет `__code__.co_firstlineno`, что может затруднить отладку.
- **Мемоизация**: Сложнее мемоизировать lambda, так как у них нет имени для кэширования.

8. **Специфика Python 3.12+**:

В Python 3.12 появилась ограниченная поддержка аннотаций типов в lambda:

```python
f: Callable[[int], int] = lambda x: x * 2
```

Но сама lambda не может содержать аннотации в своем синтаксисе. Также улучшены сообщения об ошибках в lambda.

9. **Интроспекция lambda**:

Lambda имеет те же атрибуты, что и обычная функция, но с особенностями:

- `lambda x: x.__name__` вернет `'<lambda>'`
- `lambda x: x.__code__` содержит байткод
- `lambda x: x.__defaults__` для значений по умолчанию
- `lambda x: x.__closure__` для замыканий

Однако `inspect.getsource()` не работает для lambda, так как у них нет исходного кода в отдельной строке.

10. **Рекурсия с lambda**:

Lambda не может рекурсивно вызывать себя по имени, так как у нее нет имени. Но можно использовать Y-комбинатор:

```python
Y = lambda f: (lambda x: f(lambda y: x(x)(y)))(lambda x: f(lambda y: x(x)(y)))
factorial = Y(lambda f: lambda n: 1 if n == 0 else n * f(n - 1))
```

Практического применения мало, но показывает выразительную мощность lambda.

11. **Оптимизации компилятора**:

В Python 3.11+ компилятор пытается оптимизировать вызовы lambda:

- Инлайн-кэширование для часто вызываемых lambda
- Специализация байткода для lambda с простыми операциями
- Однако lambda все равно создает полный объект функции и фрейм при вызове

12. **Ограничения и обходные пути**:

Lambda не может содержать:

- Присваивания (`=`)
- Операторы (`if`, `for`, `while`)
- Исключения (`try/except`)

Обходные пути:

- Условное выражение: `lambda x: True if x > 0 else False`
- Логические операторы: `lambda x: x > 0 and "positive" or "non-positive"`
- Вызовы других функций: `lambda x: some_func(x)`

13. **Тестирование кода с lambda**:

Для AQA важно уметь тестировать lambda-функции:

- **Мокирование**: Lambda может быть заменена mock-объектом
- **Проверка поведения**: Можно проверить, что lambda вызывается с правильными аргументами
- **Тестирование замыканий**: Проверка захваченных значений
- **Производительность**: Замер времени выполнения lambda в цикле

14. **Диагностика проблем**:

При ошибках в lambda трассировка стека показывает `<lambda>` вместо имени функции. Для отладки можно:

- Временно заменить lambda на именованную функцию
- Использовать `inspect.currentframe()` внутри lambda для получения контекста
- Добавить отладочный вывод через обертку

- [Содержание](#содержание)

---

# *Comprehensions и генераторные выражения*

Comprehensions — это лаконичный синтаксис для создания коллекций (списков, множеств, словарей) на основе итераций с
возможностью фильтрации. Генераторные выражения, оформленные в круглые скобки, создают итераторы, которые вычисляют
элементы «лениво», экономя память. Обе конструкции выполняются в собственной области видимости и обычно работают быстрее
эквивалентных циклов за счёт внутренних оптимизаций CPython.

## **Junior Level**

Comprehensions и генераторные выражения — это лаконичные конструкции в Python для создания коллекций на основе итераций.
Представьте, что вам нужно преобразовать один список в другой: вместо написания многострочного цикла `for` можно
использовать одну строку с comprehension.

List comprehension (списковое включение) создаёт новый список: `[x*2 for x in range(5)]` даст `[0, 2, 4, 6, 8]`. Set
comprehension создаёт множество, dict comprehension — словарь. Генераторное выражение выглядит похоже, но в круглых
скобках: `(x*2 for x in range(5))`. Разница в том, что генераторное выражение не создаёт коллекцию сразу, а возвращает
итератор, который вычисляет элементы «лениво», по одному, что экономит память при работе с большими объёмами данных.

Их удобно использовать для фильтрации (добавив `if`) и для преобразования элементов. Это делает код чище и часто
быстрее, чем аналогичные циклы.

## **Middle Level**

1. **Типы comprehensions**:

- List comprehension: `[выражение for элемент in итератор]`
- Set comprehension: `{выражение for элемент in итератор}`
- Dict comprehension: `{ключ: значение for элемент in итератор}`
- Generator expression: `(выражение for элемент in итератор)`

2. **Синтаксические возможности**:

- Могут содержать несколько циклов `for`: `[x+y for x in list1 for y in list2]`
- Поддерживают условия фильтрации `if`: `[x for x in range(10) if x % 2 == 0]`
- Условия могут быть вложенными и комбинированными

3. **Область видимости**: Начиная с Python 3, comprehensions и генераторные выражения выполняются в собственной области
   видимости. Переменные, созданные внутри (например, переменная цикла), не «просачиваются» наружу, что предотвращает
   случайные перезаписи.

4. **Производительность**: List comprehensions обычно выполняются быстрее эквивалентных циклов `for`, потому что они
   оптимизированы на уровне байткода и выполняют операции добавления элементов напрямую, минуя вызовы методов.
   Генераторные выражения экономят память, но имеют небольшие накладные расходы на каждый вызов `next()`.

5. **Ленивые вычисления**: Генераторные выражения вычисляют элементы только когда они запрашиваются (например, в цикле
   `for` или при вызове `next()`). Это позволяет работать с бесконечными последовательностями и потоками данных.

6. **Отличия от функций-генераторов**: Генераторные выражения — это синтаксический сахар для создания анонимных
   генераторов. Они не могут содержать сложную логику с несколькими `yield` или `return`, в отличие от
   функций-генераторов.

## **Senior Level**

1. **Компиляция comprehensions**:

В CPython каждое comprehension компилируется во **временную функцию**. Например, list comprehension
`[x*2 for x in iterable]` преобразуется в скрытую функцию, которая создаёт список, выполняет цикл и возвращает
результат.

Байткод для вызова list comprehension:

```
LOAD_CONST               <code object <listcomp> at 0x...>
LOAD_CONST               '<listcomp>'
MAKE_FUNCTION            0
LOAD_GLOBAL              iterable
GET_ITER
CALL_FUNCTION            1
```

Объект кода `<listcomp>` содержит байткод, реализующий логику comprehension.

2. **Внутренняя функция comprehension**:

Байткод внутренней функции для `[x*2 for x in range(5)]`:

```
0 BUILD_LIST               0       # Создаём пустой список
2 LOAD_FAST                0 (.0)  # Загружаем итератор
4 FOR_ITER                16 (to 22)
6 STORE_FAST               1 (x)   # Сохраняем текущий элемент в x
8 LOAD_FAST                1 (x)
10 LOAD_CONST               0 (2)
12 BINARY_MULTIPLY
14 LIST_APPEND              2       # Добавляем результат в список
16 JUMP_ABSOLUTE            4
18 POP_BLOCK
20 RETURN_VALUE
```

Ключевая инструкция `LIST_APPEND` (код 0x69) добавляет элемент напрямую во внутренний массив списка, что быстрее, чем
вызов метода `append()`.

3. **Генераторные выражения**:

Генераторное выражение `(x*2 for x in range(5))` компилируется в код, создающий объект генератора. Внутренняя функция
`<genexpr>` использует `YIELD_VALUE`:

```
0 LOAD_FAST                0 (.0)
2 FOR_ITER                12 (to 16)
4 STORE_FAST               1 (x)
6 LOAD_FAST                1 (x)
8 LOAD_CONST               0 (2)

10 BINARY_MULTIPLY
12 YIELD_VALUE # Возвращаем значение и приостанавливаемся
14 JUMP_ABSOLUTE 2
16 LOAD_CONST 1 (None)
18 RETURN_VALUE

```

При каждом вызове `next()` выполнение возобновляется с точки после последнего `YIELD_VALUE`.

4. **Специализированные инструкции байткода**:

Для каждого типа comprehension есть своя инструкция добавления элемента:

- `LIST_APPEND` (0x69) для list comprehensions
- `SET_ADD` (0x67) для set comprehensions
- `MAP_ADD` (0x68) для dict comprehensions

Эти инструкции работают напрямую с внутренними структурами данных (`PyListObject`, `PySetObject`, `PyDictObject`), минуя
вызовы методов Python.

5. **Оптимизации CPython**:

- **Предварительное выделение памяти**: Если итерируемый объект имеет метод `__len__`, CPython использует его для
  предварительного выделения памяти под список, уменьшая количество перераспределений.
- **Инлайн-кэширование**: В Python 3.11+ добавлен адаптивный байткод, который кэширует частые операции внутри
  comprehensions.
- **Оптимизация стека**: Внутренние переменные comprehension хранятся в массиве `f_localsplus` фрейма, а не в словаре,
  что ускоряет доступ.

6. **Области видимости и cell variables**:

Если comprehension использует переменные из внешней области видимости, они захватываются через cell objects. Например, в
`[x*y for x in range(3)]`, если `y` — внешняя переменная, она загружается инструкцией `LOAD_DEREF` из cell object.

7. **Производительность: comprehension vs цикл**:

List comprehension быстрее цикла с `append()` по трём причинам:

1. Операция добавления выполняется нативным кодом в `LIST_APPEND`
2. Нет накладных расходов на поиск и вызов метода `append`
3. Весь цикл выполняется в одной области видимости без переключения контекста

8. **Генераторные выражения и память**:

Генераторные выражения создают объект типа `PyGenObject`, который содержит фрейм выполнения. Этот фрейм сохраняет
состояние между вызовами `next()`. Память освобождается только после завершения итерации или явного вызова `close()`.

9. **Вложенные comprehensions**:

Вложенное comprehension `[[i*j for j in range(3)] for i in range(4)]` компилируется в две внутренние функции. Внешняя
функция создаёт внешний список, внутренняя — внутренние списки. Это может создавать дополнительные накладные расходы
из-за создания множества временных объектов.

10. **Словарные comprehension**:

Dict comprehension `{x: x**2 for x in range(5)}` использует инструкцию `MAP_ADD`, которая добавляет пару ключ-значение
напрямую в хеш-таблицу словаря. Это быстрее, чем вызов `dict.__setitem__`.

11. **Особенности множественных comprehension**:

Set comprehension использует `SET_ADD`, который проверяет уникальность элемента через хеш-таблицу множества. При
дублировании элементов выполняется лишняя работа, но результат остаётся корректным.

12. **Потенциальные проблемы**:

- **Утечка памяти в генераторах**: Если генераторное выражение не итерируется до конца (например, из-за `break`), его
  фрейм может остаться в памяти до сборки мусора.
- **Неправильный порядок вложенных циклов**: В comprehension вида `[x+y for x in A for y in B]` сначала фиксируется `x`,
  затем итерируется `y`. Это противоположно вложенным циклам `for x in A: for y in B:`.
- **Оценка условий**: Условия `if` оцениваются для каждого элемента, что может быть дорого, если условие сложное.

13. **Тестирование для AQA**:

При тестировании кода с comprehensions и генераторными выражениями важно:

- Проверять корректность выходных данных для всех типов входных данных (пустые, большие, с дубликатами)
- Измерять потребление памяти при использовании генераторных выражений
- Тестировать производительность на больших наборах данных
- Проверять обработку исключений внутри выражений
- Убеждаться, что генераторные выражения не используются повторно (они одноразовые)
- Проверять корректность работы с замыканиями

- [Содержание](#содержание)

---

# *copy() и deepcopy()*

copy() создает поверхностную копию объекта, копируя сам контейнер, но не вложенные объекты, в то время как deepcopy()
рекурсивно создает полную копию всей структуры. Поверхностная копия подходит, когда вложенные объекты неизменяемы или
нет необходимости их копировать, тогда как глубокая копия необходима для полной изоляции, особенно для конфигураций или
тестовых данных.

## **Junior Level**

`copy()` и `deepcopy()` — это функции из модуля `copy`, которые создают копии объектов Python.

`copy()` делает поверхностную (shallow) копию объекта — создает новый контейнер, но элементы внутри остаются теми же
самыми объектами. Для списка `copy()` создаст новый список, но элементы этого списка будут ссылаться на те же объекты,
что и в оригинале.

`deepcopy()` делает глубокую (deep) копию — рекурсивно копирует все объекты внутри контейнера, создавая полностью
независимую копию всей иерархии объектов. Это гарантирует, что изменения в копии не затронут оригинал, и наоборот.

Пример: если у вас есть список списков, `copy()` скопирует только внешний список, а внутренние списки останутся общими.
`deepcopy()` скопирует и внешний список, и все внутренние списки.

## **Middle Level**

1. **Поверхностное копирование (`copy()`)**:

- Создает новый объект того же типа
- Копирует ссылки на вложенные объекты, а не сами объекты
- Для изменяемых объектов (списков, словарей, множеств) изменение вложенных объектов в копии отразится на оригинале
- Для неизменяемых объектов (числа, строки, кортежи) разницы между поверхностной и глубокой копией нет
- Использует метод `__copy__()` объекта, если он определен

2. **Глубокое копирование (`deepcopy()`)**:

- Рекурсивно обходит всю структуру объекта
- Создает новые экземпляры для всех вложенных объектов
- Обрабатывает циклические ссылки через словарь `memo` для избежания бесконечной рекурсии
- Использует метод `__deepcopy__()` объекта, если он определен
- Может быть очень медленным для больших структур

3. **Когда использовать**:

- `copy()`: когда объекты неизменяемы или вы уверены, что не будете изменять вложенные объекты
- `deepcopy()`: когда нужна полная изоляция, особенно для конфигураций, тестовых данных, фикстур

4. **Особые случаи**:

- Файловые объекты, потоки, сокеты не могут быть скопированы нормально
- Некоторые объекты (модули, классы, функции) возвращаются как есть
- Дескрипторы и свойства требуют специальной обработки

## **Senior Level (CPython, память и рекурсивные алгоритмы)**

1. **Реализация в CPython**:

Модуль `copy` реализован на Python (`Lib/copy.py`), но использует низкоуровневые механизмы CPython.

```python
# Упрощенная структура
def copy(x):
    cls = type(x)
    copier = _copy_dispatch.get(cls)
    if copier:
        return copier(x)
    # Попытка использовать __copy__ метод
    # или создание через конструктор
```

`_copy_dispatch` — это словарь, который сопоставляет типы с функциями-копировщиками.

2. **Алгоритм `deepcopy()`**:

Основная функция `_deepcopy_atomic()`:

```python
def _deepcopy_atomic(x, memo):
    # Атомарные объекты (неизменяемые) возвращаются как есть
    return x


def _deepcopy_list(x, memo, recursive=0):
    id_x = id(x)
    if id_x in memo:
        return memo[id_x]

    memo[id_x] = y = []
    for item in x:
        y.append(deepcopy(item, memo))
    return y
```

Ключевые элементы:

- `memo` словарь: `{id(original): copy}` для обработки циклических ссылок
- Рекурсивный обход через `deepcopy(item, memo)`
- Специальные обработчики для разных типов в `_deepcopy_dispatch`

3. **Обработка циклических ссылок**:

Для структуры с циклической ссылкой:

```python
a = []
b = [a]
a.append(b)
```

`deepcopy()` работает так:

1. Видит список `a`, создает пустую копию `a'`, сохраняет `{id(a): a'}` в `memo`
2. Начинает копировать элементы `a` — видит `b`
3. Создает пустую копию `b'`, сохраняет `{id(b): b'}` в `memo`
4. Начинает копировать элементы `b` — видит ссылку на `a`
5. Находит `a'` в `memo`, вставляет ссылку на `a'` в `b'`
6. Возвращается к `a'`, вставляет `b'` в него

4. **Байткод и производительность**:

Каждый вызов `deepcopy()`:

- Создает новый фрейм для рекурсивных вызовов
- Выполняет множество проверок типа
- Использует `id()` для каждого объекта (хэш от адреса памяти)
- Для больших структур может вызвать `RecursionError`

Оптимизация: `deepcopy()` использует `_deepcopy_dispatch` для быстрого вызова специализированных функций.

5. **Специальные методы `__copy__()` и `__deepcopy__()`**:

Классы могут определить эти методы для кастомного поведения:

```python
class MyClass:
    def __init__(self, data):
        self.data = data
        self._cache = None

    def __deepcopy__(self, memo):
        # Копируем только data, cache не копируем
        new_obj = self.__class__(deepcopy(self.data, memo))
        memo[id(self)] = new_obj
        return new_obj
```

`__deepcopy__()` получает `memo` словарь и должен использовать его для рекурсивных вызовов.

6. **Проблемы с объектами CPython**:

Некоторые объекты не могут быть скопированы:

- Модули: возвращаются как есть
- Классы: копируются только ссылки
- Файловые дескрипторы: могут быть "скопированы", но это опасно
- Сокеты, потоки: обычно вызывают исключение
- Weak references: требуют специальной обработки

7. **Производительность `copy()`**:

Для встроенных типов `copy()` часто реализована на C:

- `list.copy()`: `PyList_Copy()` в `Objects/listobject.c`
- `dict.copy()`: `PyDict_Copy()` в `Objects/dictobject.c`
- `set.copy()`: `PySet_Copy()` в `Objects/setobject.c`

Эти функции работают за O(n) времени и создают новые структуры данных.

8. **Memoryview и копирование**:

Для объектов с буферами памяти (memoryview, array.array, numpy arrays):

- `copy()` создает новый объект с тем же буфером (shallow)
- `deepcopy()` обычно делает то же самое, что и `copy()`
- Для настоящего копирования данных нужны специальные методы (`copy()`, `copy.deepcopy()` с кастомной логикой)

9. **Копирование дескрипторов и property**:

При копировании класса или экземпляра с дескрипторами:

- Дескрипторы не копируются (они принадлежат классу)
- Property объекты копируются как есть
- Важно: копирование не создает новые функции-геттеры/сеттеры

10. **Тестирование и отладка**:

Для AQA важно тестировать копирование:

- Проверять идентичность вложенных объектов после `copy()`
- Проверять независимость после `deepcopy()`
- Тестировать циклические ссылки
- Проверять кастомные `__copy__`/`__deepcopy__` методы
- Измерять производительность для больших структур

11. **Альтернативные методы копирования**:

- `pickle.loads(pickle.dumps(obj))`: создает глубокую копию через сериализацию
- `json.loads(json.dumps(obj))`: для JSON-сериализуемых объектов
- Специализированные методы: `list()`, `dict()`, `set()` для простых случаев

12. **Копирование в многопоточной среде**:

При копировании объектов, к которым обращаются несколько потоков:

- `copy()` может захватить неконсистентное состояние
- `deepcopy()` рекурсивно захватывает объекты, что может привести к deadlock
- Нужно использовать блокировки или immutable структуры

13. **Сборка мусора и копирование**:

При копировании больших объектов:

- `copy()` увеличивает счетчики ссылок у вложенных объектов
- `deepcopy()` создает полностью новые объекты, увеличивая нагрузку на GC
- `memo` словарь в `deepcopy()` предотвращает дублирование, но может удерживать ссылки

- [Содержание](#содержание)

---

# **Асинхронность**

Асинхронность в Python — это модель программирования, которая позволяет выполнять множество операций ввода-вывода (I/O)
в одном потоке без блокировок, используя ключевые слова `async` и `await`. Она основана на корутинах, которые
приостанавливают и возобновляют выполнение, и цикле событий (event loop), который управляет их выполнением. Это особенно
эффективно для сетевых запросов, работы с базами данных и других операций, где важно избегать простоев в ожидании
ответа.

## **Junior Level**

Асинхронность в Python — это модель программирования, позволяющая выполнять множество операций без блокировки потока
выполнения. Ключевые слова `async` и `await` используются для определения и работы с асинхронным кодом.

`async def` определяет асинхронную функцию (корутину), которая может приостанавливать своё выполнение, не блокируя
другие операции. `await` используется для ожидания результата другой асинхронной операции, освобождая управление, пока
эта операция выполняется. Это особенно полезно для операций ввода-вывода (I/O), таких как сетевые запросы или чтение
файлов, где программа может тратить время в ожидании ответа.

Для запуска асинхронного кода нужен цикл событий (event loop), который управляет выполнением корутин. Модуль `asyncio`
предоставляет инфраструктуру для написания асинхронного кода, включая планирование задач и синхронизацию.

## **Middle Level**

1. **Корутины (coroutines)**: Функции, определённые с `async def`. При вызове они возвращают объект корутины, который
   необходимо запустить в цикле событий. Корутины могут быть приостановлены (`await`) и возобновлены.

2. **Event Loop**: Центральный механизм, управляющий выполнением асинхронных задач. Он запускает корутины, обрабатывает
   системные события (например, готовность сокета), и переключается между задачами при их приостановке.

3. **Задачи (Tasks)**: Обёртки вокруг корутин, которые планируются в цикле событий. `asyncio.create_task()` запускает
   корутину как конкурентную задачу.

4. **Awaitable объекты**: Любой объект, который можно использовать с `await`. Это включает корутины, задачи и объекты
   `Future` (представляющие результат асинхронной операции).

5. **Принципы работы**: При встрече `await` корутина приостанавливается, управление возвращается в цикл событий, который
   запускает другие задачи. Когда ожидаемая операция завершается, цикл событий возобновляет корутину.

6. **Синхронизация**: `asyncio` предоставляет примитивы для синхронизации задач: `Lock`, `Semaphore`, `Event`,
   `Condition`.

7. **Асинхронные контекстные менеджеры и итераторы**: `async with` и `async for` для работы с асинхронными ресурсами и
   итерируемыми объектами.

## **Senior Level**

**1. Реализация корутин в CPython**

Корутины строятся на генераторах. Асинхронная функция компилируется с флагом `CO_COROUTINE` (0x80) в `code.co_flags`.
Объект корутины имеет тип `PyCoroObject` (наследуется от `PyGenObject`):

```c
typedef struct {
    PyGenObject gen;
    PyObject *cr_origin;  // Для отладки
} PyCoroObject;
```

При вызове `async def` функции:

- Байткод `MAKE_FUNCTION` создаёт объект функции с флагом `CO_COROUTINE`
- Вызов функции возвращает объект корутины (не выполняя код)
- Для запуска корутины вызывается `coro.send(None)` или `await`

**2. Механизм `await` на уровне байткода**

Байткод для `await` в Python 3.9+ использует инструкции `GET_AWAITABLE` и `YIELD_FROM`:

```
async def foo():
    await bar()

# Байткод foo():
0 LOAD_GLOBAL              0 (bar)
2 CALL_FUNCTION            0
4 GET_AWAITABLE
6 LOAD_CONST               0 (None)
8 YIELD_FROM
10 POP_TOP
12 LOAD_CONST               0 (None)
14 RETURN_VALUE
```

`GET_AWAITABLE` проверяет, является ли объект awaitable (имеет метод `__await__`). `YIELD_FROM` приостанавливает
корутину и передаёт управление.

**3. Цикл событий (Event Loop)**

Реализация цикла событий в `asyncio` использует селекторы (select, epoll, kqueue) для мониторинга файловых дескрипторов.
Основные компоненты:

- **Ready Queue**: Очередь готовых к выполнению задач (корутин)
- **Scheduled Queue**: Очередь отложенных задач (через `asyncio.sleep`)
- **Selector**: Мониторит сокеты и файловые дескрипторы

Работа цикла:

```python
while True:
    # 1. Выполнить готовые задачи
    while ready_queue:
        task = ready_queue.popleft()
        task._step()  # Продвинуть выполнение

    # 2. Ожидать событий ввода-вывода
    timeout = calculate_timeout()
    events = selector.select(timeout)
    for fd, event in events:
        callback = fd_to_callback[fd]
        callback()

    # 3. Проверить отложенные задачи
    current_time = time()
    while scheduled_queue and scheduled_queue[0].when <= current_time:
        task = scheduled_queue.pop(0)
        ready_queue.append(task)
```

**4. Задачи (Tasks) и Future**

`Task` наследуется от `Future`. `Future` представляет результат асинхронной операции. Структура `PyTaskObject` включает:

- `_coro`: ссылка на корутину
- `_state`: состояние (PENDING, CANCELLED, FINISHED)
- `_result`: результат или исключение
- `_callbacks`: список колбэков при завершении

Когда задача завершается, она устанавливает результат во `Future` и запускает прикреплённые колбэки.

**5. Асинхронные генераторы**

Асинхронные генераторы (`async def` с `yield`) используют отдельный тип `PyAsyncGenObject`. Они поддерживают:

- `__anext__()` для асинхронной итерации
- `asend()`, `athrow()`, `aclose()` аналогично обычным генераторам
- Финализацию через `async_gen_finalizer()`

**6. Протокол `__await__`**

Любой объект может стать awaitable, реализовав `__await__()`, который должен возвращать итератор. Цикл событий будет
итерировать по нему до завершения.

**7. Отладка и интроспекция**

- `asyncio.current_task()`: текущая задача
- `asyncio.all_tasks()`: все активные задачи
- `task.get_stack()`: стек вызовов корутины
- `asyncio.get_event_loop_policy()`: политика цикла событий

**8. Производительность и оптимизации**

- **Быстрый путь для локального event loop**: `asyncio.get_running_loop()` кэширует ссылку на текущий цикл
- **Инлайн-кэширование в Python 3.11**: байткод `ASYNC_GEN_WRAP` оптимизирует асинхронные генераторы
- **Протокол буферизации**: асинхронные итераторы могут реализовать `__aiter__`, возвращающий асинхронный итератор, и
  `__anext__` для получения элементов

**9. GIL и асинхронность**

Асинхронный код выполняется в одном потоке, поэтому GIL не препятствует параллелизму. Однако:

- Блокирующие вызовы блокируют весь цикл событий
- Для CPU-интенсивных задач используется `run_in_executor()` (пул потоков/процессов)
- Освобождение GIL в `await` происходит автоматически при вызове системных функций

**10. Системные вызовы и обратные вызовы**

При асинхронном I/O:

- Системный вызов (например, `socket.recv`) регистрируется в селекторе
- Цикл событий добавляет колбэк, который будет вызван при готовности данных
- Колбэк возобновляет корутину через `task.set_result()`

**11. Обработка исключений**

Исключения в корутинах пробрасываются через механизм `throw()`:

- Если корутина не обрабатывает исключение, оно устанавливается в задачу
- `await` пробрасывает исключение вызывающей корутине
- `asyncio.gather()` собирает исключения из нескольких задач

**12. Тестирование асинхронного кода**

Для AQA критически важно:

- **Изоляция тестов**: Каждый тест должен запускаться в новом цикле событий
- **Мокирование**: Подмена асинхронных функций через `unittest.mock.AsyncMock`
- **Таймауты**: Контроль времени выполнения тестов
- **Обработка исключений**: Проверка асинхронных исключений через `pytest.raises()`
- **Нагрузочное тестирование**: Проверка поведения при множестве одновременных корутин

**13. Расширенные паттерны**

- **Шардинг циклов событий**: Несколько циклов в разных потоках
- **Custom Event Loop**: Реализация своего цикла событий для специализированных нужд
- **Протоколы транспорта**: Низкоуровневая работа с сокетами через `asyncio.Protocol`

- [Содержание](#содержание)

---

# Многопоточность

Многопоточность в Python позволяет выполнять несколько потоков в одном процессе, но из-за Global Interpreter Lock (GIL)
только один поток может выполнять байткод Python в любой момент времени. Это делает её эффективной для операций
ввода-вывода (I/O), где потоки освобождают GIL во время ожидания, но непригодной для параллелизации CPU-интенсивных
задач. Для работы с потоками используется модуль `threading`, предоставляющий примитивы синхронизации, такие как Lock и
Semaphore.

## **Junior Level**

Многопоточность в Python — это возможность выполнять несколько потоков (threads) в рамках одного процесса. Каждый поток
представляет собой последовательность инструкций, которая может выполняться параллельно с другими потоками. Это похоже
на несколько сотрудников, работающих над разными задачами в одном офисе, где они могут использовать общие ресурсы (
память, файлы).

Основная цель многопоточности — улучшить отзывчивость приложений и эффективно использовать время простоя, например, при
ожидании ввода-вывода (I/O). Однако в Python из-за Global Interpreter Lock (GIL) потоки не выполняются по-настоящему
параллельно при работе с Python-кодом — в один момент времени только один поток выполняет байткод Python.

Для работы с потоками используется модуль `threading`. Потоки создаются через класс `Thread`, а для синхронизации
используются примитивы вроде `Lock`, `RLock`, `Semaphore`, `Event`, `Condition`.

## **Middle Level**

1. **GIL (Global Interpreter Lock)**: Это мьютекс, который защищает доступ к объектам Python, предотвращая одновременное
   выполнение байткода Python несколькими потоками. GIL необходим потому, что менеджер памяти CPython не является
   потокобезопасным. Это означает, что для CPU-интенсивных задач многопоточность не даёт прироста производительности, но
   для I/O-операций (сеть, дисковые операции) она эффективна, так как во время ожидания I/O GIL может быть отпущен.

2. **Состояния потока**: Поток может находиться в состояниях: создан (new), готов (runnable), запущен (running),
   заблокирован (blocked) и завершён (terminated). Переходы между состояниями управляются планировщиком операционной
   системы.

3. **Примитивы синхронизации**:
    - **Lock**: базовый мьютекс для взаимного исключения
    - **RLock**: реентерабельный замок, который может быть захвачен несколько раз одним потоком
    - **Semaphore**: счётчик для ограничения доступа к ресурсу
    - **Event**: флаг для уведомления между потоками
    - **Condition**: более сложный механизм для ожидания условий

4. **Демонические потоки**: Потоки, помеченные как `daemon`, завершаются вместе с основным потоком. Обычные (
   не-демонические) потоки продолжают выполнение до завершения даже после завершения основного потока.

5. **Локальное хранилище потока**: `threading.local()` позволяет создавать переменные, уникальные для каждого потока.

6. **Пул потоков**: `concurrent.futures.ThreadPoolExecutor` предоставляет высокоуровневый интерфейс для пула потоков,
   упрощая параллельное выполнение задач.

7. **Взаимодействие с GIL**: При выполнении I/O-операций или вызове функций, отпускающих GIL (например, некоторые
   функции из `numpy` или `zlib`), другие потоки могут получить доступ к интерпретатору.

## **Senior Level**

**1. Реализация потоков в CPython**

Потоки в CPython реализованы через нативные потоки операционной системы (pthreads в Unix/Linux, Win32 threads в
Windows). Структура `PyThreadState` представляет состояние интерпретатора для каждого потока:

```c
typedef struct _ts {
    struct _ts *prev;
    struct _ts *next;
    PyInterpreterState *interp;
    
    PyFrameObject *frame;  // текущий фрейм выполнения
    int recursion_depth;
    long thread_id;  // идентификатор потока ОС
    
    // Стек исключений
    PyObject *exc_type;
    PyObject *exc_value;
    PyObject *exc_traceback;
    
    // Словарь для threading.local()
    PyObject *dict;
    
    // Счётчик ссылок на GIL
    int gilstate_counter;
} PyThreadState;
```

**2. GIL: детальная реализация**

GIL реализован через глобальную переменную `_PyRuntimeState.gil` и мьютекс + condition variable для синхронизации. В
Python 3.9+ используется усовершенствованный алгоритм с таймаутами:

```c
typedef struct {
    PyThread_type_lock lock;
    unsigned long locked;
    PyCOND_T cond;
    PyThreadState *last_holder;
    unsigned long switch_number;
} _PyGilState;
```

**3. Механизм переключения GIL**

Переключение GIL происходит по:

- **Числу выполненных инструкций**: Счётчик `_PyRuntimeState.gilstate.ticks` уменьшается при каждой инструкции. Когда он
  достигает 0, текущий поток отпускает GIL.
- **Таймауту ввода-вывода**: При системных вызовах, блокирующих поток, GIL отпускается.
- **Принудительному переключению**: Через `PyEval_ReleaseThread()`.

Алгоритм переключения (упрощённо):

1. Текущий поток устанавливает флаг `gil_drop_request`
2. Другие потоки опрашивают этот флаг
3. Когда поток замечает флаг, он пытается захватить GIL
4. После успешного захвата он уведомляет предыдущий поток

**4. Байткод и GIL**

При выполнении байткода интерпретатор периодически проверяет необходимость отпустить GIL. В байткоде нет явных
инструкций для GIL, но функция `_PyEval_EvalFrameDefault()` содержит проверку:

```c
if (_Py_atomic_load_relaxed(&_PyRuntimeState.gilstate.ticks) <= 0) {
    _PyEval_ReleaseThread(tstate);
}
```

**5. Deadlock detection и отладка**

В CPython нет автоматического обнаружения deadlock-ов, но есть механизмы отладки:

- `sys.setswitchinterval()`: установка интервала переключения GIL
- `threading.get_ident()`: получение идентификатора потока ОС
- `threading.enumerate()`: список всех активных потоков
- `faulthandler`: для дампов стека при deadlock

**6. Memory management и потоки**

Менеджер памяти CPython (pymalloc) имеет отдельные пулы для каждого потока (arena), чтобы минимизировать конфликты.
Каждый поток имеет свой кэш свободных блоков памяти, что уменьшает необходимость в глобальной синхронизации при
выделении памяти.

**7. Состояние интерпретатора и суб-интерпретаторы**

`PyInterpreterState` содержит глобальное состояние для группы потоков. В Python 3.12+ усиливается поддержка
суб-интерпретаторов, где каждый имеет свой собственный GIL, позволяя настоящий параллелизм.

**8. Взаимодействие с async/await и asyncio**

При использовании asyncio в многопоточном контексте:

- Цикл событий работает в основном потоке
- `asyncio.run_coroutine_threadsafe()` позволяет запускать корутины из других потоков
- Для передачи данных между потоками и циклом событий используется `concurrent.futures.Future`

**9. Производительность и оптимизации**

- **Явное освобождение GIL**: Расширения C могут отпускать GIL через макросы `Py_BEGIN_ALLOW_THREADS` /
  `Py_END_ALLOW_THREADS`
- **Атомарные операции**: `_Py_atomic_*` функции для атомарного доступа к переменным
- **Thread-local storage (TLS)**: Используется `pthread_key_t` для хранения `PyThreadState*`

**10. Планировщик потоков ОС vs Python**

CPython делегирует планирование потоков операционной системе. Однако GIL добавляет свой уровень планирования: даже если
ОС переключила контекст на другой поток Python, он не сможет выполнять байткод, пока не получит GIL.

**11. Обработка сигналов и потоки**

Сигналы Unix обрабатываются только в главном потоке. Вспомогательные потоки блокируют сигналы через `pthread_sigmask()`.
Это предотвращает прерывание выполнения байткода в случайный момент.

**12. Для AQA: тестирование многопоточного кода**

Критически важные аспекты тестирования:

- **Детерминизм**: Многопоточные тесты должны быть воспроизводимы. Использование `random.seed()` и фиксированных
  интервалов.
- **Гонки данных (data races)**: Инструменты вроде `threading.gettrace()` для отслеживания переключений потоков.
- **Deadlock detection**: Принудительное создание дампов стека через сигналы или таймауты.
- **Нагрузочное тестирование**: Создание сценариев с большим количеством потоков и проверка утечек ресурсов.
- **Изоляция тестов**: Каждый тест должен запускаться в свежем процессе, чтобы избежать влияния глобального состояния.
- **Мокирование**: Подмена `threading.Thread` или примитивов синхронизации для контроля выполнения.
- **Проверка отложенных состояний (flaky tests)**: Автоматический повтор тестов с разными интервалами переключения GIL
  через `sys.setswitchinterval()`.

**13. Будущие изменения (Python 3.12+)**

- **Без-GIL режим**: Экспериментальная возможность запуска интерпретатора без GIL (флаг `--disable-gil`)
- **Per-interpreter GIL**: Разные GIL для разных суб-интерпретаторов
- **Улучшенная поддержка атомарных операций**: Новые API для безопасной работы с разделяемыми данными

- [Содержание](#содержание)

---

# Мультипроцессинг

Мультипроцессинг в Python использует несколько процессов, каждый со своим интерпретатором и памятью, что позволяет
обойти ограничения GIL и использовать все ядра процессора для CPU-интенсивных задач. Процессы изолированы, поэтому обмен
данными требует межпроцессного взаимодействия (IPC), но обеспечивает настоящий параллелизм.

## **Junior Level**

Мультипроцессинг в Python — это подход к параллельным вычислениям, использующий несколько процессов вместо потоков.
Каждый процесс работает в своём собственном пространстве памяти, имеет отдельный экземпляр интерпретатора Python и
собственный GIL. Это позволяет обойти ограничения GIL и использовать все доступные ядра процессора для выполнения
CPU-интенсивных задач.

Процессы полностью изолированы друг от друга, что обеспечивает лучшую стабильность (падение одного процесса не
затрагивает другие), но усложняет обмен данными. Для создания и управления процессами используется модуль
`multiprocessing`, который предоставляет высокоуровневый API, похожий на `threading`, но для процессов.

## **Middle Level**

1. **Архитектура процессов**: Каждый процесс — это отдельный экземпляр Python-интерпретатора с собственным адресным
   пространством, кучей (heap) и стеком. Процессы создаются через системные вызовы `fork()` (Unix) или
   `CreateProcess()` (Windows).

2. **Межпроцессное взаимодействие (IPC)**:
    - **Очереди (Queue)**: Потокобезопасные FIFO-очереди, реализованные через каналы (pipes) и блокировки.
    - **Каналы (Pipe)**: Двунаправленные каналы связи между двумя процессами.
    - **Разделяемая память (Shared Memory)**: `multiprocessing.Value` и `multiprocessing.Array` для создания переменных
      в общей памяти.
    - **Менеджеры (Managers)**: Высокоуровневый API для создания разделяемых объектов (словари, списки) через
      прокси-объекты.

3. **Способы создания процессов**:
    - **Fork** (Unix): Быстрое копирование памяти родительского процесса. Проблема: копируются все дескрипторы файлов и
      блокировки.
    - **Spawn** (по умолчанию с Python 3.8+): Запуск нового интерпретатора Python с импортом модуля и выполнением
      целевой функции. Безопаснее, но медленнее.
    - **Forkserver**: Компромиссный вариант — серверный процесс, от которого форкаются все остальные.

4. **Синхронизация**: Те же примитивы, что и в threading (Lock, Semaphore, Event, Condition), но работающие через общую
   память или семафоры ОС.

5. **Пулы процессов (Process Pool)**: `multiprocessing.Pool` управляет фиксированным числом рабочих процессов,
   предоставляет методы `map()`, `apply()`, `apply_async()` для распределения задач.

6. **Наследование данных**: При создании процесса через fork дочерний процесс получает копию всей памяти родителя. При
   spawn — только необходимые для выполнения функции данные.

7. **Производительность**: Процессы эффективны для CPU-задач благодаря отсутствию GIL, но имеют высокие накладные
   расходы на создание и IPC.

## **Senior Level**

**1. Реализация процессов в CPython**

Модуль `multiprocessing` написан на Python и C, использует низкоуровневые API ОС. На Unix системах используется
`os.fork()`:

```c
// Упрощённая реализация fork в CPython
pid_t fork(void) {
    pid_t pid = fork_syscall();  // Системный вызов fork()
    if (pid == 0) {
        // Дочерний процесс
        _PyRuntimeState_Reinitialize();  // Переинициализация runtime
        reset_all_locks();  // Сброс всех Python-блокировок
    }
    return pid;
}
```

**2. Структура Process объекта**

`Process` объект в CPython содержит:

- `_popen`: Объект `Popen`, управляющий дочерним процессом
- `_target`: Вызываемая функция
- `_args`, `_kwargs`: Аргументы функции
- `_pid`: Идентификатор процесса
- `_sentinel`: Файловый дескриптор для отслеживания завершения

**3. Межпроцессное взаимодействие на низком уровне**

**Очереди (Queue)**:

- Используют `Pipe` (неименованные каналы) для передачи данных
- Данные сериализуются через `pickle` с оптимизациями для больших объектов
- Синхронизация через семафоры POSIX или именованные семафоры System V

**Разделяемая память**:

- На Unix: `mmap()` с флагом `MAP_SHARED`
- На Windows: `CreateFileMapping()` + `MapViewOfFile()`
- В Python: `multiprocessing.shared_memory.SharedMemory` (Python 3.8+)

```c
// Создание разделяемой памяти в Unix
void* create_shared_memory(size_t size) {
    int fd = shm_open("/python_shm_XXXXXX", O_CREAT | O_RDWR, 0666);
    ftruncate(fd, size);
    return mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
}
```

**4. Сериализация и десериализация**

Все данные для IPC проходят через `pickle`. Важные оптимизации:

- **Протокол 5 (Python 3.8+)**: Поддержка out-of-band данных для больших объектов
- **Reduce/Reconstruct**: Кастомные классы могут реализовать `__reduce__` для контроля сериализации
- **Копирование при записи (Copy-on-Write)**: При fork память не копируется физически до модификации

**5. Пул процессов: внутренняя работа**

`Pool` использует паттерн "мастер-воркер":

1. Мастер создает N рабочих процессов
2. Создаются две очереди: task_queue и result_queue
3. Мастер отправляет задачи через `SimpleQueue` (pipe + lock)
4. Воркеры выполняют задачи и возвращают результаты
5. Для load balancing используется предварительное извлечение задач

**6. Синхронизация на уровне ядра**

Примитивы синхронизации реализованы через:

- **Семафоры POSIX**: `sem_open()`, `sem_wait()`, `sem_post()`
- **Мьютексы в разделяемой памяти**: Для `Lock` и `RLock`
- **Condition variables**: Через мьютексы и condition variables POSIX

**7. Проблема fork и GIL**

При использовании `fork()` в многопоточном приложении:

- Дочерний процесс получает копию только вызывающего потока
- Другие потоки "исчезают" в дочернем процессе
- Все блокировки (включая GIL) остаются в том же состоянии, что может вызвать deadlock

Решение: `multiprocessing` использует `pthread_atfork()` для регистрации обработчиков, которые сбрасывают GIL и другие
блокировки.

**8. Управление ресурсами процессов**

- **Дескрипторы файлов**: При fork все открытые файлы копируются. `multiprocessing` закрывает ненужные дескрипторы.
- **Сигналы**: Установка обработчиков для SIGINT, SIGTERM, SIGCHLD
- **Зомби-процессы**: Использование `waitpid()` с флагом WNOHANG для предотвращения

**9. Производительность и оптимизации**

- **Pickle протокол 5 с out-of-band**: Позволяет передавать большие данные (массивы numpy) без копирования
- **Наследование дескрипторов**: Передача открытых файлов или сокетов через fork
- **Предзагрузка (preloading)**: Импорт модулей до fork для экономии памяти

**10. Для AQA: тестирование многопроцессного кода**

Критически важные аспекты:

- **Детерминированность**: Использование фиксированного seed для random, контроль порядка выполнения
- **Изоляция тестов**: Каждый тест должен запускаться в отдельном процессе или использовать чистые фикстуры
- **Проверка утечек ресурсов**:
    - Отслеживание числа процессов через `psutil`
    - Проверка file descriptor leak через `os.sysconf('SC_OPEN_MAX')`
    - Мониторинг памяти через `resource.getrusage()`
- **Тестирование граничных условий**:
    - Обработка SIGKILL/SIGTERM
    - Переполнение очередей
    - Deadlock в IPC
- **Производительность**:
    - Замеры скорости IPC для разных размеров данных
    - Scalability testing: увеличение числа процессов
    - Нагрузочное тестирование с ограничением памяти
- **Интеграционное тестирование**:
    - Воспроизведение production-окружения
    - Тестирование с разными методами запуска (fork, spawn, forkserver)
    - Проверка восстановления после сбоев
- **Mocking и stubbing**:
    - Подмена `multiprocessing.Process` для unit-тестов
    - Заглушки для системных вызовов через `unittest.mock`
- **Отладка сложных сценариев**:
    - Использование `faulthandler` для дампов при deadlock
    - Логирование с идентификаторами процессов
    - Визуализация взаимодействия процессов

**11. Интеграция с asyncio**

Python 3.8+ предоставляет `asyncio.run_in_executor()` с `ProcessPoolExecutor`:

- Асинхронный запуск CPU-интенсивных задач
- Использование `loop.subprocess_exec()` для запуска внешних процессов
- Обработка результатов через callbacks без блокировки event loop

**12. Альтернативные реализации**

- **`concurrent.futures.ProcessPoolExecutor`**: Более высокоуровневый API
- **`joblib`**: Специализированная библиотека для численных вычислений
- **`ray`**: Распределенные вычисления с продвинутым IPC
- **`dask`**: Параллельные вычисления с отложенным выполнением

**13. Безопасность**

- **Валидация входных данных**: Pickle уязвим для атак, необходимо проверять данные
- **Изоляция процессов**: Использование chroot, namespaces, cgroups
- **Очистка разделяемой памяти**: Гарантированное удаление shm объектов

**14. Отладка и профилирование**

- **`multiprocessing.get_logger()`**: Встроенное логирование
- **`cProfile` + `pstats`**: Профилирование каждого процесса отдельно
- **`tracemalloc`**: Отслеживание утечек памяти между процессами
- **Системные утилиты**: `strace`, `perf`, `valgrind` для низкоуровневой отладки

- [Содержание](#содержание)

---

# **Dataclass**

Dataclass — это декоратор из модуля `dataclasses`, который автоматически генерирует специальные методы (такие как
`__init__`, `__repr__` и `__eq__`) для классов, предназначенных в основном для хранения данных. Это позволяет избежать
написания шаблонного кода, делает классы чище и менее подверженными ошибкам. Dataclass поддерживает гибкую настройку
через параметры декоратора и функцию `field()`, включая создание неизменяемых (frozen) классов и оптимизацию памяти с
помощью `slots=True`.

## **Junior Level**

Dataclass — это декоратор из модуля `dataclasses`, который автоматически добавляет в класс специальные методы на основе
объявленных полей. Он предназначен для создания классов, которые в основном хранят данные, избавляя разработчика от
написания шаблонного кода.

Вместо того чтобы вручную писать `__init__`, `__repr__`, `__eq__` и другие методы для классов-контейнеров данных, можно
использовать `@dataclass`, и Python сам сгенерирует эти методы. Это делает код чище, уменьшает вероятность ошибок и
упрощает поддержку.

Например, класс для представления точки с координатами x и y можно определить одной строкой:
`@dataclass class Point: x: int; y: int`. Автоматически появятся конструктор, строковое представление и сравнение по
значениям полей.

## **Middle Level**

1. **Автоматически генерируемые методы**:
    - `__init__()`: Инициализатор с параметрами для всех полей
    - `__repr__()`: Читаемое строковое представление
    - `__eq__()`: Сравнение по значениям всех полей
    - `__ne__()`: Автоматически на основе `__eq__`
    - Опционально (через параметры декоратора):
        - `__lt__()`, `__le__()`, `__gt__()`, `__ge__()`: при `order=True`
        - `__hash__()`: при `frozen=True` или явном указании

2. **Параметры декоратора `@dataclass`**:
    - `init=True`: Генерация `__init__`
    - `repr=True`: Генерация `__repr__`
    - `eq=True`: Генерация `__eq__`
    - `order=False`: Генерация методов сравнения
    - `frozen=False`: Запрет изменения полей после создания
    - `unsafe_hash=False`: Принудительная генерация `__hash__`
    - `match_args=True`: Поддержка паттерн-матчинга (Python 3.10+)

3. **Поле `field()`**: Функция для тонкой настройки полей:
    - `default`: Значение по умолчанию
    - `default_factory`: Фабрика для изменяемых значений по умолчанию
    - `init`: Включать ли поле в `__init__`
    - `repr`: Включать ли поле в `__repr__`
    - `compare`: Участвует ли поле в сравнении
    - `hash`: Участвует ли поле в вычислении хеша
    - `metadata`: Произвольные метаданные

4. **Наследование**: Dataclasses поддерживают наследование, поля собираются от всех родительских классов с учётом MRO.
   Важно правильно использовать значения по умолчанию при наследовании.

5. **Пост-инициализация**: Метод `__post_init__()` вызывается после автоматического `__init__` и может использоваться
   для дополнительной валидации или вычисления производных полей.

6. **Иммутабельные dataclasses**: При `frozen=True` экземпляры становятся неизменяемыми. Попытка изменить поле вызывает
   `FrozenInstanceError`.

## **Senior Level**

**1. Внутренняя реализация в CPython**

Модуль `dataclasses` написан на чистом Python, но использует низкоуровневые механизмы CPython:

```python
# Логика генерации методов
def _create_fn(name, args, body, *, globals=None, locals=None):
    # Создание объекта кода через compile()
    # Исполнение через exec() для создания функции
    pass
```

Декоратор `@dataclass` работает в несколько этапов:

1. Сканирование аннотаций класса
2. Сбор информации о полях через `Field` объекты
3. Генерация исходного кода методов как строк
4. Компиляция и привязка методов к классу

**2. Генерация `__init__`**

Для класса:

```python
@dataclass
class Point:
    x: int
    y: int = 0
```

Генерируется код, эквивалентный:

```python
def __init__(self, x: int, y: int = 0):
    self.x = x
    self.y = y
```

Но реальная реализация использует `object.__setattr__` для обхода возможных ограничений (например, для frozen классов
или классов со `__slots__`).

**3. Сбор полей и разрешение MRO**

Алгоритм сбора полей:

- Обход MRO от базовых классов к производным
- Сбор полей в порядке определения
- Разрешение конфликтов: поля из производных классов переопределяют поля из базовых
- Обработка полей без аннотаций (игнорируются в Python 3.10+)

**4. Оптимизация с `__slots__`**

Начиная с Python 3.10, можно использовать `slots=True`:

```python
@dataclass(slots=True)
class Point:
    x: int
    y: int
```

Это:

- Создаёт `__slots__` автоматически
- Уменьшает потребление памяти на ~40%
- Ускоряет доступ к атрибутам
- Но запрещает добавление новых атрибутов

Реализация: декоратор динамически создаёт класс с помощью `type()`.

**5. Хеширование и frozen-классы**

Для frozen dataclass:

- `__hash__` генерируется на основе всех полей
- Используется кортеж из хешей полей: `hash((self.x, self.y, ...))`
- При `unsafe_hash=True` хеш генерируется даже для изменяемых классов (опасно!)

**6. Паттерн-матчинг (Python 3.10+)**

При `match_args=True` (по умолчанию) генерируется `__match_args__`:

```python
Point.__match_args__ = ('x', 'y')
```

Это позволяет использовать dataclass в match:

```python
match point:
    case Point(x=0, y=0):
        print("Origin")
```

**7. Поля с `init=False`**

Поля, исключённые из `__init__`, часто вычисляются в `__post_init__`. Пример:

```python
@dataclass
class Rectangle:
    width: float
    height: float
    area: float = field(init=False)

    def __post_init__(self):
        self.area = self.width * self.height
```

**8. Наследование и default-значения**

Проблема: mutable default values. Решение — использовать `default_factory`:

```python
@dataclass
class Node:
    children: list = field(default_factory=list)  # А не children: list = []
```

При наследовании важно: сначала идут поля без значений по умолчанию, затем с значениями по умолчанию.

**9. Методы сравнения при `order=True`**

Генерируются все методы сравнения на основе кортежа полей:

```python
def __lt__(self, other):
    if isinstance(other, self.__class__):
        return (self.x, self.y) < (other.x, other.y)
    return NotImplemented
```

Этот подход гарантирует тотальный порядок.

**10. Производительность**

- **Плюсы**: Сгенерированный код на C-уровне быстрее, чем ручной Python-код
- **Минусы**: На создание класса уходит больше времени (генерация методов)
- **Память**: Обычные dataclass используют `__dict__`, что увеличивает потребление памяти

Оптимизация: `@dataclass(slots=True)` + `__slots__` для экономии памяти.

**11. Интроспекция**

Dataclass предоставляет API для интроспекции:

- `fields(class_or_instance)`: Список полей
- `asdict(instance)`: Конвертация в словарь
- `astuple(instance)`: Конвертация в кортеж
- `is_dataclass(obj)`: Проверка, является ли dataclass

**12. Кастомные дескрипторы и property**

Можно комбинировать с property:

```python
@dataclass
class Circle:
    radius: float

    @property
    def diameter(self):
        return self.radius * 2
```

Но property не участвует в автогенерации методов.

**13. Тестирование для AQA**

При тестировании dataclass важно проверять:

1. **Корректность генерации методов**: `__init__`, `__repr__`, `__eq__`, etc.
2. **Наследование**: Правильный порядок и обработка полей
3. **Frozen-классы**: Неизменяемость после создания
4. **Значения по умолчанию**: Особенно изменяемые (list, dict)
5. **Хеширование**: Для frozen классов и при `unsafe_hash`
6. **Сериализация**: `asdict()` и `astuple()`
7. **Паттерн-матчинг**: Корректность работы с match/case
8. **Производительность**: Время создания экземпляров, сравнения
9. **Память**: Потребление при использовании `slots=True` vs обычных

**14. Отладка и внутренние структуры**

Можно исследовать сгенерированный код:

```python
import dis

dis.dis(Point.__init__)  # Посмотреть байткод __init__
print(Point.__annotations__)  # Аннотации
from dataclasses import fields

print(fields(Point))  # Объекты Field
```

- [Содержание](#содержание)

---

# **Enum**

Enum (перечисление) в Python — это специальный тип данных, позволяющий создавать набор именованных констант, что делает
код более читаемым и защищённым от опечаток. Члены Enum являются иммутабельными синглтонами и поддерживают итерацию,
сравнение и доступ по имени или значению. Python предоставляет несколько вариантов: базовый `Enum`, `IntEnum` (
наследующий `int`), `Flag` для битовых операций и `StrEnum` (с Python 3.11) для строковых констант.

## **Junior Level**

Enum (перечисление) в Python — это специальный тип данных, который позволяет создавать набор именованных констант.
Вместо использования "магических чисел" или строк в коде вы можете создать понятные имена для значений, что делает код
более читаемым и защищённым от опечаток.

Например, вместо использования чисел 0, 1, 2 для статусов задачи можно создать Enum:

```python
from enum import Enum


class TaskStatus(Enum):
    PENDING = 0
    RUNNING = 1
    COMPLETED = 2
    FAILED = 3
```

Теперь в коде можно использовать `TaskStatus.RUNNING` вместо просто `1`. Enum предоставляет итерацию, сравнение и доступ
к членам по имени или значению. Особенно полезны они для ограничения допустимых значений параметров функции и замены
строковых констант.

## **Middle Level**

1. **Типы Enum**:
    - `Enum`: Базовый класс для создания перечислений
    - `IntEnum`: Члены являются подклассами `int`, могут использоваться везде, где ожидается целое число
    - `Flag`, `IntFlag`: Для битовых флагов, поддерживают побитовые операции
    - `StrEnum` (Python 3.11+): Члены являются подклассами `str`

2. **Свойства членов Enum**:
    - `name`: Имя члена (строка)
    - `value`: Значение члена (может быть любого типа)
    - `__members__`: Словарь всех членов {имя: член}

3. **Автоматические значения**:
    - `auto()`: Автоматически присваивает уникальные значения (начиная с 1)
    - `@enum.unique`: Декоратор, гарантирующий уникальность значений

4. **Методы и операции**:
    - Итерация: `for status in TaskStatus:`
    - Доступ по значению: `TaskStatus(1)`
    - Доступ по имени: `TaskStatus['RUNNING']`
    - Проверка принадлежности: `isinstance(value, TaskStatus)`

5. **Расширенные возможности**:
    - Методы класса: можно добавлять методы в класс Enum
    - Свойства (property): вычисляемые атрибуты
    - Наследование: Enum может наследоваться от других классов (кроме другого Enum)

6. **Иммутабельность**: Члены Enum — синглтоны. Нельзя изменить их значение после создания.

## **Senior Level**

**1. Метапрограммирование и EnumMeta**

Ключ к пониманию Enum — метакласс `EnumMeta`. При создании класса Enum:

- Метакласс `EnumMeta` перехватывает создание класса
- Сканирует атрибуты класса, собирая члены перечисления
- Заменяет простые присваивания (например, `RED = 1`) объектами-членами Enum

```python
# Внутренний процесс создания Enum
class Color(Enum):
    RED = 1
    GREEN = 2

# На самом деле создаётся:
# Color.RED = Color(name='RED', value=1)
# Color.GREEN = Color(name='GREEN', value=2)
```

**2. Структура объекта члена Enum**

В CPython (`Modules/enummodule.c`):

```c
typedef struct {
    PyObject_HEAD
    PyObject *name;     // Имя члена (строка)
    PyObject *value;    // Значение члена (любой объект Python)
    PyObject *type;     // Ссылка на класс Enum
} enumobject;
```

Члены Enum наследуются от своего класса, но являются отдельными экземплярами:

- `Color.RED.__class__` вернёт `<enum 'Color'>`
- `isinstance(Color.RED, Color)` вернёт `True`

**3. Синглтон-поведение и `__new__`**

Метод `__new__` в классе Enum переопределён для обеспечения синглтон поведения:

```python
def __new__(cls, value):
    # Если значение уже существует, возвращаем существующий член
    if value in cls._value2member_map_:
        return cls._value2member_map_[value]
    # Иначе создаём новый
    obj = object.__new__(cls)
    obj._value_ = value
    return obj
```

Кэширование в `_value2member_map_` обеспечивает O(1) доступ по значению.

**4. IntEnum и наследование от встроенных типов**

`IntEnum` наследуется от `int` и `Enum`:

```python
class IntEnum(int, Enum):
    pass
```

Это создаёт сложности с MRO (Method Resolution Order). Python использует специальную логику в метаклассе `EnumMeta` для
разрешения конфликтов.

**5. Flag и IntFlag: битовые операции**

`Flag` и `IntFlag` используют другой подход:

- Значения должны быть степенями двойки (1, 2, 4, 8...)
- Поддерживают побитовые операции: `|`, `&`, `^`, `~`
- Могут комбинироваться: `Permissions.READ | Permissions.WRITE`

Внутренне комбинации флагов представляются как отдельные экземпляры с составными значениями.

**6. Дескрипторы и доступ к членам**

При доступе к атрибуту Enum через точку (`Color.RED`):

- Срабатывает дескриптор в метаклассе
- Возвращается заранее созданный объект-член
- Дескриптор обеспечивает иммутабельность — нельзя переназначить `Color.RED`

**7. `__members__` и интроспекция**

`__members__` — это `collections.OrderedDict`, создаваемый метаклассом:

- Ключи: имена членов
- Значения: объекты-члены Enum
- Сохраняет порядок определения

Метакласс также создаёт:

- `_member_names_`: список имён в порядке определения
- `_member_map_`: {имя: член}
- `_value2member_map_`: {значение: член}

**8. Оптимизации производительности**

- **Кэширование**: Доступ по значению кэшируется в `_value2member_map_`
- **Слоты**: Объекты членов могут использовать `__slots__` для экономии памяти
- **Хеширование**: Члены Enum хешируемы и могут использоваться как ключи словаря

**9. Создание Enum во время выполнения**

Enum можно создавать динамически:

```python
Color = Enum('Color', ['RED', 'GREEN', 'BLUE'])
# Эквивалентно:
# class Color(Enum):
#     RED = 1
#     GREEN = 2
#     BLUE = 3
```

Внутренне это использует `type()` для создания класса.

**10. Сериализация и pickle**

Enum корректно сериализуется pickle:

- При сериализации сохраняется ссылка на член
- При десериализации восстанавливается тот же объект (синглтон)
- Это гарантирует, что `Color.RED is pickle.loads(pickle.dumps(Color.RED))` равно `True`

**11. Проверка типов и аннотации**

Для аннотаций типов можно использовать:

- `Literal[Color.RED, Color.GREEN]` для конкретных значений
- `Color` для любого члена перечисления
- `typing.get_args()` и `typing.get_origin()` для интроспекции

**12. Проблемы и ограничения**

- **Наследование**: Нельзя наследоваться от другого Enum (кроме смешивания с другими классами)
- **Изменяемые значения**: Если значение Enum изменяемое (например, список), это может привести к неожиданному поведению
- **Производительность**: Создание Enum класса медленнее, чем создание простого класса
- **Память**: Каждый член Enum — отдельный объект, что увеличивает потребление памяти

**13. Тестирование для AQA**

При тестировании Enum важно проверять:

1. **Уникальность значений**: Все значения должны быть уникальны (если используется `@unique`)
2. **Корректность операций сравнения**: `==`, `is`, `in`
3. **Итерацию**: Порядок итерации должен соответствовать порядку определения
4. **Сериализацию**: Корректность pickle и JSON сериализации
5. **Типизацию**: Корректность работы с type hints и mypy
6. **Битовые операции**: Для Flag и IntFlag проверка побитовых операций
7. **Граничные случаи**: Доступ к несуществующим членам, сравнение с не-Enum
8. **Производительность**: Время доступа к членам, итерации

**14. Отладка и интроспекция**

Можно исследовать внутреннюю структуру:

```python
print(Color._member_names_)
print(Color._member_map_)
print(Color._value2member_map_)
print(Color.RED._value_)
```

- [Содержание](#содержание)

---

# **Garbage Collector (Сборщик мусора)**

Сборщик мусора в Python использует два основных механизма: подсчёт ссылок для немедленного освобождения памяти, когда
счётчик достигает нуля, и циклический сборщик для обнаружения и удаления недостижимых групп объектов, ссылающихся друг
на друга. Объекты делятся на три поколения, что оптимизирует процесс, так как большинство объектов живут недолго. Модуль
`gc` позволяет управлять этим процессом, получать статистику и настраивать поведение сборщика.

## **Junior Level**

Сборщик мусора в Python — это автоматический механизм управления памятью, который освобождает память от объектов,
которые больше не используются программой. Python использует комбинированный подход: **подсчет ссылок (reference
counting)** для немедленного освобождения памяти и **циклический сборщик мусора (cycle collector)** для обнаружения и
удаления циклических ссылок.

Представьте, что каждый объект в Python имеет счётчик, который увеличивается при создании новой ссылки на объект и
уменьшается при удалении ссылки. Когда счётчик достигает нуля, объект немедленно удаляется. Однако, если два объекта
ссылаются друг на друга (циклическая ссылка), их счётчики никогда не станут нулевыми. Для таких случаев существует
циклический сборщик мусора, который периодически запускается и находит недостижимые циклы.

Модуль `gc` позволяет управлять сборщиком мусора, получать статистику и настраивать поведение.

## **Middle Level**

1. **Два механизма управления памятью**:
    - **Подсчет ссылок**: Каждый объект хранит счётчик ссылок (`ob_refcnt`). При создании ссылки счётчик увеличивается,
      при удалении — уменьшается. При достижении нуля вызывается деструктор (`__del__`) и память освобождается. Это
      происходит немедленно и предсказуемо.
    - **Циклический сборщик**: Обнаруживает и удаляет группы объектов, которые ссылаются друг на друга, но недостижимы
      из основной программы.

2. **Поколения объектов (Generational GC)**:
    - Объекты делятся на три поколения (0, 1, 2). Новые объекты попадают в поколение 0.
    - Чаще всего запускается сборка в поколении 0, реже — в 1, ещё реже — в 2.
    - Основано на эмпирическом наблюдении: большинство объектов живут недолго.

3. **Пороги сборки**:
    - Параметры `gc.get_threshold()` возвращают кортеж (threshold0, threshold1, threshold2).
    - Сборка в поколении 0 запускается, когда количество выделений минус освобождений превышает threshold0.
    - После определённого числа сборок в поколении 0 запускается сборка в поколении 1 и т.д.

4. **Модуль `gc`**:
    - `gc.enable()` / `gc.disable()`: Включение/выключение циклического сборщика.
    - `gc.collect(generation=None)`: Принудительный запуск сборки.
    - `gc.get_referents(obj)`: Объекты, на которые ссылается `obj`.
    - `gc.get_referrers(obj)`: Объекты, которые ссылаются на `obj`.
    - `gc.set_debug(flags)`: Установка флагов отладки.

5. **Проблемные случаи**:
    - **Циклические ссылки с `__del__`**: Если объекты в цикле имеют метод `__del__`, сборщик не может безопасно удалить
      их (возникает `uncollectable`).
    - **Слабые ссылки (`weakref`)**: Не увеличивают счётчик ссылок, не препятствуют сборке мусора.

6. **Производительность**:
    - Подсчёт ссылок происходит при каждой операции с объектами (быстро, но постоянно).
    - Циклический сборщик запускается периодически и может вызывать паузы.

## **Senior Level**

**1. Реализация подсчёта ссылок в CPython**

В `Include/object.h`:

```c
typedef struct _object {
    Py_ssize_t ob_refcnt;        // Счётчик ссылок
    PyTypeObject *ob_type;
} PyObject;
```

Макросы для работы со счётчиком:

- `Py_INCREF(op)`: Увеличивает счётчик (атомарно в многопоточном режиме).
- `Py_DECREF(op)`: Уменьшает счётчик. При достижении нуля вызывает `_Py_Dealloc(op)`.

**2. Циклический сборщик: алгоритм и структуры**

**Поколения**: Каждое поколение — это двусвязный список объектов (`_PyGC_Head`):

```c
typedef struct _gc_head {
    uintptr_t _gc_next;   // Следующий объект в списке поколения
    uintptr_t _gc_prev;   // Предыдущий объект
    Py_ssize_t gc_refs;   // Временное поле для алгоритма
} PyGC_Head;
```

**Алгоритм обнаружения циклов (трехцветная маркировка)**:

1. **Идентификация**: Все объекты в поколении помечаются как белые (непосещённые).
2. **Поиск корней**: Обход корневых объектов (глобальные переменные, локальные переменные в стеке вызовов, регистры
   процессора). Они помечаются как серые.
3. **Распространение**: Пока есть серые объекты:
    - Берём серый объект, обходим все его ссылки.
    - Если ссылка ведёт на белый объект, делаем его серым.
    - Исходный объект становится чёрным.
4. **Очистка**: Все белые объекты (недостижимые) удаляются.

**3. Поколения и пороги**

Внутренние счетчики:

- `generations[0].count`: Количество объектов в поколении 0.
- `generations[0].threshold`: Порог для запуска сборки (по умолчанию 700).

Когда `generations[0].count > generations[0].threshold`, запускается сборка в поколении 0. После сборки выжившие объекты
перемещаются в поколение 1, и т.д.

**4. Обработка `__del__` и `gc.garbage`**

Если в цикле есть объекты с методом `__del__`, сборщик не может определить порядок вызова деструкторов. Такие объекты
помещаются в `gc.garbage` (список), чтобы программист мог обработать их вручную.

**5. Многопоточность и GIL**

Подсчёт ссылок атомарен благодаря GIL. Циклический сборщик работает так:

- При запуске сборки все потоки приостанавливаются.
- Сборщик захватывает GIL и выполняет сборку.
- Потоки возобновляются после завершения.

**6. Оптимизации CPython**

**Быстрое выделение памяти (pymalloc)**:

- Использует собственный аллокатор для небольших объектов (до 512 байт).
- Уменьшает фрагментацию и ускоряет выделение/освобождение.

**Кэширование свободной памяти**:

- Освобождённая память кэшируется для быстрого повторного использования.
- Для каждого размера объекта есть свой пул свободных блоков.

**7. Слабые ссылки (weakref)**

Реализованы через отдельную структуру:

```c
typedef struct _PyWeakReference {
    PyObject_HEAD
    PyObject *wr_object;    // Слабая ссылка (может быть NULL)
    PyObject *wr_callback;  // Колбэк при удалении объекта
    // ...
} PyWeakReference;
```

Слабые ссылки не увеличивают `ob_refcnt`, поэтому не препятствуют сборке мусора.

**8. Отладка и диагностика**

Флаги отладки (`gc.set_debug()`):

- `DEBUG_STATS`: Вывод статистики после сборки.
- `DEBUG_COLLECTABLE`: Вывод информации об удаляемых объектах.
- `DEBUG_UNCOLLECTABLE`: Вывод информации о неудаляемых объектах (с `__del__`).
- `DEBUG_SAVEALL`: Сохранение всех удаляемых объектов в `gc.garbage`.

**9. Производительность и настройка**

**Проблемы**:

- **Паузы**: Циклический сборщик может вызывать заметные паузы в работе программы.
- **Фрагментация памяти**: Частые выделения/освобождения приводят к фрагментации.

**Оптимизации**:

- Настройка порогов через `gc.set_threshold()`.
- Отключение сборщика (`gc.disable()`) в критичных по времени участках кода (с последующим ручным запуском).
- Использование слабых ссылок для предотвращения циклических ссылок.

**10. Тестирование для AQA**

**Подходы к тестированию**:

1. **Тестирование утечек памяти**:
    - Использование `tracemalloc` для отслеживания выделений.
    - Создание сценариев, которые должны освобождать память.
    - Проверка, что `gc.collect()` не находит новых недостижимых объектов.

2. **Тестирование циклических ссылок**:
    - Создание искусственных циклов и проверка их удаления.
    - Проверка объектов с `__del__` (должны попадать в `gc.garbage`).

3. **Нагрузочное тестирование**:
    - Длительная работа приложения с проверкой стабильности потребления памяти.
    - Тестирование в условиях нехватки памяти (out-of-memory).

4. **Интеграция с профилировщиками**:
    - Использование `objgraph` для визуализации графа объектов.
    - Интеграция с `pympler` для детального анализа памяти.

**11. Инструменты для анализа памяти**

- `tracemalloc`: Трассировка выделений памяти.
- `objgraph`: Визуализация графа объектов.
- `pympler`: Детальный анализ использования памяти.
- `memory_profiler`: Профилирование использования памяти построчно.
- `gc.get_objects()`: Получение списка всех отслеживаемых объектов (осторожно — может быть медленно).

**12. Специфика для разных реализаций Python**

- **CPython**: Описанный выше подход (подсчёт ссылок + циклический сборщик).
- **PyPy**: Использует сложный сборщик мусора с перемещением объектов (moving GC), более агрессивный и эффективный.
- **Jython/IronPython**: Полагаются на сборщик мусора JVM/.NET соответственно.

- [Содержание](#содержание)

---

# **Сложность кода**

## **Junior Level**

1. **Асимптотическая сложность** — как быстро растёт время выполнения программы или использование памяти при увеличении
   объёма данных. Говорят "O-большое": O(1) — постоянно, O(n) — линейно, O(n²) — квадратично и т.д.

2. **Цикломатическая сложность** — мера количества путей выполнения в программе. Чем больше if/else, for/while, тем выше
   эта сложность и тем больше тестов нужно написать.

3. **Сложность связей** — насколько модули/классы/функции зависят друг от друга. Сильно связанный код труднее
   тестировать, потому что приходится мокать много зависимостей.

4. **Сложность поддержки** — насколько легко изменять и развивать код. Включает читаемость, документированность,
   соответствие стандартам.

Для тестировщика: высокая асимптотическая сложность → проверяем производительность на больших данных; высокая
цикломатическая сложность → больше тестовых сценариев; высокая связность → сложнее изолировать модули для тестирования;
низкая поддерживаемость → больше времени на понимание кода перед тестированием.

## **Middle Level**

**1. Асимптотическая сложность (Asymptotic Complexity)**

- **Верхняя граница (O-нотация)**: O(f(n)) — время выполнения не превысит c·f(n) для больших n.
- **Нижняя граница (Ω-нотация)**: Ω(f(n)) — время выполнения не меньше c·f(n).
- **Точная граница (Θ-нотация)**: Θ(f(n)) — и O(f(n)), и Ω(f(n)).
- **Амортизированный анализ**: Средняя производительность по серии операций.
- **Космическая сложность**: Аналогично временной, но для памяти.

**Практика для AQA**:

- Тестирование boundary values: малые, средние, большие объёмы данных.
- Профилирование с помощью cProfile, memory_profiler.
- Нагрузочное тестирование с постепенным увеличением нагрузки.

**2. Цикломатическая сложность (Cyclomatic Complexity)**

- **Расчёт**: M = E - N + 2P, где E — рёбра в графе потока управления, N — узлы, P — компоненты связности.
- **Практические рекомендации**:
    - 1-10: простая функция, низкий риск
    - 11-20: умеренная сложность
    - 21-50: высокая сложность, требует переработки
    - 51+: очень высокая, неприемлемо
- **Инструменты**: radon, mccabe, pylint, sonarqube.

**Связь с тестированием**:

- Минимальное количество тестов ≥ цикломатической сложности.
- Каждый независимый путь должен быть покрыт тестом.
- Особое внимание сложным условиям и вложенным циклам.

**3. Сложность связей (Coupling Complexity)**

**Типы связности**:

- **Content coupling**: Прямой доступ к внутренним данным.
- **Common coupling**: Использование глобальных переменных.
- **Control coupling**: Передача флагов управления.
- **Stamp coupling**: Передача составных объектов.
- **Data coupling**: Передача только необходимых данных (идеал).

**Метрики**:

- **CBO (Coupling Between Objects)**: Количество классов, с которыми связан данный класс.
- **Ca (Afferent Coupling)**: Сколько классов зависят от данного.
- **Ce (Efferent Coupling)**: От скольких классов зависит данный.

**Для тестирования**:

- Высокая связность → сложнее мокировать зависимости.
- Необходимость интеграционных тестов в дополнение к модульным.
- Риск каскадных изменений при модификации кода.

**4. Сложность поддержки (Maintainability Complexity)**

**Компоненты**:

- **Анализируемость**: Насколько легко понять код.
- **Изменяемость**: Насколько легко вносить изменения.
- **Стабильность**: Насколько изменения в одной части влияют на другие.
- **Тестируемость**: Насколько легко писать и выполнять тесты.

**Метрики**:

- **MI (Maintainability Index)**: Рассчитывается на основе цикломатической сложности, LOC, комментариев.
- **Комментарии/код**: Соотношение комментариев к объёму кода.
- **Нарушения кодстайла**: Количество нарушений PEP8, других стандартов.

**Инструменты**: SonarQube, CodeClimate, Codacy.

- [Содержание](#содержание)

---

# **ООП**

## **Junior Level**

Объектно-ориентированное программирование (ООП) — это подход к разработке программ, где основными строительными блоками
являются объекты, которые объединяют данные и методы для работы с ними.

Основные концепции ООП:

1. **Класс** — чертёж или шаблон для создания объектов (например, класс `Dog`).
2. **Объект** — конкретный экземпляр класса (например, `my_dog = Dog()`).
3. **Инкапсуляция** — объединение данных и методов в одном объекте и сокрытие внутренней реализации от внешнего мира.
4. **Наследование** — возможность создавать новые классы на основе существующих, перенимая их свойства и методы (
   например, класс `Poodle` наследует от `Dog`).
5. **Полиморфизм** — возможность использовать объекты разных классов через одинаковый интерфейс (например, методы
   `speak()` для `Dog` и `Cat` могут работать по-разному).
6. **Абстракция** - это концепция, которая позволяет выделить существенные характеристики объекта, игнорируя
   несущественные детали реализации. Она помогает работать со сложными системами, представляя их в виде упрощённых
   моделей.

ООП помогает организовать код, делает его более понятным, удобным для повторного использования и изменения.

## **Middle Level**

В Python ООП реализовано динамически и поддерживает все классические принципы, а также некоторые уникальные особенности:

**1. Динамическая природа классов и объектов**:

- Классы являются объектами первого класса (можно присваивать переменным, передавать в функции).
- Атрибуты и методы могут добавляться, изменяться или удаляться во время выполнения.
- Поддержка динамического создания классов через `type()`.

**2. Множественное наследование и MRO**:

- Python поддерживает множественное наследование.
- Алгоритм C3 Linearization определяет порядок разрешения методов (Method Resolution Order, MRO).
- `super()` используется для корректного вызова методов родительских классов.

**3. Дескрипторы и свойства**:

- **Дескрипторы** — объекты, реализующие протокол `__get__`, `__set__`, `__delete__`. Лежат в основе свойств, методов,
  статических методов и методов класса.
- **Свойства (property)** — позволяют использовать getter, setter и deleter для атрибутов, сохраняя синтаксис доступа к
  атрибуту.

**4. Абстрактные классы**:

- Модуль `abc` позволяет создавать абстрактные классы и методы.
- Абстрактный класс не может быть инстанциирован, требует переопределения абстрактных методов в дочерних классах.

**5. Магические методы (dunder methods)**:

- Методы вида `__init__`, `__str__`, `__add__` и т.д.
- Позволяют переопределять поведение операторов и встроенных функций для объектов пользовательских классов.

**6. Метаклассы**:

- Классы, экземпляры которых являются другими классами.
- Позволяют вмешиваться в процесс создания класса, добавлять валидацию, регистрацию и т.д.

**7. Принципы SOLID в Python**:

- **Single Responsibility**: Каждый класс должен иметь одну причину для изменения.
- **Open/Closed**: Классы должны быть открыты для расширения, но закрыты для изменения.
- **Liskov Substitution**: Объекты базового класса должны быть заменяемы объектами производных классов без изменения
  корректности программы.
- **Interface Segregation**: Много специализированных интерфейсов лучше одного универсального.
- **Dependency Inversion**: Зависимости должны строиться на абстракциях, а не на деталях.

**8. Протоколы и утиная типизация**:

- Python использует утиную типизацию: интерфейс объекта определяется его поведением (наличием методов), а не явным
  объявлением.
- **Протоколы** — неформальные интерфейсы, например, протокол итератора требует методы `__iter__` и `__next__`.

## **Senior Level**

**1. Философские основы ООП и их реализация в Python**

**Онтология объектов**:

- В Python всё является объектом, включая классы и метаклассы, что создаёт рекурсивную онтологическую структуру.
- Классы как объекты порождают эпистемологическую проблему: класс одновременно является и шаблоном, и экземпляром.

**Проблема идентичности vs равенства**:

- `is` проверяет идентичность (совпадение объектов в памяти).
- `==` проверяет равенство (через `__eq__`).
- Для иммутабельных объектов (строк, чисел) Python кэширует и переиспользует объекты, что размывает границу.

**2. Динамизм как фундаментальное свойство**

**Модификация во время выполнения**:

- Возможность изменения класса влияет на все существующие экземпляры (через обновление `__class__.__dict__`).
- Паттерн "Monkey Patching" — модификация поведения во время выполнения, мощный, но опасный инструмент.

**Интроспекция и рефлексия**:

- `inspect` модуль позволяет исследовать структуру классов во время выполнения.
- `__dict__` и `__slots__` как различные подходы к хранению атрибутов.
- `getattr`, `setattr`, `hasattr` — доступ к атрибутам через строковые имена.

**3. Множественное наследование и проблема ромбовидного наследования**

**Алгоритм C3 Linearization**:

- Решает проблему неоднозначности порядка вызова методов.
- Гарантирует сохранение порядка из родительских классов и монотонность.
- Формально: пусть L[C] — линеаризация класса C, тогда L[C] = [C] + merge(L[B1], ..., L[Bn], [B1, ..., Bn]), где merge
  удаляет первый элемент из списков, который не встречается в хвостах других списков.

**Миксины (Mixins)**:

- Классы, предназначенные для расширения функциональности через множественное наследование.
- Не предназначены для самостоятельного использования, только в комбинации с другими классами.

**4. Дескрипторы и метапрограммирование**

**Теория дескрипторов**:

- **Data descriptors** (имеют `__set__` или `__delete__`) имеют приоритет над записью в `__dict__`.
- **Non-data descriptors** (только `__get__`) уступают `__dict__`.
- Цепочка вызовов: `obj.attr` → `type(obj).__dict__['attr'].__get__(obj, type(obj))`.

**Метаклассы как вершина метапрограммирования**:

- `__prepare__` возвращает пространство имён для тела класса (может быть `OrderedDict` для сохранения порядка).
- `__new__` создаёт объект класса, `__init__` инициализирует.
- Использование метаклассов для регистрации классов, валидации атрибутов, автоматического создания свойств.

**5. Альтернативные парадигмы в контексте Python**

**Композиция vs Наследование**:

- Наследование создаёт жёсткую связь "является" (is-a).
- Композиция создаёт гибкую связь "имеет" (has-a).
- В Python композиция часто предпочтительнее из-за динамической природы.

**Функциональное программирование в ООП**:

- Функции как объекты первого класса.
- Замыкания и декораторы как функциональные элементы.
- Иммутабельность и чистота функций vs изменяемое состояние объектов.

**Протокольно-ориентированное программирование**:

- Акцент на поведении (протоколах), а не иерархии классов.
- `typing.Protocol` (PEP 544) для статической проверки протоколов.
- Структурная типизация вместо номинальной.

**6. ООП и тестируемость**

**Проблемы тестирования ООП-кода**:

- **Состояние, рассеянное по многим объектам**: Трудно воспроизвести конкретное состояние для теста.
- **Наследование усложняет мокирование**: Зависимости от родительских классов.
- **Побочные эффекты**: Изменение состояния объектов влияет на последующие вызовы.

**Паттерны для улучшения тестируемости**:

- **Dependency Injection**: Внедрение зависимостей через конструктор для упрощения мокирования.
- **Сохранение чистоты функций**: Выделение бизнес-логики в чистые функции.
- **Интерфейсы и абстракции**: Использование `abc` для определения чётких контрактов.

**7. Эволюция ООП в Python**

**Историческое развитие**:

- От классических классов к new-style классам (Python 2.2+).
- Унификация типов и классов в Python 3.

**Современные тенденции**:

- **Data Classes** (Python 3.7+): Автоматическая генерация boilerplate кода для классов-данных.
- **Protocols** (Python 3.8+): Структурная типизация для статической проверки.
- **Pattern Matching** (Python 3.10+): Новый способ работы с структурами данных.

**8. Критика ООП и ответы на неё**

**Аргументы против ООП**:

- **Наследование создаёт хрупкие иерархии**: Изменение базового класса ломает дочерние.
- **Проблема "взрыва подклассов"**: Необходимость создавать множество подклассов для комбинаций признаков.
- **Состояние как источник ошибок**: Разделяемое изменяемое состояние усложняет многопоточность.

**Ответы в контексте Python**:

- **Миксины и композиция** решают проблему взрыва подклассов.
- **Иммутабельные объекты** уменьшают проблемы с состоянием.
- **Модули и функции** как альтернатива излишней объектной ориентированности.

**9. Будущее ООП и парадигм в Python**

**Конвергенция парадигм**:

- Слияние ООП, функционального и асинхронного программирования.
- Декораторы и контекстные менеджеры как элементы аспектно-ориентированного программирования.

**Влияние машинного обучения**:

- Автоматическая генерация и оптимизация кода.
- Нейросетевые метапрограммирование: AI, предлагающий архитектурные решения.

**Квантовые вычисления**:

- Объектно-ориентированное представление квантовых состояний и операций.
- Новая парадигма для квантовых алгоритмов.

- [Содержание](#содержание)

---

# **Абстракция**

## **Junior Level**

Абстракция в объектно-ориентированном программировании — это процесс выделения существенных характеристик объекта и
игнорирования несущественных деталей. Представьте, что вы заказываете пиццу: вас интересует её состав и цена, но не
детали того, как её готовят на кухне или доставляют курьером. Вы абстрагируетесь от сложных процессов, фокусируясь
только на том, что важно для вас как заказчика.

В программировании абстракция позволяет создавать модели реальных объектов, которые содержат только те свойства и
методы, которые нужны для решения конкретной задачи. Например, в программе для библиотеки класс "Книга" может иметь
свойства "автор", "название", "год издания" и методы "взять", "вернуть". Но он не будет включать такие детали, как "вес
книги", "цвет обложки" или "материал страниц", если они не важны для работы библиотечной системы.

Абстракция делает код проще, понятнее и легче для изменения.

## **Middle Level**

В Python абстракция реализуется через несколько механизмов, каждый из которых предоставляет свой уровень сокрытия
деталей:

1. **Абстрактные классы**:
    - Определяются с помощью модуля `abc` (abstract base classes).
    - Не могут быть инстанциированы напрямую.
    - Содержат абстрактные методы (помеченные `@abstractmethod`), которые должны быть реализованы в дочерних классах.
    - `@abstractproperty` (устаревшее в Python 3.3+) и комбинация `@property` с `@abstractmethod`.

2. **Интерфейсы**:
    - В Python нет явных интерфейсов как в Java/C#, но их роль выполняют абстрактные классы и протоколы.
    - Протоколы — неформальные интерфейсы, основанные на утиной типизации.

3. **Инкапсуляция как средство абстракции**:
    - `_single_underscore`: защищённый атрибут (соглашение).
    - `__double_underscore`: приватный атрибут с name mangling.
    - Свойства (`@property`) для контроля доступа к атрибутам.

4. **Уровни абстракции**:
    - **Высокоуровневые абстракции**: интерфейсы, абстрактные классы.
    - **Среднеуровневые**: конкретные классы с детализированным поведением.
    - **Низкоуровневые**: вспомогательные классы, детали реализации.

5. **Шаблоны проектирования, основанные на абстракции**:
    - **Фабричный метод**: абстрагирует процесс создания объектов.
    - **Мост (Bridge)**: разделяет абстракцию и реализацию.
    - **Стратегия (Strategy)**: абстрагирует семейство алгоритмов.

6. **Абстракция данных**:
    - Создание типов данных, которые скрывают свою внутреннюю структуру.
    - Предоставление только операций для работы с данными.

- [Содержание](#содержание)

---

# **Инкапсуляция**

## **Junior Level**

Инкапсуляция — это принцип объектно-ориентированного программирования, который объединяет данные и методы, работающие с
этими данными, внутри одного объекта и скрывает внутренние детали реализации от внешнего мира.

Представьте, что у вас есть банковский счет. Вы можете пополнять его, снимать деньги и проверять баланс, но вам не нужно
знать, как именно банк хранит ваши данные, как они обрабатывают транзакции или как обновляют баланс. Вам предоставляют
простой интерфейс (например, банкомат или мобильное приложение), а все сложности скрыты внутри.

В Python инкапсуляция реализуется через модификаторы доступа: публичные (public), защищенные (protected) и приватные (
private) атрибуты и методы. Это помогает защитить данные от неправильного использования и обеспечивает контроль над тем,
как объект взаимодействует с внешним миром.

## **Middle Level**

В Python инкапсуляция имеет свои особенности из-за динамической природы языка:

1. **Уровни доступа**:
    - **Публичные (public)**: Обычные атрибуты и методы, доступные отовсюду.
    - **Защищенные (protected)**: Имена с одним подчёркиванием (`_attr`). Это соглашение, а не строгая защита. Говорит
      разработчикам: "это для внутреннего использования".
    - **Приватные (private)**: Имена с двумя подчёркиваниями (`__attr`). Python применяет **name mangling** (искажение
      имён), превращая `__attr` в `_ClassName__attr`. Это затрудняет случайный доступ, но не делает атрибут полностью
      недоступным.

2. **Свойства (property)**:
    - Декораторы `@property`, `@attr.setter`, `@attr.deleter`.
    - Позволяют контролировать доступ к атрибутам, добавляя логику при чтении, записи или удалении.
    - Сохраняют синтаксис доступа как к обычному атрибуту.

3. **Дескрипторы**:
    - Классы, реализующие протокол `__get__`, `__set__`, `__delete__`.
    - Лежат в основе свойств, методов класса, статических методов.
    - Позволяют создавать переиспользуемые механизмы управления доступом.

4. **Методы доступа (getters/setters)**:
    - В Python не принято создавать простые getters/setters для всех атрибутов.
    - Используются только при необходимости добавить логику (валидацию, вычисления).
    - Иначе нарушается принцип "We're all consenting adults here".

5. **`__slots__`**:
    - Ограничивает набор атрибутов экземпляра.
    - Экономит память, предотвращая создание `__dict__`.
    - Ограничивает динамическое добавление атрибутов.

6. **Интерфейсы и абстракции**:
    - Абстрактные классы (`abc.ABC`) определяют контракты без раскрытия реализации.
    - Протоколы (`typing.Protocol`) для структурной типизации.

- [Содержание](#содержание)

---

# **Наследование**

## **Junior Level**

Наследование в ООП — это механизм, позволяющий создавать новый класс на основе существующего, перенимая его свойства и
методы. Новый класс называется **дочерним** (подклассом), а существующий — **родительским** (суперклассом).

Представьте, что у вас есть класс `Animal` с методами `eat()` и `sleep()`. Вы можете создать класс `Dog`, который
наследует от `Animal`, и автоматически получит эти методы. Затем вы можете добавить специфичное поведение для собаки:
метод `bark()`. Это позволяет избежать дублирования кода и создавать логические иерархии объектов.

Наследование представляет отношение **"является"** (is-a): собака является животным. Это один из основных способов
достижения полиморфизма — возможности использовать объекты разных классов через общий интерфейс.

## **Middle Level**

В Python наследование имеет несколько ключевых особенностей:

1. **Синтаксис наследования**:
   ```python
   class Parent:
       pass
   
   class Child(Parent):
       pass
   ```

2. **Множественное наследование**:
   Python поддерживает наследование от нескольких классов:
   ```python
   class Child(Parent1, Parent2, Parent3):
       pass
   ```

3. **Переопределение методов**:
   Дочерний класс может переопределять методы родительского класса, предоставляя свою реализацию.

4. **Расширение методов**:
   Использование `super()` для вызова родительской реализации:
   ```python
   def method(self):
       super().method()  # Вызов родительского метода
       # Дополнительная логика
   ```

5. **Method Resolution Order (MRO)**:
   Порядок поиска методов при множественном наследовании. Определяется алгоритмом C3 и доступен через
   `ClassName.__mro__`.

6. **Абстрактные классы и наследование**:
   Использование `abc.ABC` для создания классов, которые нельзя инстанциировать, но от которых можно наследоваться.

7. **Миксины (Mixins)**:
   Классы, предназначенные для добавления функциональности через множественное наследование. Не предназначены для
   самостоятельного использования.

8. **Доступ к родительским атрибутам**:
    - Через `super()`
    - Через явное указание имени родительского класса: `ParentClass.method(self)`

9. **Наследование встроенных типов**:
   Можно наследоваться от `list`, `dict`, `str` и других встроенных типов, но это требует осторожности.

10. **Композиция vs наследование**:
    Наследование создаёт жёсткую связь "является", композиция — гибкую связь "имеет". Композиция часто предпочтительнее.

- [Содержание](#содержание)

---

# **Полиморфизм**

## **Junior Level**

Полиморфизм — это способность объектов с разной внутренней структурой иметь одинаковый интерфейс и реагировать на одни и
те же команды по-разному.

Представьте, что у вас есть пульт дистанционного управления, который работает с разными
устройствами: нажатие кнопки "включить" включает и телевизор, и кондиционер, и музыку, но делает это по-разному. Каждое
устройство понимает команду "включить" по-своему.

В программировании полиморфизм позволяет использовать один и тот же метод или функцию для работы с объектами разных
классов. Например, если у классов `Dog` и `Cat` есть метод `speak()`, то при вызове `dog.speak()` мы услышим "Гав!", а
при `cat.speak()` — "Мяу!". Это позволяет писать более гибкий и универсальный код.

В Python полиморфизм тесно связан с концепцией "утиной типизации" (duck typing): если объект ходит как утка и крякает
как утка, то он утка. То есть Python не проверяет тип объекта заранее, а смотрит, есть ли у него нужный метод в момент
вызова.

## **Middle Level**

В Python полиморфизм проявляется в нескольких формах:

1. **Утиная типизация (Duck Typing)**:
    - Основной механизм полиморфизма в Python
    - Объекты используются на основе наличия методов/атрибутов, а не их типа
    - Пример: любой объект с методом `__len__()` можно передать в функцию `len()`

2. **Переопределение методов в наследовании**:
    - Дочерние классы могут переопределять методы родительских классов
    - Вызов метода у объекта дочернего класса использует переопределённую версию

3. **Перегрузка операторов**:
    - Магические методы (`__add__`, `__str__`, `__getitem__`) позволяют определить поведение операторов для
      пользовательских классов
    - Один и тот же оператор (`+`, `==`, `[]`) работает по-разному для разных типов

4. **Абстрактные базовые классы (ABC)**:
    - Определяют интерфейсы, которые должны быть реализованы
    - `collections.abc` содержит абстрактные классы для коллекций (`Iterable`, `Sequence`, `Mapping`)

5. **Протоколы**:
    - Неформальные интерфейсы, основанные на наличии определённых методов
    - Пример: протокол итератора требует методы `__iter__()` и `__next__()`

6. **Функции высшего порядка**:
    - Функции, принимающие другие функции как аргументы или возвращающие функции
    - `map()`, `filter()`, `sorted()` с параметром `key`

7. **Множественная диспетчеризация**:
    - Используется в библиотеках типа `multipledispatch`
    - Выбор реализации функции на основе типов нескольких аргументов

8. **`singledispatch` из `functools`**:
    - Позволяет создавать перегруженные функции на основе типа первого аргумента

- [Содержание](#содержание)

---

# **Diamond Problem**

## **Junior Level**

Проблема ромбовидного наследования (Diamond Problem) — это классическая проблема в объектно-ориентированном
программировании, возникающая при множественном наследовании.
Представьте себе ромб (алмаз), где вершина — это базовый класс A, от него наследуются
два класса B и C, а от обоих наследуется класс D. Если в классе A есть метод `some_method()`, и классы B и C
переопределяют его по-разному, то возникает вопрос: какую реализацию унаследует класс D — от B или от C?

Эта проблема создает неоднозначность в поведении программы. В Python эта проблема решается с помощью четко определенного
**порядка разрешения методов (MRO - Method Resolution Order)**, который определяет, в каком порядке интерпретатор будет
искать метод в иерархии наследования.

## **Middle Level**

Технически в Python проблема решается алгоритмом **C3 linearization**, который гарантирует детерминированный и
предсказуемый порядок поиска методов.

1. **MRO (Method Resolution Order):**
    - MRO — это порядок, в котором Python ищет методы в иерархии классов. Его можно посмотреть через атрибут класса
      `__mro__` или метод `mro()`.
    - Алгоритм C3 гарантирует, что:
        - Каждый класс встречается в MRO ровно один раз.
        - Подклассы идут перед своими суперклассами.
        - Порядок наследования, указанный в определении класса, сохраняется.

2. **Пример:**
   Для иерархии A -> B, A -> C, B -> D, C -> D, MRO для D будет `[D, B, C, A, object]`. При вызове метода из D Python
   будет искать его в этом порядке.

3. **Механизм `super()`:**
    - `super()` — это не вызов родительского класса в традиционном понимании, а вызов следующего класса в MRO.
    - Это позволяет реализовать **кооперативное множественное наследование**, где каждый класс в цепочке может добавить
      свою функциональность, вызывая `super()`.
    - При правильном использовании `super()` во всех классах иерархии (A, B, C, D) метод будет вызван у каждого из них,
      что позволяет избежать "потери" вызова.

- [Содержание](#содержание)

---

# **Магические методы**

## **Junior Level**

Магические методы в Python — это специальные методы, которые начинаются и заканчиваются двойным подчеркиванием (dunder),
например `__init__`, `__str__`, `__add__`. Они позволяют настраивать поведение объектов при выполнении стандартных
операций: создание, вывод на печать, сложение, сравнение и т.д.

Когда вы пишете `obj + other`, Python внутри вызывает `obj.__add__(other)`. Если в вашем классе определен этот метод, вы
можете задать, как именно будет работать сложение для ваших объектов. Это делает код интуитивно понятным и позволяет
вашим объектам вести себя как встроенные типы данных.

Самые распространенные магические методы:

- `__init__` — инициализатор объекта (конструктор)
- `__str__` — строковое представление для человека (`str(obj)`, `print(obj)`)
- `__repr__` — строковое представление для разработчика (показывается в консоли)
- `__len__` — поддержка функции `len(obj)`
- `__getitem__`, `__setitem__` — доступ по индексу или ключу: `obj[key]`

Использование магических методов делает ваш код более "питоничным" и понятным.

## **Middle Level**

Магические методы — это основа протоколов в Python, реализующих полиморфизм через утиную типизацию. Они делятся на
несколько категорий:

1. **Методы жизненного цикла**:
    - `__new__` — создание объекта (вызывается до `__init__`)
    - `__init__` — инициализация объекта
    - `__del__` — финализатор (вызывается перед удалением объекта)

2. **Строковые представления**:
    - `__str__` — для неформального представления (читабельного для человека)
    - `__repr__` — для формального представления (должно позволять воссоздать объект)
    - `__format__` — для поддержки `format(obj, spec)`
    - `__bytes__` — для `bytes(obj)`

3. **Протоколы сравнения**:
    - `__eq__`, `__ne__` — равенство/неравенство (`==`, `!=`)
    - `__lt__`, `__le__`, `__gt__`, `__ge__` — сравнения (`<`, `<=`, `>`, `>=`)
    - `__hash__` — вычисление хеша (обязателен для использования в множествах и как ключ словаря)

4. **Протоколы числовых операций**:
    - Арифметические: `__add__`, `__sub__`, `__mul__`, `__truediv__`
    - Унарные: `__neg__`, `__pos__`, `__abs__`
    - С преобразованием типов: `__int__`, `__float__`, `__bool__`

5. **Протоколы коллекций**:
    - `__len__` — длина
    - `__getitem__`, `__setitem__`, `__delitem__` — доступ по индексу/ключу
    - `__contains__` — поддержка оператора `in`
    - `__iter__`, `__next__` — итерация

6. **Протоколы вызова и контекста**:
    - `__call__` — вызов объекта как функции
    - `__enter__`, `__exit__` — контекстные менеджеры (оператор `with`)

7. **Дескрипторы и атрибуты**:
    - `__getattr__`, `__setattr__`, `__delattr__` — доступ к атрибутам
    - `__getattribute__` — перехват всех обращений к атрибутам
    - `__dir__` — список атрибутов для `dir(obj)`

8. **Сериализация**:
    - `__getstate__`, `__setstate__` — для pickle
    - `__reduce__`, `__reduce_ex__` — кастомная сериализация


- [Содержание](#содержание)

---

# **Инвариантность и ковариантность**

## **Junior Level**

Инвариантность и ковариантность — это понятия из теории типов, которые описывают, как отношения между типами сохраняются
при использовании в обобщённых (generic) конструкциях.

Представьте, что у вас есть коробки для фруктов. У вас есть:

- Коробка для любых фруктов (`Box[Fruit]`)
- Коробка для яблок (`Box[Apple]`), где `Apple` — подтип `Fruit`

**Ковариантность** означает, что отношение "является подтипом" сохраняется и для коробок: если `Apple` — подтип `Fruit`,
то `Box[Apple]` — подтип `Box[Fruit]`. То есть коробку яблок можно использовать там, где ожидается коробка фруктов.

**Инвариантность** означает, что эти типы не связаны: `Box[Apple]` и `Box[Fruit]` — совершенно разные, независимые типы.
Коробку яблок нельзя использовать вместо коробки фруктов, и наоборот.

**Контравариантность** (обратное ковариантности) — когда отношение обращается: если `Apple` — подтип `Fruit`, то
`Box[Fruit]` — подтип `Box[Apple]`.

В Python эти концепции важны при работе с типизацией (type hints), особенно при использовании обобщённых типов вроде
`List[T]`, `Callable[[T], R]`.

## **Middle Level**

В контексте Python и статической типизации:

1. **Ковариантность (covariant)**:
    - Обозначается `+T` в определении типа
    - Если `A` — подтип `B`, то `Generic[A]` — подтип `Generic[B]`
    - Пример: `typing.Sequence` ковариантен по типу элемента

2. **Контравариантность (contravariant)**:
    - Обозначается `-T`
    - Если `A` — подтип `B`, то `Generic[B]` — подтип `Generic[A]` (обратное отношение)
    - Пример: `typing.Callable` контравариантен по типам аргументов

3. **Инвариантность (invariant)**:
    - Наиболее распространённый случай по умолчанию
    - Никаких отношений между `Generic[A]` и `Generic[B]`, даже если `A` и `B` связаны
    - Пример: `List[T]` инвариантен (по соображениям безопасности)

4. **Проблема изменяемости**:
    - Ковариантность безопасна для "read-only" типов (`Sequence`, `Iterable`)
    - Изменяемые типы (`List`, `Dict`) должны быть инвариантны для безопасности типов
    - Иначе можно было бы добавить апельсин в список яблок

5. **Практическое применение в Python**:
   ```python
   from typing import TypeVar, Generic, List, Sequence
   
   class Fruit: pass
   class Apple(Fruit): pass
   
   # Ковариантный тип
   T_co = TypeVar('T_co', covariant=True)
   class ReadOnlyBox(Generic[T_co]): pass
   
   # Контравариантный тип  
   T_contra = TypeVar('T_contra', contravariant=True)
   class Consumer(Generic[T_contra]): pass
   ```

6. **Проверка типов (mypy, pyright)**:
    - Статические анализаторы используют информацию о вариативности для проверки безопасности типов
    - Ошибки вариативности — частые причины ошибок типизации


- [Содержание](#содержание)

---

# **Декораторы классов и методов**

## **Junior Level**

Декораторы классов и методов — это специальные функции или классы, которые позволяют модифицировать поведение классов и
методов без изменения их исходного кода. Синтаксически они выглядят как `@декоратор` перед определением класса или
метода.

Для классов декоратор принимает класс, изменяет его (добавляет/удаляет/изменяет методы или атрибуты) и возвращает
модифицированный класс. Например, декоратор может автоматически добавить логирование ко всем методам класса.

Для методов декоратор принимает метод, оборачивает его дополнительной логикой и возвращает новую функцию. Классические
примеры: `@staticmethod`, `@classmethod`, `@property`. Также можно создавать собственные декораторы, например, для
проверки прав доступа, кэширования результатов или измерения времени выполнения.

## **Middle Level**

1. **Декораторы методов**:
    - **Стандартные декораторы**: `@staticmethod`, `@classmethod`, `@property` (с `@setter`, `@deleter`)
    - **Пользовательские декораторы**: функции, принимающие функцию и возвращающие новую функцию
    - **Декораторы с параметрами**: требуют дополнительного уровня вложенности
    - `@functools.wraps(func)` для сохранения метаданных оригинальной функции

2. **Декораторы классов**:
    - Принимают класс, модифицируют его и возвращают (часто тот же класс, но изменённый)
    - Могут добавлять/удалять/изменять атрибуты и методы
    - Могут регистрировать класс в каком-либо реестре
    - Могут заменять класс другим классом (фабрика классов)

3. **Особенности декораторов методов в классах**:
    - При декорировании метода в классе декоратор получает функцию, а не связанный метод
    - Во время создания класса метод существует как обычная функция в пространстве имён класса
    - При доступе через экземпляр (`instance.method`) срабатывает протокол дескриптора

4. **Порядок применения декораторов**:
    - Декораторы применяются снизу вверх (ближайший к определению применяется первым)
    - `@a @b def method(): ...` → `method = a(b(method))`
    - Для классов аналогично

5. **Использование в тестировании**:
    - `@unittest.mock.patch` для мокирования атрибутов и методов
    - `@pytest.fixture` для создания фикстур
    - `@pytest.mark.parametrize` для параметризации тестов
    - `@pytest.mark.skip` для пропуска тестов

6. **Метаклассы vs декораторы классов**:
    - Метаклассы работают на уровне создания класса
    - Декораторы классов работают после создания класса
    - Часто одну задачу можно решить как метаклассом, так и декоратором


- [Содержание](#содержание)

---

# **Множественное наследование и MRO**

## **Junior Level**

Множественное наследование — это возможность класса наследоваться от нескольких родительских классов одновременно. В
Python вы можете указать несколько классов в скобках при определении класса: `class Child(Parent1, Parent2, Parent3):`.
Это позволяет комбинировать функциональность из разных источников.

Проблема возникает, когда несколько родительских классов имеют методы с одинаковыми именами. Какой из них должен быть
вызван? Для решения этой проблемы Python использует **MRO (Method Resolution Order)** — порядок разрешения методов. Это
алгоритм, который определяет последовательность, в которой Python ищет методы в иерархии классов.

MRO можно посмотреть с помощью атрибута `__mro__` или метода `mro()`. Например, `MyClass.__mro__` покажет кортеж классов
в порядке их поиска.

## **Middle Level**

1. **Алгоритм C3 Linearization**:
    - Алгоритм, используемый в Python для вычисления MRO (начиная с Python 2.3).
    - Гарантирует, что порядок будет **монотонным** (если класс A предшествует классу B в MRO класса C, то так будет и
      во всех подклассах C).
    - Учитывает порядок родителей в определении класса.

2. **Принципы MRO**:
    - **Локальный порядок приоритетов**: порядок классов в списке наследования важен.
    - **Сохранение прямых родительских классов**: родительский класс проверяется до своих собственных родителей.
    - **Монотонность**: если класс X предшествует классу Y в MRO класса C, то X предшествует Y в MRO любого подкласса C.

3. **Проблема ромбовидного наследования (diamond problem)**:
   ```python
   class A: pass
   class B(A): pass
   class C(A): pass
   class D(B, C): pass
   ```
   MRO для D будет: D → B → C → A (а не D → B → A → C → A, что было бы с повторением A).

4. **`super()` и MRO**:
    - `super()` использует MRO для определения следующего класса в цепочке.
    - Важно для кооперативного множественного наследования, когда методы вызывают `super()` для передачи управления
      следующему классу.

5. **Миксины (Mixins)**:
    - Классы, предназначенные для множественного наследования.
    - Не предназначены для самостоятельного использования.
    - Обычно добавляют конкретную функциональность (например, логирование, сериализацию).

6. **Практические рекомендации**:
    - Избегайте слишком сложных иерархий множественного наследования.
    - Используйте абстрактные классы и интерфейсы для определения контрактов.
    - Композиция часто предпочтительнее множественного наследования.


- [Содержание](#содержание)

---

# **ABC**

## **Junior Level*

Представьте, что вы архитектор, который разрабатывает проект дома (интерфейс, контракт). Вы создаете подробный чертеж,
где указано: "здесь должна быть дверь", "здесь должно быть не менее двух окон", "обязательно должна быть крыша". Однако
сам чертеж — это не дом, по нему нельзя жить. Вы просто задаете стандарт, которому должен следовать любой построенный по
этому проекту дом.

В Python ABC (Abstract Base Classes) — это и есть такие "чертежи" для классов. Это специальный механизм, который
позволяет нам декларативно сказать: "Любой класс, который будет моим наследником, ОБЯЗАН реализовать конкретные методы (
например, `save()`, `load()`, `validate()`)". Если наследник не реализует все обязательные методы, Python не даст
создать его экземпляр и сразу укажет на ошибку. Это делает код предсказуемым, помогает избежать ошибок в рантайме и
четко документирует ожидания от объекта. Для QA инженера это мощный инструмент для стандартизации тестовых утилит,
плагинов и проверки контрактов в системе.

# ## **Middle Level**

На техническом уровне ABC — это классы, наследующиеся от встроенного класса `abc.ABC` и использующие декоратор
`@abstractmethod` (а также `@abstractclassmethod`, `@abstractstaticmethod`, `@abstractproperty`) для пометки абстрактных
методов. Ключевые аспекты:

1. **Инстанцирование:** Попытка создать экземпляр класса, у которого есть хотя бы один не переопределенный
   `@abstractmethod`, немедленно вызовет исключение `TypeError`. Это проверка происходит в момент вызова `__new__`
   класса, еще до вызова `__init__`.

2. **Регистрация (Registration):** Помимо явного наследования, класс может быть "зарегистрирован" как виртуальный
   подкласс ABC с помощью метода `register()`. После этого `isinstance()` и `issubclass()` будут возвращать для него
   `True`, но при этом **не проводится проверка на наличие абстрактных методов!** Это "мягкий" способ заявить о
   соответствии интерфейсу, полезный для интеграции сторонних или унаследованных классов.

3. **`__subclasshook__`:** Это специальный метод класса (`@classmethod`), который позволяет кастомизировать логику
   проверки `issubclass()`. Метод может проверять наличие у класса требуемых атрибутов или методов (через `hasattr`), а
   не только факт прямого наследования или регистрации. Это самый гибкий и "питонический" способ определения виртуальных
   подклассов.

4. **Встроенные ABCs в `collections.abc`:** Модуль предоставляет богатейший набор готовых абстрактных классов для
   стандартных протоколов: `Iterable`, `Iterator`, `Container`, `Sequence`, `Mapping`, `Callable` и т.д. Использование
   `isinstance(obj, collections.abc.Sequence)` вместо `isinstance(obj, list)` делает код полиморфным и независимым от
   конкретной реализации.

# ## **Senior Level**

Здесь мы говорим о метаклассах, тонкостях разрешения методов (MRO) и системном дизайне.

1. **Метакласс `ABCMeta`:** За кулисами `abc.ABC` — это просто удобный сахар. Настоящая магия происходит в метаклассе
   `abc.ABCMeta`. Именно он отвечает за:
    * Накопление множества абстрактных методов в атрибуте `__abstractmethods__` (это `frozenset`).
    * **Перехват момента создания класса.** При создании нового класса (наследника ABC) `ABCMeta.__new__` проверяет,
      остались ли в нем элементы из `__abstractmethods__`. Если да — инстанцирование блокируется.
    * Реализацию методов `register()` и `__subclasshook__`.

2. **Динамика `__abstractmethods__`:** Это не статический атрибут. При наследовании Python строит MRO (Method Resolution
   Order) для нового класса и динамически вычисляет итоговый набор `__abstractmethods__` как разность между абстрактными
   методами всех родителей и уже реализованными методами в самом классе. Это позволяет создавать сложные иерархии с
   частичными реализациями.

3. **`__instancecheck__` и `__subclasscheck__`:** Метакласс `ABCMeta` переопределяет эти специальные методы. Именно
   благодаря этому `isinstance()` и `issubclass()` работают с виртуальными подклассами. `ABCMeta.__subclasscheck__`
   последовательно проверяет: является ли класс зарегистрированным виртуальным подклассом, есть ли в MRO целевого класса
   данный ABC, и наконец, вызывается ли пользовательский `__subclasshook__`. Это цепочка ответственности (Chain of
   Responsibility) на уровне метаклассов.

4. **Для AQA (Кровь и Кишки):**
    * **Проектирование фреймворка:** Senior AQA использует ABC для создания *строгих контрактов* плагинов. Например,
      метакласс `ABCMeta` гарантирует, что забытый метод `teardown` в кастомном плагине для фикстур будет обнаружен не
      когда тесты упадут в CI, а в момент импорта модуля с плагином.
    * **Валидация архитектуры:** В интеграционных и системных тестах мы можем проверять, что фабрики возвращают объекты,
      соответствующие не конкретному классу, а абстрактному протоколу (`Mapping`, `AsyncIterator`). Это защищает тесты
      от изменений реализации.
    * **Моки и стабы:** При использовании `unittest.mock` или создании собственных заглушек ABC — идеальная основа. Мы
      можем создать абстрактный класс `DataService`, написать против него тесты, а затем подменить его строгим моком (
      `Mock(spec=AbstractDataService)`), который будет валидировать сигнатуры всех вызовов.
    * **Обратная сторона:** Слепо наследоваться от ABC для каждого интерфейса — овердизайн. Python — язык с утиной
      типизацией. Часто достаточно использовать `__subclasshook__`, который проверяет наличие методов, или вообще
      полагаться на протоколы (PEP 544 – `typing.Protocol`), которые появились как альтернатива ABC для структурной
      проверки типов. Senior-инженер должен понимать, когда `Protocol` (проверка статическим анализатором)
      предпочтительнее `ABC` (проверка в рантайме).

- [Содержание](#содержание)

---

# **Протокол (Protocol)**

## **Junior Level*

Представьте, что вы описываете не конкретный предмет, а его роль. Например, "то, что можно включить". Под это описание
подходит и лампа, и компьютер, и телевизор — у всех есть кнопка "включить". Вам неважно, что это за устройство внутри;
важно, что оно поддерживает операцию "включения".

В Python **Протокол (Protocol)** — это именно такое формальное описание роли или поведения. Он говорит: "Если у объекта
есть вот такие методы и атрибуты, то он автоматически считается подходящим для определенной цели, даже если он не был
изначально для этого предназначен". Это продвинутая, "официальная" версия принципа утиной типизации ("если ходит как
утка и крякает как утка, то это утка"). В отличие от Abstract Base Class (ABC), где класс должен явно заявить "я
наследуюсь от этого чертежа", протокол работает на уровне "если ты имеешь эти черты, то ты соответствуешь".

# ## **Middle Level**

Технически `Protocol` — это конструкция системы типизации, определенная в PEP 544 и доступная в модуле `typing`. Его
ключевые аспекты:

1. **Структурная, а не номинативная типизация:** Класс соответствует протоколу, если его **структура** (набор методов и
   атрибутов с правильными сигнатурами) совпадает с протоколом. Ему не нужно явно наследоваться от протокола. Это
   фундаментальное отличие от ABC, где требуется явное номинативное объявление (наследование или регистрация).

2. **Синтаксис и `@runtime_checkable`:** Протокол определяется как класс, наследующийся от `typing.Protocol`. Его методы
   часто помечаются как абстрактные, используя `...` (ellipsis) в теле. По умолчанию протоколы используются **только
   статическими анализаторами типов** (mypy, pyright). Чтобы позволить проверку соответствия протоколу во время
   выполнения (`isinstance(obj, MyProtocol)`), протокол необходимо декорировать `@typing.runtime_checkable`. Однако
   такая проверка ограничена: она проверяет только **наличие** указанных атрибутов, но не их сигнатуры или типы.

3. **Generic и вариативность:** Протоколы могут быть параметризованы (Generic), что позволяет описывать типы, зависящие
   от других типов (например, `Iterable[T]`). Они также поддерживают ковариантность и контравариантность через параметры
   `covariant=True`/`contravariant=True`, что критично для точного описания отношений между сложными типами.

4. **Встроенные протоколы:** Модуль `typing` и `collections.abc` предоставляют множество встроенных протоколов:
   `SupportsInt`, `SupportsBytes`, `ContextManager`, `Iterable`, `Sized` и др. Их использование в аннотациях делает код
   гораздо более выразительным и безопасным.

# ## **Senior Level**

Здесь мы погружаемся в механику структурной типизации, взаимодействие с системой типов и метаклассами.

1. **Двойственная природа `Protocol`:** `Protocol` — это одновременно и конструкция для статического анализа, и
   полноценный объект Python. На уровне метаклассов `typing.Protocol` наследуется от `abc.ABCMeta`. Это делает его
   метаклассом, который объединяет возможности ABC (вроде `register()`) с новой семантикой структурной проверки.
   Декоратор `@runtime_checkable` модифицирует класс протокола, добавляя корректную реализацию `__instancecheck__`.

2. **Механика `__instancecheck__` при `@runtime_checkable`:** Когда используется `isinstance(obj, MyProtocol)`,
   вызывается `MyProtocol.__instancecheck__()`. Его реализация, предоставленная декоратором, выполняет итерацию по
   аннотациям протокола (полученным через `get_type_hints()`) и для каждого атрибута выполняет проверку
   `hasattr(obj, attr_name)`. Это **поверхностная проверка существования**, без валидации типов, сигнатур или свойств
   дескрипторов. Она не может отличить метод от простого атрибута с тем же именем. Это ее ключевое и часто упускаемое из
   виду ограничение.

3. **Статический анализ vs Runtime:** Настоящая сила протоколов раскрывается в статическом анализаторе. Анализатор
   строит карту всех типов и для каждого места, где ожидается `ProtocolX`, ищет все классы, структура которых совместима
   с ним. Это включает проверку сигнатур методов, типов возвращаемых значений, свойств (property) и даже
   позиционных/ключевых аргументов. Протокол может требовать наличия определенного `@property` или `__slots__`.

4. **Протоколы и `__protocol_attrs__`:** На низком уровне протоколы могут использовать специальные атрибуты (например,
   `_is_protocol`, `_is_runtime_protocol`) для самоидентификации. Некоторые реализации статических анализаторов кэшируют
   информацию о протоколах для ускорения проверки.

5. **Для AQA (Кровь и Безумие):**
    * **Проектирование тестовых артефактов:** Senior AQA использует протоколы для создания *архитектурных тестов*. Мы
      можем определить протоколы, описывающие контракты критических модулей системы (например, `CacheBackend`,
      `MessageQueue`), а затем написать тест, который рефлексией проверяет, что все реализующие их классы в проекте
      действительно соответствуют не только по наличию методов, но и по сигнатурам (используя `inspect.signature`). Это
      следующий уровень после `@runtime_checkable`.
    * **Моки с глубокой валидацией:** Мы можем создать фабрику моков, которая на основе протокола динамически генерирует
      объект-заглушку с помощью `unittest.mock.Mock` или `unittest.mock.AsyncMock`, автоматически настраивая сигнатуры
      методов. Это дает автодополнение в IDE и предотвращает ошибки в тестах при изменении реального интерфейса.
    * **Инверсия зависимости в тестах:** Используя протоколы в аннотациях, мы явно декларируем, что наши тестовые
      хелперы или фикстуры зависят не от `ConcreteClassA`, а от `SupportsFeatureX`. Это делает тестовый код максимально
      устойчивым к рефакторингу продакшен-кода.
    * **Ограничения и тонкости:** Важно понимать, что `isinstance` с `@runtime_checkable` — это компромисс. Он не
      заменяет статический анализ. Также протоколы не могут требовать наличия специальных методов, которые ищутся
      напрямую в классе (через `__class__`), а не в экземпляре (например, `__len__` является методом класса, но
      `hasattr(instance, '__len__')` часто возвращает `True` из-за наследования). Для таких случаев все еще могут
      потребоваться ABC или прямое использование `issubclass`.

- [Содержание](#содержание)

---

# **Паттерны проектирования**

## **Junior Level*

Паттерны проектирования — это проверенные временем решения типовых проблем в разработке программного обеспечения. Можно
сравнить их с архитектурными чертежами для строительства: вместо того чтобы каждый раз изобретать велосипед, опытные
архитекторы используют готовые схемы организации кода, которые уже доказали свою эффективность для конкретных ситуаций.

Это не готовые куски кода, а скорее концепции или шаблоны мышления о том, как структурировать взаимодействие между
компонентами системы. Например, "Фабрика" — это паттерн для создания объектов, не привязываясь к их конкретным классам,
а "Наблюдатель" описывает, как один объект может уведомлять множество других о произошедших изменениях.

Для AQA инженера понимание паттернов критически важно по нескольким причинам: во-первых, чтобы понимать архитектуру
тестируемого приложения и находить в ней слабые места; во-вторых, чтобы проектировать собственные тестовые фреймворки,
которые будут гибкими, расширяемыми и поддерживаемыми; в-третьих, чтобы говорить с разработчиками на одном языке при
обсуждении дизайна и потенциальных проблем.

# ## **Middle Level**

С технической точки зрения, паттерны проектирования (особенно из классического каталога GoF — "Банды четырех") в Python
часто реализуются с учетом уникальных особенностей языка.

1. **Идиоматичность Python:** Многие классические паттерны в Python могут быть реализованы проще и элегантнее, чем в
   статически типизированных языках. Например:
    * **Стратегия (Strategy):** Вместо создания иерархии классов можно просто передавать callable-объекты (функции,
      лямбды, экземпляры с `__call__`).
    * **Декоратор (Decorator):** Прямо соответствует синтаксическому декоратору Python, который является "обёрткой"
      вокруг функции или метода, модифицирующей его поведение.
    * **Адаптер (Adapter):** Часто реализуется не через наследование, а через композицию и магический метод
      `__getattr__` для перенаправления вызовов.

2. **Категории паттернов:**
    * **Порождающие (Creational):** Решают задачу гибкого и контролируемого создания объектов (Синглтон, Фабрика,
      Строитель). В Python Синглтон часто реализуют через метакласс или модуль (сам модуль по своей природе — синглтон).
    * **Структурные (Structural):** Организуют композицию классов и объектов для образования более крупных структур (
      Адаптер, Мост, Компоновщик, Декоратор, Фасад).
    * **Поведенческие (Behavioral):** Определяют эффективные способы взаимодействия и распределения ответственности
      между объектами (Наблюдатель, Стратегия, Команда, Цепочка обязанностей, Состояние).

3. **Для AQA:**
    * **Page Object** — это специализированный структурный паттерн для UI-тестирования, по сути являющийся Фасадом,
      скрывающим детали HTML-структуры за удобным API.
    * **Фабрика** и **Строитель (Builder)** незаменимы для создания сложных тестовых данных и фикстур.
    * **Наблюдатель (Observer)** или **Издатель-Подписчик (Pub/Sub)** лежит в основе систем событийного логирования или
      сбора результатов тестов в реальном времени.
    * **Прокси (Proxy)** и **Заместитель (Mock/Stub)** — основа библиотек для мокирования (unittest.mock), позволяющих
      подменять реальные зависимости в тестах.

4. **Dependency Injection (Внедрение зависимостей):** Хотя формально не входит в каталог GoF, это ключевой архитектурный
   паттерн, который в Python часто реализуется просто через передачу зависимостей в конструктор (Constructor Injection),
   без использования тяжеловесных фреймворков. Это краеугольный камень тестируемого дизайна.

# ## **Senior Level**

На этом уровне мы рассматриваем паттерны не как рецепты, а как отражение фундаментальных принципов проектирования (
SOLID, DRY, LoD) и глубоко анализируем их реализацию через механизмы Python.

1. **Метапаттерны и Python-идиомы:**
    * **Синглтон:** Реализация через метакласс, переопределяющий `__call__`, чтобы всегда возвращать один экземпляр. Но
      истинный Python-way — это *borg-паттерн* (он же "монограф"): все экземпляры разделяют одно состояние через общий
      атрибут `__dict__`. Это дает преимущества синглтона без жесткой привязки к единственному экземпляру.
    * **Декоратор:** Под капотом `@decorator` — это синтаксический сахар для `func = decorator(func)`. Настоящая мощь в
      том, что декоратор может быть классом с `__call__` (сохраняющим состояние между вызовами) или функцией,
      возвращающей обертку. Декораторы с аргументами — это функция, возвращающая декоратор, который возвращает обертку.
    * **Наблюдатель:** Помимо классической реализации со списком callback'ов, в Python можно использовать
      `weakref.WeakSet` для автоматического удаления "мертвых" подписчиков и предотвращения утечек памяти. Или применить
      механизм дескрипторов (через `property.setter`) для автоматической нотификации при изменении атрибута.

2. **Паттерны и тестируемость:**
    * **Инверсия зависимостей (DIP)** через абстракции (ABC или Protocol) — это главный паттерн, делающий код пригодным
      для автотестов. Код, написанный с нарушением DIP, будет сопротивляться изоляции.
    * **Шаблонный метод (Template Method)** в Python часто заменяется на *Hollywood Principle* ("не звоните нам, мы вам
      позвоним") через передачу хуков (hook methods) в виде callable-объектов, что делает его более гибким и
      тестируемым.
    * **Цепочка обязанностей (Chain of Responsibility)** может быть реализована через корутины (generator-based) или
      асинхронные цепочки (async/await), что особенно актуально для тестирования middleware в веб-приложениях.

3. **Антипаттерны и "запахи кода":**
   Senior AQA должен уметь распознавать, когда паттерн применен неправильно (over-engineering):
    * **Божественный объект (God Object)** под маской Фасада.
    * **Избыточное наследование** вместо композиции.
    * **Ненужный синглтон**, который превращается в глобальное состояние и убивает параллельный запуск тестов.
    * **Увлечение паттернами ради паттернов**, когда простая функция решает задачу лучше сложной иерархии классов.

4. **Паттерны для тестовых фреймворков (Blood & Guts):**
    * **Посетитель (Visitor)** для обхода AST (Abstract Syntax Tree) при статическом анализе тестового кода или
      генерации отчетов.
    * **Состояние (State)** для моделирования сложного жизненного цикла тестового запуска (INIT -> SETUP -> TEST ->
      TEARDOWN -> FINALIZE).
    * **Команда (Command)** для инкапсуляции отдельных шагов теста в объекты, которые можно логгировать, отменять или
      повторять.
    * **Итератор (Iterator)** и **Генератор (Generator)** — не просто паттерны, а встроенные протоколы Python, которые
      лежат в основе `pytest` фикстур с `yield` и ленивой загрузки тестовых данных.

5. **Дизайн через контракты (Design by Contract):**
   Это надстройка над паттернами. Используя ABC, Protocol, декораторы (для pre/post-conditions) и аннотации типов, мы
   можем формализовать ожидания от компонентов. Для AQA это прямой мост к созданию *автоматизированных проверок
   архитектуры* (архитектурных тестов), которые валидируют соблюдение паттернов и контрактов в кодовой базе.

- [Содержание](#содержание)

---

# **Композиция и агрегация**

## **Junior Level*

Композиция и агрегация — это два способа создания отношений между объектами в объектно-ориентированном программировании.
Обе описывают ситуацию, когда один объект содержит в себе другой, но с критически важным различием в силе связи и
управлении жизненным циклом.

Представьте, что вы строите дом. **Композиция** — это как комната в доме. Комната не существует отдельно от дома. Когда
дом сносят, комната исчезает вместе с ним. Объект-владелец (дом) полностью контролирует жизнь объекта-части (комнаты). *
*Агрегация** — это как мебель в доме. Стол, стул, диван существуют независимо от дома. Их занесли в дом, а потом могут
вынести в другой дом или на склад. Объект-владелец (дом) использует объект-часть (мебель), но не управляет его рождением
и смертью.

В разработке композиция означает, что при уничтожении основного объекта уничтожаются и все его составные части.
Агрегация означает, что объекты собраны вместе, но могут жить самостоятельно. Для QA инженера понимание этого различия
помогает проектировать тестовые фикстуры и моки, правильно управлять их жизненным циклом и понимать, какие зависимости
нужно создавать заново, а какие можно переиспользовать между тестами.

# ## **Middle Level**

С технической точки зрения, композиция и агрегация реализуются через **атрибуты класса**, но с разной семантикой
создания и владения.

1. **Реализация:**
    * **Композиция (Composition):** Объект-часть создается **внутри** конструктора (или иного метода) объекта-владельца.
      Владелец полностью инкапсулирует создание и, как правило, не предоставляет публичных методов для замены этой
      части. Часть реализуется как внутренний, приватный атрибут.
    * **Агрегация (Aggregation):** Объект-часть создается **вне** объекта-владельца и передается ему в качестве
      аргумента (чаще всего в конструктор). Владелец сохраняет ссылку на эту часть, но не управляет ее созданием.
      Объект-часть может быть общим (разделяемым) ресурсом.

2. **Жизненный цикл:**
    * При **композиции** жизненный цикл части жестко привязан к жизненному циклу целого. Когда объект-владелец
      удаляется (например, сборщиком мусора), удаляется и объект-часть, если на него больше нет ссылок.
    * При **агрегации** жизненные циклы независимы. Удаление владельца не влечет удаление части, так как на нее могут
      оставаться ссылки из других объектов.

3. **Для AQA:**
    * **Фикстуры в Pytest:** Композиция часто используется для создания сложных, вложенных фикстур, которые существуют
      только в рамках одной тестовой сессии или модуля и автоматически очищаются. Агрегация похожа на фикстуры с
      областью видимости `session` или `package`, которые создаются один раз и переиспользуются многими тестами.
    * **Тестовые данные:** Понимание, когда создавать новый экземпляр тестовых данных для каждого кейса (композиция), а
      когда использовать общий, предсозданный набор данных (агрегация), критично для скорости и изоляции тестов.
    * **Page Object:** Внутри Page Object может существовать композиция из элементов (например, `Button`, `InputField`),
      которые не имеют смысла вне контекста этой страницы. И агрегация — например, общий `Header` или `Footer`, которые
      могут быть переданы в несколько Page Object.

4. **Отличия от наследования:** И композиция, и агрегация — это альтернативы наследованию, предпочитаемые в современном
   дизайне ("предпочитай композицию наследованию"). Они обеспечивают большую гибкость и слабую связанность.

# ## **Senior Level**

На этом уровне мы рассматриваем композицию и агрегацию как фундаментальные паттерны управления зависимостями, памятью и
состоянием в сложных системах.

1. **Управление памятью и ссылочные циклы:**
    * При **композиции**, если используются двунаправленные ссылки (владелец знает о части, а часть хранит ссылку на
      владельца), возникает риск циклической ссылки. В Python до версии 3.4 это могло приводить к утечке памяти, так как
      сборщик мусора на основе подсчета ссылок (reference counting) не мог автоматически очистить такие объекты. Для
      решения использовались `weakref` (слабые ссылки) со стороны части на владельца. Современный сборщик мусора (GC) с
      алгоритмом обнаружения циклов справляется с этим, но `weakref` остаются важным инструментом для явного управления
      жизненным циклом и предотвращения непреднамеренного удержания объектов в памяти.
    * При **агрегации** риск циклических ссылок еще выше, так как объекты живут дольше и могут образовывать сложные
      графы зависимостей. Для тестовых фреймворков, которые создают тысячи объектов за прогон, это критично.

2. **Инверсия управления (IoC) и внедрение зависимостей (DI):** Агрегация — это механическая основа для DI. Объект не
   создает свои зависимости, а получает их извне. В продвинутых сценариях это реализуется через **IoC-контейнеры**,
   которые автоматически разрешают граф зависимостей, учитывая их жизненный цикл (синглтон, на запрос и т.д.). Для
   тестов это позволяет легко подменять реальные реализации на моки.

3. **Протоколы и абстракции:** Истинная мощь агрегации раскрывается, когда объект-владелец зависит не от конкретного
   класса части, а от абстракции (ABC или Protocol). Это превращает агрегацию в инструмент соблюдения **Dependency
   Inversion Principle (DIP)**. Владелец объявляет контракт, а часть (переданная извне) его выполняет. Это делает код
   идеально тестируемым.

4. **Для AQA:**
    * **Жизненный цикл тестовых контекстов:** При построении собственного тестового фреймворка senior-инженер должен
      спроектировать, как будут создаваться и уничтожаться контексты выполнения (test suites, fixtures, драйверы). Здесь
      композиция (вложенные контексты) и агрегация (разделяемые ресурсы, например, пул соединений с БД или экземпляр
      браузера) играют ключевую роль. Неправильный выбор приведет либо к утечкам памяти и состояния между тестами (
      недостаточная изоляция), либо к непомерному росту времени выполнения (избыточное создание).
    * **Мокирование в сложных графах:** Когда система представляет собой глубокий граф агрегированных объектов, простое
      мокирование одного интерфейса может быть недостаточно. Нужно понимать, какие связи являются композиционными (и,
      следовательно, их можно безопасно "вырвать" и заменить целиком), а какие — агрегационными (где замена одной части
      может повлиять на другие владельцы). Это важно для **shallow vs deep mocks**.
    * **Стратегия очистки (Tear-down):** При композиции очистка обычно каскадная и автоматическая. При агрегации —
      требуется явная стратегия: либо владелец не очищает часть, либо используется модель совместного владения с
      подсчетом ссылок, либо применяются паттерны вроде **Resource Acquisition Is Initialization (RAII)**, который в
      Python эмулируется через контекстные менеджеры (`__enter__`/`__exit__`) и `finally`-блоки.

5. **Отношение к другим паттернам:**
    * **Компоновщик (Composite)** — это структурный паттерн, часто строящийся на композиции, чтобы treat individual
      objects and compositions uniformly.
    * **Фасад (Facade)** — часто является агрегатором множества сложных подсистем, скрывая их за простым интерфейсом.
    * **Стратегия (Strategy)** и **Состояние (State)** — обычно агрегируются, так как должны быть заменяемы в runtime.

- [Содержание](#содержание)

---

# **Связность и связанность**

## **Junior Level*

Связность и связанность — это две фундаментальные концепции качества кода, которые описывают, насколько хорошо
организованы компоненты внутри модуля и насколько сильно они зависят друг от друга.

**Связность (Cohesion)** отвечает на вопрос: "Насколько хорошо элементы внутри одного модуля (класса, функции) связаны
между собой и работают для достижения одной четкой цели?". Высокая связность — это хорошо. Это означает, что модуль
делает что-то одно, целостное и понятное. Например, модуль `MathUtils`, который содержит только функции для
математических вычислений, обладает высокой связностью. А модуль `Utils`, который смешивает функции для работы со
строками, отправки email и логирования, имеет низкую связность — это "мусорная корзина", которую трудно поддерживать.

**Связанность (Coupling)** отвечает на вопрос: "Насколько сильно один модуль зависит от внутреннего устройства другого
модуля?". Низкая связанность — это хорошо. Это означает, что модули взаимодействуют через четкие, простые интерфейсы и
могут изменяться независимо друг от друга. Если же модуль "залезает" в приватные детали другого модуля, то любое
изменение в одном сломает другой — это высокая связанность (сильная связь).

Простой принцип: нужно стремиться к **высокой связности и низкой связанности**. Для QA инженера это напрямую влияет на
тестируемость: модули с высокой связностью легко тестировать изолированно, а при низкой связанности можно легко заменять
зависимости на моки.

# ## **Middle Level**

С технической точки зрения эти концепции реализуются через конкретные практики проектирования и имеют четкие индикаторы.

1. **Типы связности (от худшего к лучшему):**
    * **Случайная (Coincidental):** Элементы собраны вместе случайно (например, модуль `MiscHelpers`). Тестировать такое
      невозможно — нет единой ответственности.
    * **Логическая (Logical):** Элементы сгруппированы по категории (например, класс `DataProcessor`, который
      обрабатывает и CSV, и JSON, и XML). Тесты становятся размазанными и хрупкими.
    * **Временная (Temporal):** Элементы выполняются в одно время (например, функция `initialize_all()`, которая
      настраивает логгер, БД и кеш). Приводит к сложным фикстурам и непредсказуемым побочным эффектам в тестах.
    * **Процедурная (Procedural):** Элементы объединены последовательностью шагов (например, функция, которая читает
      файл, парсит данные и сохраняет в БД). В тестах приходится эмулировать всю последовательность.
    * **Коммуникационная (Communicational):** Элементы работают с одними и теми же данными (например, класс
      `CustomerReport`, который и вычисляет статистику, и форматирует отчет). Уже лучше, но еще есть смесь
      ответственностей.
    * **Последовательная (Sequential):** Выход одного элемента является входом для другого (конвейер). Хорошо для
      тестирования каждого шага.
    * **Функциональная (Functional):** Все элементы вносят вклад в выполнение одной четкой задачи — **идеал**. Класс
      `InvoiceCalculator` только считает, `InvoiceFormatter` только форматирует. Юнит-тесты пишутся легко и
      изолированно.

2. **Типы связанности (от худшего к лучшему):**
    * **Содержание (Content):** Модуль напрямую обращается к приватным данным или коду другого модуля (нарушение
      инкапсуляции). Тесты становятся хрупкими к любым изменениям.
    * **Общая (Common):** Модули используют глобальные данные. Для тестов это катастрофа — состояние тестов влияет друг
      на друга, параллельный запуск невозможен.
    * **Внешняя (External):** Модули зависят от внешнего формата данных или протокола. Требует сложных интеграционных
      тестов и моков.
    * **Управление (Control):** Один модуль управляет логикой другого (например, передача флагов). Усложняет
      тестирование, так как нужно проверять множество ветвлений.
    * **Структурная (Stamp):** Модуль принимает сложную структуру данных, но использует только часть полей. Создает
      скрытые зависимости и усложняет создание тестовых данных.
    * **Данных (Data):** Модули взаимодействуют через минимальный интерфейс (например, передача примитивных значений). *
      *Идеал** для тестирования — зависимости легко заглушить.

3. **Инструменты для достижения:**
    * **Принцип единственной ответственности (SRP)** ведет к высокой связности.
    * **Инверсия зависимостей (DIP)** через абстракции (ABC, Protocol) ведет к низкой связанности.
    * **Закон Деметры ("не разговаривай с незнакомцами")** снижает связанность.

# ## **Senior Level**

На этом уровне мы рассматриваем связность и связанность как системные силы, влияющие на энтропию кодовой базы, и
анализируем их через призму метапрограммирования и архитектурных паттернов.

1. **Связность и метаклассы:** Высокая связность на уровне архитектуры часто достигается через **модули** и **пакеты** с
   четкими границами ответственности. Однако Python позволяет нарушать эти границы через мощные механизмы интроспекции и
   метапрограммирования. Например, использование `__subclasses__()` для обнаружения всех наследников в кодовой базе
   создает **скрытую временную связность** — порядок инициализации модулей начинает иметь значение. Для тестов это может
   означать хрупкость и недетерминированное поведение при изменении порядка импортов.

2. **Связанность и система типов:** Введение **аннотаций типов** и **статической проверки** (mypy) формализует
   связанность на уровне контрактов. Однако чрезмерное использование конкретных типов (`List[ConcreteModel]`) вместо
   абстрактных (`Sequence[AbstractModel]`) создает **статическую связанность**, которая затрудняет подмену реализаций в
   тестах. Правильное применение **дженериков** и **протоколов** позволяет достичь "loose coupling by design".

3. **Динамическая природа Python как обоюдоострый меч:**
    * **Monkey Patching** (прямое изменение классов в рантайме) — это пиковый уровень **связанности по содержанию**. В
      тестах это иногда необходимо (например, для мокирования), но если это становится архитектурным приемом (плагины,
      расширения), то система превращается в "макаронный код", где откат изменений и понимание потока выполнения
      становятся невозможными. Ответственный senior AQA должен уметь предложить альтернативы на основе ABC, протоколов
      или механизма `__init_subclass__`.
    * **Импорт через `importlib.import_module()` и динамическое создание классов** (`type()` с тремя аргументами) могут
      использоваться для создания систем с низкой связанностью (например, plugin-архитектура). Но они же требуют сложных
      интеграционных тестов и могут маскировать проблемы связности.

4. **Циклические зависимости (Circular Dependencies):** Это крайнее проявление высокой связанности, когда модули
   импортируют друг друга. Python разрешает это до определенного предела (частично загруженные модули), но это убивает
   тестируемость и понятность. Решения:
    * **Рефакторинг:** Выделение общего абстрактного ядра (DIP).
    * **"Ленивые" импорты** внутри функций или методов. Это не решает проблему логической связанности, но обходит
      технические ограничения, создавая новые риски для тестов (импорт может упасть в неожиданный момент).
    * **Инверсия управления (IoC):** Зависимости не импортируются, а внедряются.

5. **Для AQA:**
    * **Проектирование тестового фреймворка:** Senior AQA обязан проектировать фреймворк с максимально высокой
      связностью (отдельные модули для: управления драйверами, генерации данных, формирования отчетов, утилит) и
      минимальной связанностью между ними (общение через четкие API-интерфейсы, события или абстракции). Это позволяет
      подключать кастомные плагины, не ломая ядро.
    * **Анализ тестируемого приложения:** Понимание уровней связности/связанности позволяет **прогнозировать хрупкость
      тестов** и точки максимального риска. Класс с низкой связностью (God Object) будет иметь монструозные, медленные и
      нестабильные интеграционные тесты. Система с высокой связанностью будет требовать сложных, многоуровневых моков (
      deep mocks), которые сами по себе становятся burden для поддержки.
    * **Метрики и техдолг:** Использование инструментов статического анализа (например, radon для расчета связности) для
      выявления модулей с критически низкой связностью и предложения их рефакторинга как части **технического долга**.
      Senior AQA выступает здесь как инженер по качеству кода, а не только функционала.
    * **Тесты как индикатор архитектурных проблем:** Если для написания простого юнит-теста требуется поднимать половину
      системы (высокая связанность) или тест проверяет несвязанные вещи (низкая связность тестируемого модуля) — это
      красный флаг для архитектуры. Senior AQA должен уметь формализовать и донести это до команды разработки.

- [Содержание](#содержание)

---

# **SOLID**

## **Junior Level*

SOLID — это набор из пяти ключевых принципов проектирования в объектно-ориентированном программировании, которые
помогают создавать гибкий, поддерживаемый и расширяемый код. Каждая буква акронима представляет отдельный принцип:

**S - Single Responsibility Principle (Принцип единственной ответственности):** Каждый класс или модуль должен иметь
только одну причину для изменения. Он должен отвечать за одну конкретную задачу или ответственность.

**O - Open/Closed Principle (Принцип открытости/закрытости):** Классы должны быть открыты для расширения, но закрыты для
модификации. Это означает, что мы можем добавлять новое поведение, не меняя существующий код.

**L - Liskov Substitution Principle (Принцип подстановки Барбары Лисков):** Объекты в программе должны быть заменяемы на
экземпляры их подтипов без изменения корректности программы. Проще говоря: если что-то работает с родительским классом,
это должно работать и с любым его наследником.

**I - Interface Segregation Principle (Принцип разделения интерфейса):** Лучше иметь много специализированных
интерфейсов, чем один универсальный. Клиенты не должны зависеть от методов, которые они не используют.

**D - Dependency Inversion Principle (Принцип инверсии зависимостей):** Модули верхнего уровня не должны зависеть от
модулей нижнего уровня. Оба должны зависеть от абстракций. Абстракции не должны зависеть от деталей — детали должны
зависеть от абстракций.

Для QA инженера понимание SOLID критично, потому что код, написанный по этим принципам, гораздо легче тестировать: он
лучше изолирован, зависимости явные и заменяемые, а поведение предсказуемо.

# ## **Middle Level**

С технической точки зрения, каждый принцип SOLID реализуется через конкретные паттерны и механизмы Python.

1. **SRP (Single Responsibility):** На уровне модуля (файла .py) это означает, что модуль должен экспортировать
   логически связанный набор функций/классов. На уровне класса — класс должен иметь минимальное количество публичных
   методов, связанных одной целью. Нарушение SRP приводит к God-объектам, которые невозможно адекватно покрыть
   юнит-тестами. Метрика: если вы не можете назвать ответственность класса одним коротким предложением без союза "и" —
   принцип нарушен.

2. **OCP (Open/Closed):** В Python реализуется через:
    * **Наследование и полиморфизм:** Создание подклассов для добавления поведения.
    * **Композицию и паттерн "Стратегия":** Передача поведения через callable-объекты.
    * **Декораторы:** Обертывание функций без изменения их исходного кода.
    * **Абстрактные базовые классы (ABC):** Определение интерфейсов для расширения.
      Код, соответствующий OCP, позволяет добавлять новые тестовые сценарии и проверки без модификации ядра тестового
      фреймворка.

3. **LSP (Liskov Substitution):** Это контрактный принцип. В Python он реализуется через:
    * **Соблюдение сигнатур методов:** Подкласс не должен ужесточать предусловия (требовать больше) или ослаблять
      постусловия (обещать меньше).
    * **Сохранение инвариантов:** Состояние объекта после операций должно оставаться валидным с точки зрения базового
      класса.
    * **Исключения:** Подкласс не должен выбрасывать новые типы исключений, не являющиеся подтипами исключений базового
      класса.
      Нарушение LSP — классическая причина падения тестов при замене реализации. Для QA это означает, что моки и стабы
      должны точно следовать контракту реальных объектов.

4. **ISP (Interface Segregation):** В Python, где нет формальных интерфейсов, принцип реализуется через:
    * **Абстрактные базовые классы (ABC)** с минимальным набором абстрактных методов.
    * **Протоколы (Protocol),** которые позволяют описывать узкие, специфичные наборы методов.
    * **Миксины (Mixins)** — классы, предоставляющие конкретную функциональность.
      Следствие для тестирования: нам не нужно мокировать гигантские интерфейсы, достаточно реализовать только
      используемую часть.

5. **DIP (Dependency Inversion):** Практическая реализация:
    * **Зависимость от абстракций:** Вместо `ConcreteService` в коде указывается `AbstractService` (ABC или Protocol).
    * **Внедрение зависимостей (DI):** Зависимости передаются извне (через конструктор, сеттеры, контекст), а не
      создаются внутри класса.
    * **IoC-контейнеры** (в продвинутых случаях) для автоматического управления зависимостями.
      Это краеугольный камень тестируемости: он позволяет легко подменять реальные сервисы моками в тестах.

# ## **Senior Level**

На этом уровне мы рассматриваем SOLID как проявление более глубоких законов проектирования и анализируем их реализацию
через механизмы метапрограммирования, систему типов и динамическую природу Python.

1. **SRP и метаклассы:** Поистине строгий контроль за SRP может быть реализован через **метаклассы**. Метакласс может
   анализировать тело создаваемого класса (через неймспейс `namespace`), подсчитывая количество публичных методов, и
   выбрасывать исключение при создании класса, нарушающего принцип. Более того, можно использовать декораторы классов
   для автоматического разделения ответственности через **делегирование**, создавая прокси-объекты, которые распределяют
   вызовы методов по специализированным классам. Для QA это означает возможность создания "самотестирующихся"
   архитектурных правил.

2. **OCP и система плагинов:** Расширяемость (Open) часто достигается через **динамическое обнаружение и загрузку
   плагинов**. Механизмы:
    * `__subclasses__()` для поиска всех наследников ABC (хрупко, зависит от импортов).
    * **Entry points** в `setup.py`/`pyproject.toml` для обнаружения зарегистрированных плагинов из установленных
      пакетов.
    * **Импорт по соглашению:** Сканирование определенных директорий и динамический импорт через
      `importlib.import_module()`.
      "Закрытость" (Closed) обеспечивается стабильностью **публичного API** модуля или класса. Для тестового фреймворка
      это означает, что можно разрабатывать кастомные плагины для отчетов, форматов данных, провайдеров без модификации
      ядра.

3. **LSP и контрактное программирование:** Принцип Лисков формализует **контракты**. В Python есть инструменты для их
   явного задания:
    * `@abstractmethod` в ABC задает обязательство реализации.
    * Декораторы для предусловий/постусловий (например, библиотека `icontract` или самописные на основе `inspect` и
      `functools.wraps`).
    * **Аннотации типов** и статические анализаторы (mypy) проверяют часть контракта — совместимость типов.
      Нарушение LSP часто проявляется в **ковariantности/контравариантности** возвращаемых типов и аргументов. Если
      подкласс меняет возвращаемый тип на более узкий (например, `-> Dog` вместо `-> Animal`) — это допустимо (
      ковариантность по возврату). Но если он меняет тип аргумента на более широкий (например, принимает `Animal` вместо
      `Dog`) — это нарушает контракт (контравариантность по аргументам). Статические анализаторы могут ловить такие
      нарушения.

4. **ISP и автоматическое разделение интерфейсов:** Протоколы позволяют создавать **адаптивные интерфейсы**. Через
   механизм `__subclasshook__` можно реализовать автоматическое определение, что класс соответствует протоколу, если он
   реализует определенный набор методов. Более того, можно использовать **дескрипторы** и **метаклассы** для
   автоматической генерации "узких" прокси-интерфейсов на лету, которые предоставляют клиенту только требуемый
   функционал, скрывая остальное. Это уменьшает связанность в тестах.

5. **DIP и метапрограммирование:** Продвинутые фреймворки внедрения зависимостей используют:
    * **Интроспекцию** (`inspect.signature`) для автоматического определения зависимостей по аннотациям типов.
    * **Декораторы** для маркировки внедряемых сервисов.
    * **Контекстные менеджеры** для управления временем жизни объектов (scopes: singleton, request, transient).
    * **Асинхронное внедрение** зависимостей с использованием `async`/`await`.
      Для AQA это открывает возможность создания **супер-изолированных тестовых сред**, где все зависимости
      автоматически подменяются на контролируемые версии, включая стандартные библиотечные модули (через
      `unittest.mock.patch` на уровне импорта).

6. **SOLID и тестовая пирамида:**
    * **SRP** позволяет писать чистые, быстрые юнит-тесты.
    * **OCP** позволяет расширять тестовые фреймворки новыми видами тестов (нагрузочными, security) без переписывания.
    * **LSP** гарантирует, что тесты, написанные для базовых классов фикстур, будут работать для производных.
    * **ISP** позволяет создавать легковесные моки для конкретных тестов.
    * **DIP** — основа для изоляции тестов и подмены интеграционных точек на стабы.

7. **Антипаттерны и запахи кода, нарушающие SOLID:**
    * **Нарушение SRP:** Класс `TestHelper`, который и генерирует данные, и делает HTTP-запросы, и пишет в лог, и
      проверяет ответы.
    * **Нарушение OCP:** Монолитный класс `TestRunner` с гигантским `if-elif` по типам тестов.
    * **Нарушение LSP:** Класс `MockResponse`, который в методе `json()` выбрасывает исключение, хотя реальный
      `Response` его не бросает.
    * **Нарушение ISP:** Интерфейс `Database` с методами `connect`, `query`, `disconnect`, `backup`, `restore`, хотя
      большинству тестов нужен только `query`.
    * **Нарушение DIP:** Тестовый класс напрямую создает экземпляр `RealDatabaseConnector` вместо получения абстракции
      `DatabaseConnector`.

- [Содержание](#содержание)

---

# **Специфика ООП в Python**

## **Junior Level*

Объектно-ориентированное программирование (ООП) в Python — это не строгая доктрина, а гибкий набор инструментов,
встроенных в самую суть языка. В отличие от некоторых других языков (например, Java или C#), Python подходит к ООП с
прагматичной точки зрения: "если что-то ведет себя как объект, то это объект".

Главные особенности:

1. **Всё является объектом.** Даже числа, строки, функции и классы — это объекты. Это дает им возможность иметь атрибуты
   и методы.
2. **Динамическая природа.** Типы проверяются во время выполнения программы, а не на этапе компиляции. Это позволяет
   создавать очень гибкие конструкции.
3. **Простота синтаксиса.** Нет сложных модификаторов доступа вроде `private`, `protected`, `public` в классическом
   понимании. Вместо них используется соглашение об одном (`_`) и двух подчеркиваниях (`__`) для указания на уровень "
   защищенности".
4. **Множественное наследование.** Класс может наследоваться от нескольких родительских классов одновременно, что
   позволяет создавать сложные иерархии, но требует понимания метода разрешения порядка наследования (MRO).
5. **Магические методы (dunder methods).** Поведение объектов (например, что происходит при сложении, при обращении по
   индексу, при преобразовании к строке) определяется специальными методами, которые начинаются и заканчиваются двойным
   подчеркиванием (`__init__`, `__str__`, `__add__`). Это позволяет перегружать операторы и делать код интуитивно
   понятным.

# ## **Middle Level**

На техническом уровне специфика ООП в Python раскрывается через несколько ключевых концепций и внутренних механизмов.

1. **Модель данных на основе специальных методов:** Ядро ООП в Python — это **протоколы**, определяемые дандер-методами.
   Например, протокол итератора требует методов `__iter__` и `__next__`. Это не абстрактные классы в классическом
   понимании, а соглашения. Класс, реализующий эти методы, автоматически становится итератором. Это называется **утиной
   типизацией**.

2. **Атрибуты и их поиск:** Атрибуты объекта в Python — это не просто поля в памяти. Поиск атрибута (`obj.attr`)
   проходит по сложной цепочке:
    * Сначала проверяется `obj.__dict__`.
    * Если не найдено, идет проверка в `type(obj).__dict__` (атрибуты класса).
    * Затем поиск поднимается по иерархии наследования (согласно MRO).
    * Если атрибут все еще не найден, вызывается метод `__getattr__` (если он определен).
      Этот механизм лежит в основе свойств (`@property`), дескрипторов и динамического добавления атрибутов.

3. **Дескрипторы (Descriptor Protocol):** Это один из самых мощных и скрытых механизмов. Если атрибут класса является
   объектом с методами `__get__`, `__set__` или `__delete__`, то при доступе к этому атрибуту через экземпляр вызываются
   эти методы. Именно так работают `@property`, `@classmethod`, `@staticmethod`, обычные методы и даже `__slots__`.
   Дескрипторы — это основа привязки методов к экземплярам.

4. **Метаклассы:** В Python классы тоже являются объектами. Объекты какого класса? Класса `type`. `type` — это
   метакласс, экземплярами которого являются все пользовательские классы. Пользователь может создавать свои метаклассы,
   наследуясь от `type`, чтобы перехватывать и модифицировать процесс создания класса. Это позволяет реализовать такие
   вещи, как автоматическую регистрацию подклассов, добавление атрибутов, проверку сигнатур методов или реализацию
   шаблонов вроде Singleton на уровне создания класса.

5. **Классы и экземпляры как namespaces:** Класс и экземпляр — это, по сути, словари (`__dict__`) с дополнительной
   логикой поиска. Это позволяет делать monkey-patching (динамическое изменение классов и объектов в рантайме), что
   широко используется в тестировании для мокирования, но может быть опасным в production-коде.

6. **Для AQA:** Динамическая природа ООП в Python позволяет создавать элегантные паттерны для тестирования:
    * **Фикстуры в pytest:** Используют декораторы и механизм зависимостей, которые были бы невозможны без гибкой модели
      объектов.
    * **Моки и патчи (`unittest.mock`):** Полностью полагаются на возможность подмены атрибутов объектов и модулей в
      рантайме.
    * **Плагины:** Система плагинов в pytest использует метаклассы и динамическое обнаружение для расширения
      функциональности.

# ## **Senior Level**

Здесь мы погружаемся в онтологию объектов Python, механику связывания (binding) и философию "всё является объектом".

1. **Объектная модель CPython (логический уровень):** Каждый объект в Python — это структура `PyObject` (или ее
   наследник в логическом представлении), содержащая как минимум счетчик ссылок и указатель на тип объекта. **Тип
   объекта (`PyTypeObject`)** — это тоже объект, содержащий таблицу виртуальных функций (slots), которая определяет
   поведение экземпляров. Эти слоты (например, `tp_getattr`, `tp_iter`, `tp_call`) соответствуют дандер-методам. Когда
   вы вызываете `len(obj)`, интерпретатор ищет слот `tp_len` в типе объекта. Если он найден (что означает, что класс
   определил `__len__`), вызывается связанная функция. Это делает операции над объектами невероятно быстрыми на уровне
   C.

2. **Механизм связывания методов (Method Binding):** Когда вы определяете метод в классе, он хранится в `class.__dict__`
   как обычная функция. При обращении к нему через экземпляр (`instance.method`) срабатывает **дескриптор**. Функция,
   будучи дескриптором, имеет метод `__get__`. При его вызове с экземпляром и классом в качестве аргументов, он
   возвращает **связанный метод (bound method)** — объект, который "запоминает" экземпляр и при вызове автоматически
   подставляет его первым аргументом (`self`). Если метод вызван через класс (`Class.method`), `__get__` возвращает
   несвязанную (unbound) функцию. Это не магия, а прямое следствие протокола дескрипторов.

3. **`__slots__` и экономия памяти:** Обычно каждый экземпляр хранит атрибуты в своем словаре `__dict__`, что требует
   много памяти. `__slots__` — это особый атрибут класса, который заменяет `__dict__` на статически выделенный массив
   слотов для указанных атрибутов. Это не только экономит память, но и ускоряет доступ к атрибутам. Однако это
   ограничивает динамическое добавление атрибутов и требует осторожности при наследовании. Для AQA это важно при
   тестировании высоконагруженных систем или работе с огромными массивами объектов (например, в генераторах тестовых
   данных).

4. **Метаклассы и `__init_subclass__`:** Вместо сложных метаклассов для простых задач модификации подклассов теперь
   можно использовать метод класса `__init_subclass__`. Он вызывается при инициализации каждого нового подкласса. Это
   более простая и понятная альтернатива для регистрации классов, добавления атрибутов по умолчанию или валидации.
   Однако для сложных трансформаций (изменение порядка наследования, модификация тела класса) метаклассы остаются
   незаменимы.

5. **Для AQA:**
    * **Создание DSL (предметно-ориентированного языка) для тестов:** Используя магические методы (`__getattr__`,
      `__call__`), дескрипторы и метаклассы, можно создавать выразительные DSL. Например, тест может выглядеть как
      `page.element.should(be_visible.and_have(text="Submit"))`. Это достигается цепочкой объектов, каждый из которых
      возвращает следующий через кастомные `__getattr__` или дескрипторы.
    * **Динамическая генерация тестов:** Метакласс может сканировать тело класса во время его создания, находить
      функции, начинающиеся с `test_`, и на их основе генерировать множество реальных тест-кейсов с разными параметрами
      или именами, что используется в `pytest` и `unittest` с параметризацией.
    * **Трассировка и интроспекция в тестах:** Используя знание о том, что вызов метода — это вызов связанного объекта,
      можно создавать декораторы, которые оборачивают каждый метод класса для трассировки вызовов, замеров времени или
      автоматического логирования. Это делается путем перехвата в метаклассе или через `__getattribute__` (но осторожно,
      это очень низкоуровневый и медленный механизм).
    * **Эмуляция и патчинг на системном уровне:** Понимание, что даже импортированные модули — это объекты в
      `sys.modules`, позволяет делать глубокие патчи, например, подменять встроенные функции (`open`, `os.listdir`) для
      изоляции тестов от файловой системы. Это основа библиотек вроде `pytest-mock`.
    * **Профилирование потребления памяти тестами:** Понимание модели объектов (`__dict__`, `__slots__`, ссылочные
      циклы) позволяет писать тесты, которые не только проверяют функциональность, но и профилируют утечки памяти или
      чрезмерное потребление ресурсов в тестируемом коде, используя `tracemalloc` или `gc` модули.

- [Содержание](#содержание)

---

# **Наследование и композиция**

## **Junior Level*

Наследование и композиция — это два основных способа повторного использования кода и построения отношений между классами
в ООП.

**Наследование** — это отношение "является" (is-a). Когда класс `Dog` наследует от класса `Animal`, мы говорим, что "
Собака **является** Животным". Наследование позволяет дочернему классу автоматически получать все методы и атрибуты
родительского класса, а также может переопределять или расширять его поведение.

**Композиция** — это отношение "имеет" (has-a). Когда класс `Car` содержит в себе объект класса `Engine`, мы говорим,
что "Автомобиль **имеет** Двигатель". Композиция позволяет строить сложные объекты, собирая их из более простых
компонентов.

Простое правило: **предпочитайте композицию наследованию**. Наследование создает жесткую связь между классами, тогда как
композиция дает больше гибкости и позволяет изменять поведение объекта в рантайме, заменяя его компоненты. Для AQA
инженера композиция часто предпочтительнее, так как она упрощает тестирование — компоненты можно легко подменить моками.

# ## **Middle Level**

**Наследование в Python:**

1. **Механизм:** Используется синтаксис `class Child(Parent1, Parent2):`. Поддерживается множественное наследование.
2. **Поиск атрибутов (MRO):** Python использует C3 Linearization для построения линеаризованного списка классов (Method
   Resolution Order), который определяет порядок поиска атрибутов и методов. Его можно посмотреть через
   `ClassName.__mro__`.
3. **Вызов родительских методов:** Для явного вызова метода родительского класса используется `super()`. При
   множественном наследовании `super()` работает в соответствии с MRO, что позволяет кооперативно вызывать методы
   родителей в предсказуемом порядке.
4. **Абстрактные классы:** Для создания интерфейсов, которые обязывают наследников реализовать определенные методы,
   используются ABC (Abstract Base Classes) с декоратором `@abstractmethod`.

**Композиция в Python:**

1. **Механизм:** Реализуется путем включения объектов других классов в качестве атрибутов. Это может быть сделано в
   конструкторе (`__init__`) или через setter-методы.
2. **Динамическое поведение:** Компоненты могут быть заменены во время выполнения, что позволяет динамически менять
   поведение объекта (паттерн "Стратегия").
3. **Управление жизненным циклом:** При композиции владеющий объект обычно управляет жизненным циклом компонентов (
   создает и уничтожает их). При агрегации (более слабая форма композиции) жизненные циклы независимы.

**Сравнение для AQA:**

* **Тестируемость:** Классы, построенные через композицию, легко тестировать, так как зависимости можно заглушить или
  замокать. Наследование же часто приводит к необходимости тестировать целые иерархии классов, а мокирование
  родительских методов становится сложнее.
* **Хрупкость базового класса (Fragile Base Class):** Изменения в родительском классе могут неожиданно сломать работу
  дочерних классов, даже если они не касались напрямую переопределенных методов. Это делает наследование рискованным.
* **Взрыв иерархии:** При активном использовании наследования для добавления новой функциональности часто приходится
  создавать новые классы на каждом уровне иерархии (проблема "ромбовидного наследования"), что ведет к сложности
  поддержки.

# ## **Senior Level**

**1. Наследование как нарушение инкапсуляции:**
Наследование — это самый сильный вид связи в ООП (white-box reuse). Дочерний класс получает доступ к защищенным (
`_protected`) атрибутам и методам родителя, тем самым нарушая инкапсуляцию родительского класса. Это создает скрытые
контракты: дочерний класс должен не только реализовывать публичный интерфейс, но и знать о внутренних деталях родителя.
Любое изменение этих внутренних деталей в родителе может сломать наследников. Для тестируемости это катастрофа: чтобы
понять поведение дочернего класса, нужно глубоко понимать поведение родителя, что резко увеличивает когнитивную нагрузку
и сложность изоляции.

**2. Композиция и протоколы:**
Вместо наследования для полиморфизма в Python всё чаще используется **композиция с протоколами** (Structural Subtyping).
Объект не обязан объявлять "я наследник абстрактного класса `Reader`", он должен просто реализовать метод `read()`. Это
проверяется статически (через `typing.Protocol` и mypy) или динамически (через `hasattr` или `isinstance` с
`@runtime_checkable`). Такой подход резко снижает связанность. В тестах мы можем передать любой объект, имеющий
`read()`, без необходимости строить иерархии наследования.

**3. Множественное наследование и Mixins:**
Python разрешает множественное наследование, что является мощным, но опасным инструментом. На практике его используют
для реализации **Mixins** — небольших классов, предоставляющих определенную функциональность (например, `LoggingMixin`,
`SerializationMixin`). Mixin не предназначен для самостоятельного использования, он добавляет методы в класс. Однако
если два Mixina определяют методы с одинаковыми именами, возникает конфликт, разрешаемый через MRO. Для тестового
фреймворка Mixins могут быть полезны для добавления общих утилит в тест-классы, но необходимо тщательно проектировать
их, чтобы избежать конфликтов и неявных зависимостей.

**4. Делегирование как альтернатива:**
Паттерн **Делегирование** (Delegation) — это явная форма композиции, где внешний объект (делегатор) передает запросы
внутреннему объекту (делегату). В Python его можно элегантно реализовать через метод `__getattr__`: если атрибут не
найден в самом объекте, запрос автоматически перенаправляется делегату. Это позволяет строить объекты-обертки (например,
адаптеры, прокси), которые полностью контролируют взаимодействие с внутренним объектом и обеспечивают точку для
внедрения моков в тестах.

**5. Для AQA:**

* **Тестирование наследуемых фикстур:** При использовании наследования для построения тестовых классов (например, в
  `unittest.TestCase`) изменение в базовом классе тестов может сломать сотни наследников. Решение — использовать *
  *композицию фикстур** (как в pytest), где каждая фикстура независима и явно запрашивается в тесте. Это дает
  детерминированное управление жизненным циклом и изоляцию.
* **Мокирование унаследованных методов:** Мокировать метод, который вызывается через `super()`, крайне сложно, потому
  что `super()` является статической конструкцией, разрешаемой во время компиляции. Приходится применять тяжеловесные
  патчи (`patch.object`) на конкретный класс, что ведет к хрупким тестам. Композиция позволяет просто подменить весь
  компонент.
* **Анализ покрытия для иерархий классов:** При наследовании инструменты покрытия кода (coverage.py) могут показывать
  вводящую в заблуждение статистику: строка кода в родительском классе может быть выполнена при тестировании дочернего,
  но это не означает, что она была протестирована во всех возможных контекстах. Ответственный AQA должен уметь
  интерпретировать такие отчеты и, возможно, требовать отдельного тестирования базовых классов.
* **Динамическое наследование и метаклассы:** В очень продвинутых фреймворках классы могут динамически наследоваться от
  других с помощью `type(name, bases, dict)` или метаклассов. Это позволяет генерировать специализированные классы на
  лету (например, для каждой тестовой конфигурации). Хотя это мощно, это усложняет отладку и понимание потока
  выполнения. Тесты для такого кода должны быть максимально простыми и изолированными, чтобы не усугублять сложность.
* **Наследование от встроенных типов:** В Python можно наследоваться от `list`, `dict` и других встроенных типов. Это
  может быть полезно для добавления поведения, но чревато тонкими ошибками, если не переопределены все relevant
  специальные методы (например, могут сломаться операции копирования или сериализации). При тестировании таких классов
  необходимо проверять не только добавленное поведение, но и что все унаследованные методы работают корректно в новом
  контексте.

- [Содержание](#содержание)

---

# **Метапрограммирование**

## **Junior Level*

Метапрограммирование в Python — это техника, когда программа может создавать или модифицировать другую программу,
включая саму себя, во время выполнения. Если обычное программирование работает с данными, то метапрограммирование
работает с кодом как с данными.

Простые примеры, которые вы уже используете:

- **Декораторы** — это функции, которые меняют поведение других функций или классов.
- **Динамическое создание классов** с помощью `type()`.
- **Изменение объектов на лету** (например, добавление новых атрибутов).

Для QA инженера метапрограммирование — мощный инструмент для создания гибких тестовых фреймворков, автоматической
генерации тестов, мокирования и построения DSL (предметно-ориентированных языков) для тестирования.

# ## **Middle Level**

Технически метаprogramming в Python охватывает несколько ключевых областей:

1. **Декораторы:** Синтаксический сахар для функций высшего порядка. Декоратор принимает функцию/класс и возвращает
   модифицированную версию. Могут быть вложенными, принимать аргументы, применяться к методам. Для AQA:
   `@pytest.fixture`, `@unittest.mock.patch`, `@pytest.mark.parametrize`.

2. **Метаклассы:** Классы — это объекты типа `type`. Метакласс — это класс классов, контролирующий создание классов.
   Позволяет перехватывать момент создания класса, модифицировать его атрибуты, методы, добавлять валидацию.
   Используется в ORM (Django), валидаторах данных.

3. **Дескрипторы:** Объекты с методами `__get__`, `__set__`, `__delete__`. Лежат в основе `@property`, методов классов,
   `__slots__`. Позволяют контролировать доступ к атрибутам.

4. **Динамическое выполнение кода:** `eval()` и `exec()` выполняют строки как код Python. Опасны, но мощны для DSL или
   конфигураций.

5. **Интроспекция:** Возможность исследовать объекты во время выполнения через `dir()`, `getattr()`, `hasattr()`, модуль
   `inspect`. Основа для автоматического обнаружения тестов, dependency injection.

6. **Monkey Patching:** Динамическое изменение модулей и классов в рантайме. Основа `unittest.mock`, но требует
   осторожности.

Для AQA: pytest использует метапрограммирование для фикстур, параметризации; unittest.mock для подмены объектов; можно
создавать кастомные маркеры, системы отчетности.

# ## **Senior Level**

Метапрограммирование — это философия проектирования систем, где код манипулирует структурой и поведением
во время выполнения.

1. **Метаклассы и `__init_subclass__`:**
    - Метакласс `ABCMeta` реализует ABC. Метаклассы могут конфликтовать при множественном наследовании — Python
      разрешает конфликты, требуя чтобы один метакласс был подклассом другого.
    - `__init_subclass__` — более простая альтернатива для модификации подклассов без полного метакласса. Вызывается при
      создании подкласса, позволяет регистрировать классы, добавлять атрибуты.

2. **Дескрипторы высшего порядка:**
    - Дескрипторы могут иметь состояние. Например, `property`, которое кэширует результат. Для этого дескриптор хранит
      данные в `instance.__dict__` под уникальным ключом (часто `_<descriptor_name>`).
    - **Неданные дескрипторы** (только `__get__`) и **данные дескрипторы** (есть `__set__`). Данные дескрипторы имеют
      приоритет над `instance.__dict__` при записи.
    - Можно создавать **ленивые дескрипторы**, которые вычисляют значение при первом обращении.

3. **Метапрограммирование с аннотациями типов:**
    - Аннотации доступны в `__annotations__`. Их можно использовать для автоматической валидации, сериализации,
      генерации схем.
    - `typing.get_type_hints()` возвращает аннотации с разрешенными forward references.
    - **Pydantic, FastAPI** используют аннотации для построения моделей и API. Для AQA — можно создавать
      само-валидирующиеся тестовые данные.

4. **Перехват создания модулей:**
    - Можно перехватывать импорт через **хуки в `sys.meta_path`**. Каждый элемент `meta_path` — это finder/loader.
      `importlib.abc.MetaPathFinder` позволяет подменять модули на лету, что используется для изоляции тестов (подмена
      `sys.path`), загрузки конфигов.

5. **Динамическое изменение MRO:**
    - MRO определяется при создании класса, но можно изменить через `__mro__` (только для чтения) или создать новый
      класс с другим порядком наследования.
    - `type.__new__` может переупорядочивать базовые классы.

6. **Для AQA:**
    - **Генерация тестов из данных:** Метакласс, который читает YAML/JSON конфиг и для каждого элемента генерирует
      тестовый метод с уникальным именем и параметрами. Это позволяет тестам быть декларативными.
    - **Автоматические моки:** Дескриптор, который при доступе к атрибуту `api.client` автоматически создает
      `unittest.mock.Mock` с сигнатурой, соответствующей аннотациям реального класса. Это дает автодополнение в IDE и
      безопасность.
    - **Трассировка вызовов в реальном времени:** Декоратор, оборачивающий каждый метод класса, который записывает
      вызовы, аргументы, возвращаемые значения и исключения. Полезно для отладки интеграционных тестов.
    - **Инъекция фикстур на уровне метакласса:** Метакласс, который анализирует аннотации методов класса и автоматически
      внедряет фикстуры перед вызовом тестовых методов, эмулируя поведение pytest, но для кастомного фреймворка.
    - **Динамическое создание классов для изоляции:** Каждый тест запускается в экземпляре класса, который создан
      динамически с уникальным набором моков. Это гарантирует, что состояния не пересекаются даже при параллельном
      запуске.
    - **Валидация архитектуры через декораторы:** Декоратор `@enforce_interface`, который проверяет, что класс реализует
      все методы протокола и их сигнатуры соответствуют ожидаемым. Использует `inspect.signature` и `get_type_hints`.
    - **Подмена стандартной библиотеки в тестах:** Через `sys.modules` подменять модули типа `time`, `random`, `uuid` на
      детерминированные версии, чтобы тесты были воспроизводимы.
    - **Сериализация сложных объектов для снимков (snapshot testing):** Дескриптор или метакласс, который автоматически
      генерирует сериализатор для класса на основе его аннотаций и `__slots__`/`__dict__`, и сравнивает состояние
      объекта со снимком.

7. **Опасности и ограничения:**
    - **Производительность:** Метапрограммирование добавляет накладные расходы. Динамическое создание классов, глубокий
      introspection замедляют код.
    - **Читаемость:** Код с активным метапрограммированием труден для понимания и отладки. Нужна отличная документация.
    - **Совместимость:** Динамические изменения могут сломаться при обновлении Python или сторонних библиотек.
    - **Тестирование самого метакода:** Требует особого подхода, часто с использованием тех же метатехник.

- [Содержание](#содержание)

---

# **Автоматизация**

## **Junior Level*

ООП в Python кажется простым и интуитивным, но содержит несколько неочевидных "подводных камней", которые могут
приводить к трудноуловимым ошибкам, особенно в сложных тестовых фреймворках и автоматизации.

Основные подводные камни:

1. **Изменяемые атрибуты класса** - если вы определите список или словарь как атрибут класса (а не экземпляра), все
   экземпляры будут разделять одну и ту же копию этого объекта.
2. **Неявное связывание self** - методы автоматически получают `self`, но при использовании методов как callback-функций
   можно забыть его привязать.
3. **Наследование от встроенных типов** - поведение может отличаться от ожидаемого, особенно при переопределении
   методов.
4. **Множественное наследование и MRO** - порядок разрешения методов может стать неожиданным в сложных иерархиях.
5. **Магические методы и операторы** - некоторые методы вызываются неявно, и их неправильная реализация может сломать
   стандартное поведение.

Для QA инженера эти нюансы критичны, так как могут вызывать недетерминированное поведение тестов, утечки памяти и
сложные для отладки проблемы в тестовых фреймворках.

# ## **Middle Level**

Технические детали основных подводных камней:

1. **Изменяемые атрибуты класса и экземпляра:**
    - Атрибуты, объявленные на уровне класса, являются общими для всех экземпляров. Если это изменяемый объект (список,
      словарь, множество), изменение через один экземпляр влияет на все.
    - Проблема возникает из-за того, что поиск атрибутов идет сначала в `instance.__dict__`, затем в `class.__dict__`.
      При присваивании `instance.attr = value` создается запись в `instance.__dict__`, но при обращении к `class.attr`
      модифицируется общий объект.

2. **Методы и bound/unbound состояния:**
    - Метод, извлеченный через экземпляр (`instance.method`), становится bound method с привязанным `self`.
    - Метод, извлеченный через класс (`Class.method`), остается unbound function.
    - Если передать `Class.method` как callback без явного связывания, вызов завершится ошибкой из-за отсутствия `self`.

3. **Наследование от встроенных типов и `super()`:**
    - Встроенные типы (`list`, `dict`, `str`) написаны на C и могут не вызывать переопределенные методы в подклассах
      ожидаемым образом.
    - `super()` в классах, наследующих от встроенных типов, может вести себя иначе, особенно при множественном
      наследовании.
    - Пример: переопределение `__init__` в наследнике `list` требует явного вызова `super().__init__()`.

4. **Множественное наследование и алгоритм C3:**
    - MRO (Method Resolution Order) определяется алгоритмом C3, который гарантирует сохранение порядка и отсутствие
      циклов.
    - Однако сложные диаграммы наследования могут приводить к неочевидному порядку вызовов.
    - Проблема "ромбовидного наследования" может вызывать многократный вызов одного и того же метода.

5. **Магические методы, не вызываемые неявно:**
    - Некоторые магические методы вызываются только для встроенных операций, но не для соответствующих функций.
    - Например, `len(x)` вызывает `x.__len__()`, но нет гарантии, что все контейнеры реализуют `__len__`.
    - `__del__` не является деструктором в классическом понимании и вызывается не детерминировано сборщиком мусора.

6. **Проблемы с `__hash__` и `__eq__`:**
    - При переопределении `__eq__` нужно переопределять `__hash__`, иначе объект станет нехешируемым.
    - Если `__eq__` возвращает `True` для объектов разных классов, их хеш-значения должны совпадать.

7. **Для AQA:**
    - Фикстуры с состоянием на уровне класса могут приводить к взаимовлиянию тестов.
    - Моки методов родительских классов требуют понимания MRO для правильного патчинга.
    - Сериализация объектов с циклическими ссылками может приводить к переполнению стека.

# ## **Senior Level**

Глубокий анализ архитектурных и системных подводных камней:

1. **Изменяемые атрибуты класса и дескрипторы:**
    - Проблема глубже, чем кажется. Дескрипторы данных (например, `property`) имеют приоритет над `instance.__dict__`.
    - При использовании `__slots__` механизм полностью меняется: объекты не имеют `__dict__`, атрибуты хранятся в
      массиве указателей.
    - **Последствие для тестирования:** Мокирование атрибутов в классах со `__slots__` невозможно стандартными
      средствами `unittest.mock`, требуется создание нового класса.

2. **Метод `__init_subclass__` и метаклассы:**
    - `__init_subclass__` вызывается при создании каждого подкласса, но если в иерархии есть метакласс, порядок вызова
      может быть неочевидным.
    - Метаклассы могут конфликтовать при множественном наследовании, вызывая `TypeError: metaclass conflict`.
    - **Для AQA:** При создании фреймворка с плагинами через наследование, неправильное сочетание метаклассов может
      сломать загрузку плагинов.

3. **Циклические ссылки и сборка мусора:**
    - Python использует комбинированный сборщик мусора (reference counting + generational GC).
    - Циклические ссылки между объектов с `__del__` никогда не будут собраны, так как GC не может определить порядок
      удаления.
    - Слабые ссылки (`weakref`) решают проблему, но требуют осторожности: `weakref.ref(obj)` возвращает прокси, который
      может стать `None` в любой момент.
    - **Для AQA:** Тесты, создающие графы объектов с циклическими ссылками, могут вызывать утечки памяти, которые
      проявляются только при длительных прогонах.

4. **Протоколы итераторов и генераторов:**
    - Итератор, исчерпавший себя, при последующих вызовах `__next__` всегда выбрасывает `StopIteration`.
    - В генераторах `return value` преобразуется в `StopIteration(value)`, что может быть неочевидно.
    - Асинхронные генераторы имеют отдельный жизненный цикл и требуют явного закрытия через `aclose()`, иначе могут
      оставаться незавершенными.
    - **Для AQA:** Тестирование генераторов требует проверки не только выдачи значений, но и корректного завершения.
      Незакрытые асинхронные генераторы в тестах могут приводить к предупреждениям `ResourceWarning`.

5. **Дескрипторы с состоянием и кэширование:**
    - Дескриптор, хранящий состояние в себе (а не в экземпляре), будет разделять это состояние между всеми экземплярами
      класса.
    - Паттерн кэширования через дескриптор требует использования `instance.__dict__` для хранения кэша, но это может
      конфликтовать с `__slots__`.
    - **Для AQA:** Мокирование property-дескрипторов требует глубокого понимания: `@property` создает дескриптор на
      уровне класса, поэтому патчить нужно именно его.

6. **Проблемы сериализации и `__getstate__`/`__setstate__`:**
    - По умолчанию `pickle` сохраняет `instance.__dict__`. Для объектов со `__slots__` это не работает.
    - `__getstate__` должен возвращать сериализуемое представление, но если он возвращает сложный граф объектов, могут
      возникнуть циклические ссылки.
    - `__setstate__` должен корректно восстанавливать состояние, включая несериализуемые ресурсы (файлы, сокеты).
    - **Для AQA:** Сериализация используется в распределенных тестовых системах. Объекты тестовых данных и фикстур
      должны быть сериализуемыми.

7. **Наследование от `dict` и `list`:**
    - Встроенные типы оптимизированы и могут обходить переопределенные методы.
    - Например, `dict.update()` может не вызывать переопределенный `__setitem__`.
    - Решение: наследоваться от `collections.UserDict` или `collections.UserList`, которые реализованы на Python и
      правильно вызывают переопределенные методы.
    - **Для AQA:** Кастомные коллекции для тестовых данных должны наследоваться от абстрактных классов
      `collections.abc`, а не от встроенных типов.

8. **Монкеи-патчинг и его последствия:**
    - Изменение классов или модулей в runtime влияет на все потоки выполнения.
    - В асинхронном коде это может вызывать состояние гонки.
    - Патчинг через `unittest.mock.patch` использует механизм `importlib`, но не всегда очищает кэши (`sys.modules`).
    - **Для AQA:** Параллельный запуск тестов с патчингом одних и тех же модулей может приводить к недетерминированному
      поведению. Нужно использовать изоляцию на уровне процесса.

9. **Проблема хрупкого базового класса (Fragile Base Class):**
    - Изменение приватного метода (`_method`) в базовом классе может сломать наследников, которые его используют.
    - Добавление публичного метода в базовый класс может случайно переопределить метод в наследнике (если у него было то
      же имя).
    - **Для AQA:** Тестовые базовые классы должны быть максимально стабильными или использовать композицию вместо
      наследования.

10. **`__prepare__` и пространства имен класса:**
    - Метакласс может определить метод `__prepare__`, который возвращает кастомный словарь для namespace класса.
    - Это позволяет, например, автоматически регистрировать все методы, но ломает ожидания о том, что `class.__dict__`
      является обычным словарем.
    - **Для AQA:** Фреймворки, использующие такие метаклассы, могут иметь неочевидное поведение при интроспекции
      тестовых классов.

11. **Проблемы с `__del__` и контекстные менеджеры:**
    - `__del__` может быть вызван в любой момент, даже во время сборки мусора при завершении интерпретатора.
    - Если в `__del__` происходит обращение к глобальным переменным или импортированным модулям, они могут быть уже
      `None`.
    - Контекстные менеджеры (`__enter__`/`__exit__`) более предсказуемы для управления ресурсами.
    - **Для AQA:** Тестовые фикстуры, управляющие ресурсами (браузер, БД), должны использовать контекстные менеджеры, а
      не `__del__`.

12. **Аннотации типов и `__init__`:**
    - Аннотации типов сохраняются в `__annotations__`, но не влияют на проверку во время выполнения.
    - `dataclasses` и Pydantic используют аннотации для генерации `__init__`, что может конфликтовать с явно
      определенным `__init__` в родительском классе.
    - **Для AQA:** Тестовые data-классы должны быть согласованы: либо использовать `@dataclass`, либо явный `__init__`,
      но не смешивать подходы.

- [Содержание](#содержание)

---

# **Миксины**

### ## **Junior Level**

Миксины (mixins) — это специальные классы в Python, предназначенные для добавления конкретной функциональности к другим
классам через множественное наследование. Если представить основной класс как основное блюдо, то миксины — это специи
или добавки, которые придают ему дополнительные свойства и возможности.

Миксины не предназначены для самостоятельного использования — они не являются полноценными объектами, а служат для "
подмешивания" методов и атрибутов к другим классам. Например, можно создать миксин `LoggingMixin`, который добавляет
методы логирования, и использовать его в разных классах, где нужна эта функциональность.

Для QA инженера миксины полезны при создании тестовых фреймворков и утилит: можно вынести общую функциональность (
например, работу с базой данных, генерацию тестовых данных, создание отчетов) в миксины и переиспользовать их в разных
тестовых классах.

## **Middle Level**

Технически миксины — это обычные классы Python, которые следуют определенным соглашениям при использовании в
множественном наследовании.

1. **Синтаксис и использование:**
    - Миксины включаются в цепочку наследования перед основным классом.
    - Обычно имеют суффикс `Mixin` в названии для ясности.
    - Не вызывают `super().__init__()` в своем `__init__`, если не предназначены для участия в MRO (Method Resolution
      Order) инициализации.

2. **Метод разрешения порядка (MRO):**
    - При множественном наследовании Python использует алгоритм C3 для определения порядка поиска методов.
    - Миксины должны быть спроектированы так, чтобы не конфликтовать с методами основных классов и других миксинов.
    - Важно правильно располагать миксины в списке наследования: обычно миксины идут слева перед основными классами.

3. **Особенности проектирования:**
    - **Одна ответственность:** Каждый миксин должен добавлять одну четкую функциональность.
    - **Независимость:** Миксины должны быть максимально независимы от деталей реализации классов, к которым они
      подмешиваются.
    - **Гибкость:** Миксины могут определять абстрактные методы (`@abstractmethod`), которые должны быть реализованы в
      основном классе.

4. **Для AQA:**
    - **Тестовые утилиты:** Создание миксинов для общих операций: `DatabaseMixin` (для работы с БД), `APIClientMixin` (
      для HTTP-запросов), `ScreenshotMixin` (для создания скриншотов при падении теста).
    - **Расширение Page Object:** Добавление функциональности к Page Object через миксины (например, `ModalDialogMixin`
      для работы с модальными окнами).
    - **Кастомизация тестовых классов:** В `unittest.TestCase` можно использовать миксины для добавления методов
      подготовки данных, ассертов.

5. **Примеры в экосистеме Python:**
    - Django использует миксины для CBV (Class-Based Views).
    - DRF (Django REST Framework) активно использует миксины для ViewSets.
    - В тестировании: `unittest.TestCase` можно расширять миксинами.

## **Senior Level**

1. **Миксины и линейная MRO:**
    - Алгоритм C3 гарантирует сохранение порядка и отсутствие циклов в наследовании.
    - При использовании нескольких миксинов их порядок в списке наследования критичен. Методы миксина, стоящего левее,
      имеют приоритет.
    - **Проблема diamond inheritance:** Если два миксина наследуются от одного базового класса, может возникнуть
      неочевидное поведение. C3 решает эту проблему, но важно понимать итоговый MRO.

2. **Миксины как протоколы дескрипторов:**
    - Миксины могут использовать дескрипторы для управления доступом к атрибутам. Например, миксин может добавлять
      property или управляемые атрибуты.
    - Дескрипторы в миксинах могут быть использованы для автоматической валидации данных, ленивой загрузки или
      кэширования.

3. **Динамическое подмешивание (монкей-патчинг vs наследование):**
    - В отличие от статического наследования, миксины можно "подмешивать" динамически с помощью `type()` с тремя
      аргументами:
      ```python
      DynamicClass = type('DynamicClass', (ExistingClass, SomeMixin), {})
      ```
    - Это позволяет создавать классы на лету с нужной комбинацией функциональности, что полезно в тестовых фреймворках
      для создания специализированных тестовых классов.
    - Однако такой подход усложняет отладку и анализ кода.

4. **Миксины и метаклассы:**
    - Миксины могут иметь свои метаклассы, что может привести к конфликту метаклассов при множественном наследовании.
    - Python разрешает конфликты, требуя чтобы один метакласс был подклассом другого. Если миксин использует метакласс,
      нужно убедиться в его совместимости.
    - Метаклассы миксинов могут использоваться для автоматической регистрации классов, добавления декораторов или
      изменения структуры класса.

5. **Для AQA:**
    - **Динамическая композиция тестовых классов:** В сложных тестовых фреймворках можно использовать фабрику классов,
      которая на основе конфигурации (например, тестируемой среды, типа браузера, нужных фич) динамически создает
      тестовый класс, подмешивая соответствующие миксины:
        - `SeleniumMixin` для UI-тестов.
        - `APITestMixin` для API-тестов.
        - `MobileTestingMixin` для мобильного тестирования.
        - `PerformanceMetricsMixin` для сбора метрик.
          Это позволяет избежать взрыва комбинаторного наследования.

    - **Миксины с зависимостями и инициализацией:** Создание миксинов, которые требуют определенной последовательности
      инициализации. Использование `__init_subclass__` для настройки класса при создании. Например, миксин может
      автоматически добавлять декораторы к методам класса, начинающимся с `test_`.

    - **Тестирование самих миксинов:** Написание тестов для миксинов — отдельная задача, так как они не предназначены
      для использования в изоляции. Стратегии:
        - Создание тестового класса-заглушки, который использует миксин.
        - Использование метаклассов для автоматического тестирования всех комбинаций миксинов.
        - Проверка, что миксин не ломает MRO основного класса.

    - **Миксины для cross-cutting concerns в тестах:**
        - `RetryMixin`: автоматически повторяет падающий тест (опасно, может маскировать проблемы).
        - `TimeoutMixin`: добавляет таймауты к тестам.
        - `IsolationMixin`: обеспечивает изоляцию тестов (создание уникальных данных, очистка состояния).
          Эти миксины могут переопределять методы выполнения тестов (`setUp`, `tearDown`, `runTest`), что требует
          глубокого понимания механизмов выполнения тестов во фреймворке.

    - **Конфликты и разрешение методов:** При использовании нескольких миксинов, определяющих методы с одинаковыми
      именами, можно реализовать механизм разрешения конфликтов:
        - Использование `super()` с явным указанием класса (но это ломает гибкость).
        - Создание миксина-медиатора, который координирует вызовы.
        - Использование декораторов для "сшивания" методов (например, вызывать оба метода).

    - **Миксины для интеграции с внешними системами:**
        - `AllureReportingMixin`: автоматически добавляет шаги и аттачменты в Allure-отчет.
        - `JiraIntegrationMixin`: создает баг-репорты в JIRA при падении теста.
        - `SlackNotificationMixin`: отправляет уведомления в Slack о начале/конце тестового прогона.
          Такие миксины часто требуют настройки (учетные данные, URL), что можно реализовать через атрибуты класса или
          конфигурационные объекты.

6. **Альтернативы миксинам в современном Python:**
    - **Композиция через классы-делегаты:** Вместо множественного наследования можно использовать композицию и
      делегирование. Это делает зависимости более явными, но требует больше кода.
    - **Протоколы и ABC:** Для определения интерфейсов лучше использовать абстрактные базовые классы или протоколы, а
      реализацию — через композицию.
    - **Декораторы классов:** Многие задачи, решаемые миксинами, можно реализовать через декораторы классов, которые
      модифицируют поведение.
    - **Хуки и плагины:** В pytest функциональность добавляется через фикстуры и плагины, что является более
      декларативным и гибким подходом, чем миксины.

7. **Опасности и антипаттерны:**
    - **God Mixin:** Миксин, который делает слишком много и становится монолитом.
    - **Неявные зависимости:** Миксин, который зависит от определенных атрибутов или методов в классе, но не декларирует
      это явно.
    - **Хрупкость MRO:** Изменение порядка миксинов может сломать работу класса.
    - **Тестируемость:** Классы с большим количеством миксинов сложно тестировать, так как приходится учитывать все
      подмешанные поведения.

- [Содержание](#содержание)

---

# **typing**

## **Junior Level*

`typing` модуль в Python предоставляет инструменты для добавления подсказок типов (type hints) в код. Это не меняет
поведение программы во время выполнения, но помогает разработчикам, IDE и статическим анализаторам понимать, какие типы
данных ожидаются.

**Optional[X]** означает, что значение может быть либо типа `X`, либо `None`. Это удобный способ сказать "этот аргумент
может быть передан, а может и нет".

**Union[X, Y, ...]** означает, что значение может быть одного из перечисленных типов. Например, `Union[int, str]` — это
либо целое число, либо строка.

**TypeVar** используется для создания обобщенных (generic) типов. Например, если у вас есть функция, которая работает со
списками любого типа, вы можете использовать `TypeVar` чтобы показать, что тип элементов входного списка и выходного
значения одинаков.

**Generic** — это способ создавать классы или функции, которые могут работать с разными типами, но сохранять информацию
о конкретном типе. Например, `List[int]` — это список целых чисел, а `List[str]` — список строк. `Generic` позволяет вам
создавать свои собственные классы, которые могут быть параметризованы типами.

# ## **Middle Level**

Технически, эти конструкции являются частью системы типизации, которая реализована в модуле `typing` и поддерживается
статическими анализаторами (mypy, pyright, PyCharm).

1. **Optional и Union:**
    - `Optional[X]` это просто сокращение для `Union[X, None]`.
    - `Union` может использоваться для любого количества типов. В Python 3.10 появился синтаксис `X | Y` как
      альтернатива `Union[X, Y]`.
    - Важно: `Optional` и `Union` не выполняют проверку типов во время выполнения. Они только для статического анализа.
    - Для проверки в runtime можно использовать `isinstance()` с кортежем типов, но это не связано напрямую с
      аннотациями.

2. **TypeVar:**
    - Создается вызовом `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`.
    - **Ограничения (constraints):** TypeVar может быть ограничен конкретными типами. Тогда он может представлять только
      один из них.
    - **Связывание (bound):** TypeVar может быть связан с базовым классом или протоколом. Тогда он представляет любой
      подтип этого базового класса.
    - **Ковариантность/контравариантность:** Позволяют выразить отношения между производными типами. Например, если `C`
      ковариантен по `T`, то `C[Dog]` является подтипом `C[Animal]` (при условии, что `Dog` подтип `Animal`). Это важно
      для корректного определения подтипов в обобщенных классах.

3. **Generic:**
    - Класс, наследующийся от `Generic[T]`, где `T` — это TypeVar, становится обобщенным.
    - Можно использовать несколько TypeVar: `Generic[T, U]`.
    - Внутри класса можно использовать `T` как обычный тип в аннотациях методов и атрибутов.
    - При наследовании от обобщенного класса можно либо конкретизировать тип (`ChildClass[int]`), либо передать TypeVar
      дальше.

4. **Для AQA:**
    - Type hints помогают документировать интерфейсы тестовых утилит и фикстур.
    - `Optional` часто используется для необязательных аргументов в функциях-фикстурах.
    - `Union` полезен, когда функция может возвращать разные типы данных в зависимости от условий (например,
      `Union[WebElement, None]` при поиске элемента).
    - `Generic` и `TypeVar` позволяют создавать гибкие, переиспользуемые компоненты тестовых фреймворков, например,
      абстрактный репозиторий для тестовых данных `Repository[T]`, где `T` — тип модели.

# ## **Senior Level**

1. **Optional и Union как алгебраические типы:**
    - В теории типов `Union` является суммой типов (sum type), а кортеж — произведением типов (product type). Это основа
      для построения сложных типов данных.
    - `Optional` — это частный случай суммы типа с единичным типом (`None`).
    - В Python 3.10 появился `TypeGuard` и `isinstance` с `Union` теперь работает лучше, но все еще есть ограничения.

2. **TypeVar и вариативность:**
    - **Инвариантность по умолчанию:** `List[T]` инвариантен: `List[Dog]` не является ни подтипом, ни надтипом
      `List[Animal]`, потому что список можно изменять. Это предотвращает ошибки типа "добавление кота в список собак".
    - **Ковариантность:** `Sequence[T]` (как и `Iterable[T]`) может быть ковариантен, потому что он только читает
      данные. Если `Dog` подтип `Animal`, то `Sequence[Dog]` подтип `Sequence[Animal]`.
    - **Контравариантность:** `Callable[[Animal], ...]` может быть контравариантен по аргументам. Функция, которая
      принимает `Animal`, может принимать и `Dog`, поэтому `Callable[[Animal], ...]` является подтипом
      `Callable[[Dog], ...]` (обратное отношение).
    - Правильное указание вариативности в пользовательских дженериках критично для корректной работы статических
      анализаторов.

3. **Generic и `__class_getitem__`:**
    - Когда вы пишете `List[int]`, это не создает новый класс. Вместо этого вызывается метод `__class_getitem__` класса
      `List`, который возвращает объект-алиас с сохраненной информацией о параметре типа.
    - Пользовательские классы могут реализовать `__class_getitem__` для поддержки такой нотации, не наследуясь от
      `Generic`. Но наследование от `Generic` автоматически предоставляет эту реализацию и другие возможности.
    - `Generic` использует метаклассы и магию, чтобы сохранить параметры типов в `__parameters__` и `__args__`.

4. **TypeVar и `NewType`:**
    - `NewType` создает непрозрачный псевдоним типа, который статически считается подтипом исходного типа, но в runtime
      это отдельный тип. Полезно для предотвращения логических ошибок (например, `UserId = NewType('UserId', int)`).
    - `TypeVar` же используется для параметризации, а не создания новых типов.

5. **Для AQA:**
    - **Создание типобезопасных DSL для тестов:** Используя `Generic` и `TypeVar`, можно создать DSL, который статически
      проверяет цепочки вызовов. Например, `browser.element(by.id("login")).click()` — если `element` возвращает
      `WebElement[T]`, где `T` — это тип элемента (кнопка, поле ввода), то можно гарантировать, что методы `click()` или
      `send_keys()` доступны только для соответствующих типов.
    - **Генерация тестов на основе типов:** Метакласс может анализировать аннотации типов тестовых методов и
      автоматически генерировать тесты для разных типов данных. Например, для параметризованного теста
      `test_sort[T: (int, str)]` можно сгенерировать два теста.
    - **Валидация конфигураций тестов:** Используя `TypedDict` и `Literal`, можно статически проверять конфигурационные
      файлы тестов (YAML/JSON) на соответствие схеме.
    - **Мокирование с проверкой типов:** Создание моков, которые наследуются от `Generic[T]` и проверяют, что
      возвращаемые значения соответствуют аннотациям. Можно использовать `__annotations__` реального класса для
      автоматической настройки мока.
    - **Протоколы и дженерики:** Комбинирование `Protocol` и `Generic` для описания обобщенных интерфейсов. Например,
      `class Repository[T, U]: ...` — где `T` — тип модели, `U` — тип идентификатора. Это позволяет создавать
      типобезопасные абстракции для тестовых данных.
    - **Зависимые типы (эмуляция):** Хотя Python не поддерживает зависимые типы полноценно, можно использовать `Literal`
      и `TypeVar` с bound для эмуляции простых случаев. Например, функция, которая возвращает длину списка, может быть
      аннотирована так, чтобы статический анализатор понимал, что индекс не выйдет за границы.
    - **Проблемы с рекурсивными типами:** Определение рекурсивных структур данных (например, дерева) требует
      использования строковых аннотаций (`'Tree'`) или `from __future__ import annotations`. В тестовых фреймворках это
      может возникать при описании сложных JSON-схем.
    - **Производительность:** Использование `typing` модуля может замедлить запуск программы, потому что он содержит
      много сложной логики. В продакшн-коде иногда используют `from typing import TYPE_CHECKING` для условного импорта.
      В тестах это менее критично, но стоит помнить.

6. **Ограничения и будущее:**
    - **PEP 563 (Postponed Evaluation of Annotations):** Строковые аннотации по умолчанию с Python 3.10 (в 3.7—3.9 через
      `from __future__ import annotations`). Это решает проблему forward references, но делает аннотации недоступными
      для интроспекции в runtime (нужно использовать `get_type_hints`).
    - **PEP 646 (Variadic Generics):** В Python 3.11 добавлены `TypeVarTuple` и `Unpack` для работы с произвольным
      количеством типов (например, для многомерных массивов).
    - **PEP 675 (LiteralString):** Для более точной типизации строковых литералов, полезно в SQL-запросах или командах
      shell, чтобы предотвранить injection.
    - **PEP 655 (Required/NotRequired для TypedDict):** Для указания обязательных и необязательных ключей.

- [Содержание](#содержание)

---

# **Literal, TypedDict, Protocol**

## **Junior Level**

**Literal** позволяет указать конкретное значение, которое может принимать переменная. Например,
`Literal["GET", "POST"]` означает, что параметр может быть только строкой "GET" или "POST". Это полезно для ограничения
допустимых значений.

**TypedDict** позволяет описывать структуру словаря с конкретными ключами и типами их значений. Это как схема для
словаря. Например, можно описать тип пользователя: `{"name": str, "age": int}`. Статические анализаторы будут проверять,
что словарь имеет именно такую структуру.

**Protocol** (о котором уже говорили) позволяет описывать интерфейсы через набор методов и атрибутов, которые должны
быть у объекта. Объект считается соответствующим протоколу, если у него есть все указанные методы и атрибуты, даже если
он не наследуется от протокола явно.

Для QA инженера эти инструменты помогают четко описывать форматы тестовых данных, конфигураций и ожидаемых структур
ответов, а также создавать типобезопасные моки и заглушки.

# ## **Middle Level**

Рассмотрим технические особенности каждой конструкции:

1. **Literal:**
    - Введен в PEP 586 (Python 3.8).
    - Может содержать конкретные значения: строки, числа, булевы значения, `None`.
    - Часто используется с `Union` для ограничения допустимых значений параметра.
    - Пример: `def request(method: Literal["GET", "POST", "PUT"]) -> Response: ...`
    - Статические анализаторы проверяют, что передаваемые значения входят в указанный набор.

2. **TypedDict:**
    - Введен в PEP 589 (Python 3.8).
    - Определяется через наследование от `TypedDict` или с использованием синтаксиса `TypedDict()`.
    - Поддерживает обязательные и необязательные ключи (через `total=False` и `NotRequired` в Python 3.11+).
    - Пример:
      ```python
      class User(TypedDict):
          name: str
          age: int
          email: NotRequired[str]  # Python 3.11+
      ```
    - В рантайме `TypedDict` ведет себя как обычный `dict`, но статические анализаторы используют его для проверки
      структуры.

3. **Protocol:**
    - Введен в PEP 544 (Python 3.8).
    - Определяет интерфейс через аннотации методов и атрибутов.
    - Может быть параметризован с помощью `Generic`.
    - Декоратор `@runtime_checkable` позволяет использовать `isinstance()` для проверки соответствия протоколу (но
      проверяет только наличие атрибутов, не их типы).
    - Пример:
      ```python
      class SupportsClose(Protocol):
          def close(self) -> None: ...
      ```

4. **Для AQA:**
    - **Literal:** Типизация параметров тестов (например, имена браузеров, окружения).
    - **TypedDict:** Описание форматов JSON-ответов API, конфигураций тестового окружения, фикстур.
    - **Protocol:** Определение интерфейсов для зависимостей, которые нужно мокировать. Создание абстракций для тестовых
      утилит.

# ## **Senior Level**

Глубокий анализ внутренней механики и продвинутое применение:

1. **Literal и зависимые типы:**
    - `Literal` может использоваться для уточнения типов в зависимости от других значений (эмуляция зависимых типов).
    - В сочетании с `@overload` и `TypeGuard` позволяет создавать точные аннотации для функций с ветвлением.
    - **Проблема:** `Literal` значения не считаются подтипами базового типа (`Literal["GET"]` не подтип `str`) для
      статического анализатора, что может привести к неожиданностям.
    - **Для AQA:** Использование `Literal` для параметризации тестов гарантирует, что тестовые значения принадлежат
      допустимому набору. Можно создавать перечисления тестовых сценариев.

2. **TypedDict и динамическая валидация:**
    - `TypedDict` использует метаклассы для создания классов-схем.
    - Атрибуты `__required_keys__` и `__optional_keys__` доступны для интроспекции.
    - **Проблема:** Наследование `TypedDict` работает, но требует осторожности при переопределении ключей.
    - **Интеграция с runtime-валидацией:** Библиотеки типа `pydantic` или `marshmallow` могут использовать `TypedDict`
      для генерации схем валидации.
    - **Для AQA:** Создание самодокументирующихся и валидируемых конфигураций тестового фреймворка. Автогенерация
      тестовых данных на основе схем.

3. **Protocol и метаклассы:**
    - `Protocol` наследуется от `ABCMeta`, что делает его метаклассом.
    - Декоратор `@runtime_checkable` добавляет метод `__instancecheck__`, который проверяет наличие атрибутов через
      `hasattr()`.
    - **Generic протоколы:** Позволяют создавать параметризованные интерфейсы. Например,
      `class Repository[T](Protocol): ...`.
    - **Проблема:** `@runtime_checkable` не проверяет сигнатуры методов и типы атрибутов. Это ограничение.
    - **Для AQA:** Создание библиотеки типобезопасных моков. Мок, созданный на основе протокола, будет иметь все
      необходимые методы, и IDE сможет предоставлять автодополнение.

4. **Комбинирование конструкций:**
    - Создание сложных типов: `TypedDict`, где некоторые значения имеют тип `Literal`.
    - Протоколы, требующие наличия атрибутов определенного `TypedDict` типа.
    - Пример: `class APIResponse(TypedDict): status: Literal["success", "error"]`.

5. **Ограничения и обходные пути:**
    - **Производительность:** Большое количество `TypedDict` и `Protocol` может замедлить статический анализ.
    - **Динамические ключи:** `TypedDict` не подходит для словарей с динамическими ключами. Для этого есть
      `typing.TypedDict` с `total=False`.
    - **Рекурсивные типы:** Для рекурсивных `TypedDict` или `Protocol` требуются строковые аннотации.

6. **Для AQA:**
    - **Создание DSL для тестов:** Используя `Protocol` и `Literal`, можно построить DSL, который статически проверяет
      корректность цепочек вызовов. Например, `page.form().fill().submit()` — если `form()` возвращает объект,
      соответствующий протоколу `Form`, то доступны только методы `fill()` и `submit()`.
    - **Генерация тестов из OpenAPI/Swagger:** На основе схемы API (которая сама по себе `TypedDict`-подобна) можно
      генерировать типизированные клиенты и тесты.
    - **Валидация тестовых сценариев:** Статическая проверка, что тестовые сценарии (описанные как `TypedDict`)
      соответствуют ожидаемой структуре. Например, сценарий теста UI:
      `{"action": Literal["click", "fill"], "element": str, ...}`.
    - **Мокирование внешних сервисов:** Создание моков, которые точно соответствуют протоколу внешнего сервиса.
      Использование `TypedDict` для описания форматов запросов и ответов.
    - **Статический анализ тестовых данных:** Написание скриптов, которые с помощью `TypedDict` проверяют, что тестовые
      данные (JSON, YAML) соответствуют ожидаемой структуре. Интеграция этой проверки в CI/CD.
    - **Протоколы для плагинов:** Описание интерфейсов плагинов для тестового фреймворка через `Protocol`. Это позволяет
      сторонним плагинам быть типобезопасными.
    - **Селекторы и локаторы:** Использование `Literal` для типизации селекторов: `by: Literal["id", "xpath", "css"]`.

7. **Будущее:**
    - **PEP 655 (NotRequired/Required):** Улучшает работу с необязательными ключами в `TypedDict`.
    - **PEP 681 (Data Class Transforms):** Упрощает создание классов, похожих на `TypedDict`.
    - **PEP 692 (TypedDict для **kwargs):** Позволит типизировать `**kwargs` с помощью `TypedDict`.

- [Содержание](#содержание)

---

# **Ковариантность, контравариантность**

## **Junior Level*

Ковариантность и контравариантность — это понятия из теории типов, которые описывают, как отношения между типами (
например, "является подтипом") переносятся на более сложные типы, содержащие эти типы (например, на списки или функции).

Представьте, что у вас есть иерархия классов: `Animal` -> `Dog` (собака — это животное). Теперь рассмотрим контейнеры:

- **Ковариантность** означает, что если `Dog` является подтипом `Animal`, то `Container[Dog]` является подтипом
  `Container[Animal]`. То есть контейнер сохраняет направление отношения. Это безопасно, если контейнер только читает
  данные (например, итератор).
- **Контравариантность** означает обратное: если `Dog` является подтипом `Animal`, то `Container[Animal]` является
  подтипом `Container[Dog]`. То есть контейнер инвертирует отношение. Это имеет смысл для контейнеров, которые только
  записывают данные (например, функция, которая принимает аргументы).

Простой пример: если у вас есть функция, которая может принять любое `Animal`, то она может принять и `Dog` (
контравариантность по аргументу). Если функция возвращает `Dog`, то эта возвращаемая собака также является `Animal` (
ковариантность по возвращаемому значению).

# ## **Middle Level**

Технически, ковариантность и контравариантность определяются в параметризованных (generic) типах. В Python они задаются
с помощью `TypeVar` с соответствующими флагами.

1. **Инвариантность (по умолчанию):**
    - Если `TypeVar('T')` объявлен без указания `covariant` или `contravariant`, то `Generic[T]` инвариантен.
    - Это значит, что `Container[Dog]` не является ни подтипом, ни надтипом `Container[Animal]`, даже если `Dog` —
      подтип `Animal`.
    - Пример: `List[T]` инвариантен, потому что список можно и читать, и изменять. Если бы он был ковариантен, то можно
      было бы присвоить `List[Dog]` переменной типа `List[Animal]` и добавить `Cat`, что привело бы к ошибке типа.

2. **Ковариантность:**
    - Объявляется как `TypeVar('T', covariant=True)`.
    - Пример: `Iterable[T]` ковариантен, потому что он только производит (yield) значения типа `T`. Если `Dog` подтип
      `Animal`, то `Iterable[Dog]` можно использовать везде, где требуется `Iterable[Animal]`.

3. **Контравариантность:**
    - Объявляется как `TypeVar('T', contravariant=True)`.
    - Пример: `Callable[[T], ...]` контравариантен по параметру `T`. Функция, которая может обработать любое `Animal`,
      может обработать и `Dog`. Поэтому `Callable[[Animal], ...]` является подтипом `Callable[[Dog], ...]`.

4. **Правила вариативности:**
    - Ковариантные типовые параметры могут использоваться только в "выходных" позициях (например, возвращаемый тип
      метода).
    - Контравариантные типовые параметры могут использоваться только во "входных" позициях (например, аргументы метода).
    - Инвариантные параметры могут использоваться и там, и там.

5. **Для AQA:**
    - При создании собственных generic-классов для тестовых фреймворков (например, `Repository[T]`) важно правильно
      указать вариативность, чтобы обеспечить типобезопасность.
    - Понимание вариативности помогает правильно аннотировать моки и стабы, особенно когда они используются в
      полиморфных контекстах.

# ## **Senior Level**

1. **Теоретические основы и Liskov Substitution Principle (LSP):**
    - Вариативность — это формальное выражение принципа подстановки Барбары Лисков для параметризованных типов.
    - Ковариантность гарантирует, что если `A` — подтип `B`, то `F[A]` — подтип `F[B]` (при условии, что `F`
      ковариантен).
    - Контравариантность гарантирует, что если `A` — подтип `B`, то `F[B]` — подтип `F[A]`.
    - Эти правила обеспечивают безопасность типов при использовании полиморфизма.

2. **Система типов Python и вариативность:**
    - Python использует структурную типизацию для протоколов, что делает вариативность еще более важной.
    - Например, протокол `SupportsLessThan[T]` может быть ковариантен по `T`, потому что если `Dog` можно сравнивать, то
      и `Animal` (если `Dog` — подтип `Animal`) можно сравнивать? На самом деле, это не всегда так, и поэтому
      вариативность должна быть определена аккуратно.
    - Статические анализаторы (mypy, pyright) проверяют вариативность и выдают ошибки, если ковариантный параметр
      появляется в контравариантной позиции (и наоборот).

3. **Вариативность и исключения:**
    - В Python исключения ковариантны. Это означает, что `Exception[Dog]` является подтипом `Exception[Animal]`. Это
      безопасно, потому что исключения обычно только читаются (их ловят и обрабатывают).
    - Однако, если бы исключения могли изменяться, это могло бы привести к проблемам. Но поскольку исключения в Python
      неизменяемы (immutable) после создания, ковариантность безопасна.

4. **Практические проблемы и обходные пути:**
    - **Проблема с изменяемыми данными:** Изменяемые контейнеры не могут быть ковариантными или контравариантными,
      потому что это привело бы к нарушению типобезопасности. Поэтому `list` инвариантен.
    - **Обходной путь:** Использование неизменяемых типов, таких как `Sequence` (ковариантен) или `Mapping` (ковариантен
      по значениям, но инвариантен по ключам).
    - **Self-тип:** Паттерм `Self` (введенный в Python 3.11) позволяет методам возвращать экземпляр текущего класса, что
      полезно для fluent-интерфейсов. `Self` является ковариантным.

5. **Для AQA:**
    - **Создание типобезопасных API для тестов:** При проектировании фреймворка для тестирования API можно использовать
      ковариантность для представления иерархий ответов. Например, `APIResponse[Success]` и `APIResponse[Error]` могут
      быть подтипами `APIResponse[Any]`. Это позволяет писать общие обработчики ответов.
    - **Мокирование с вариативностью:** При создании моков для generic-интерфейсов важно учитывать вариативность.
      Например, если интерфейс `Repository[T]` инвариантен, то мок для `Repository[Dog]` не может быть использован там,
      где ожидается `Repository[Animal]`, даже если `Dog` — подтип `Animal`. Это может потребовать создания отдельных
      моков для каждого типа.
    - **Параметризация тестов:** Используя вариативность, можно создавать параметризованные тесты, которые безопасно
      работают с иерархиями типов. Например, тест для `Animal` должен работать и для `Dog`, если только тест не изменяет
      объект (в этом случае требуется инвариантность).
    - **Статическая проверка тестовых сценариев:** С помощью правильной вариативности в аннотациях можно заставить
      статический анализатор проверять, что тестовые данные соответствуют ожидаемым типам, даже при использовании
      полиморфизма.
    - **Проблема с двойной вариативностью:** Некоторые структуры данных, такие как `Comparable`, могут требовать, чтобы
      тип был одновременно и ковариантным, и контравариантным. Это невозможно в большинстве систем типов. В Python это
      решается через протоколы с `Self` или через перегрузку (overloading).
    - **Использование `typing.covariant_check` и `contravariant_check`:** Эти декораторы (гипотетические) могли бы
      использоваться для проверки вариативности в runtime, но в стандартной библиотеке их нет. Однако можно создать свои
      проверки, используя `isinstance` и `issubclass`.

6. **Ограничения и будущее:**
    - **Производительность:** Проверка вариативности во время статического анализа может быть сложной и замедлять
      анализ.
    - **Динамическая природа Python:** В runtime информация о типах стирается (type erasure), поэтому вариативность
      важна только для статической проверки.
    - **PEP 484 и последующие:** Постепенное улучшение поддержки вариативности в системе типов Python.

- [Содержание](#содержание)

---

# **Pytest**

## **Junior Level*

Pytest — это современный фреймворк для тестирования в Python, который делает написание и выполнение тестов простым,
интуитивным и эффективным. В отличие от стандартного модуля unittest, pytest требует меньше шаблонного кода и предлагает
более мощные возможности.

Ключевые преимущества:

1. **Простой синтаксис:** Тесты пишутся как обычные функции с префиксом `test_`, а проверки — с помощью оператора
   `assert`. Не нужно запоминать множество методов вроде `assertEqual`, `assertTrue`.
2. **Фикстуры (Fixtures):** Механизм для подготовки и очистки тестового окружения. Фикстуры объявляются декоратором
   `@pytest.fixture` и могут использоваться в тестах путем указания их имен в параметрах функции.
3. **Параметризация:** Легко запустить один тест с разными наборами данных с помощью декоратора
   `@pytest.mark.parametrize`.
4. **Плагины:** Богатая экосистема плагинов расширяет возможности pytest (например, `pytest-cov` для измерения покрытия,
   `pytest-xdist` для параллельного запуска).
5. **Подробные отчеты:** При падении теста pytest предоставляет детальную информацию, что упрощает отладку.

# ## **Middle Level**

Технически pytest — это сложная система, построенная вокруг нескольких ключевых концепций:

1. **Архитектура запуска:**
    - Основная точка входа — функция `pytest.main()`, которая вызывает внутренний механизм, собирающий тесты,
      выполняющий их и формирующий отчет.
    - Процесс включает **обнаружение тестов** (сканирование файлов и каталогов по соглашениям), **сбор тестов** (
      создание объектов `Item` для каждого теста) и **выполнение** (запуск тестов с учетом фикстур и хуков).

2. **Фикстуры (Fixtures):**
    - Это функции, помеченные декоратором `@pytest.fixture`. Они могут возвращать данные или объекты, которые затем
      передаются в тестовые функции.
    - **Области видимости (scope):** Фикстуры могут быть уровня функции, класса, модуля или сессии, что определяет, как
      часто они создаются и уничтожаются.
    - **Зависимости фикстур:** Фикстуры могут зависеть от других фикстур, образуя граф зависимостей, который pytest
      разрешает автоматически.
    - **Finalizer и yield:** Фикстуры могут использовать `yield` для разделения кода установки и очистки (альтернатива
      `addfinalizer`).

3. **Хуковая система (Hook System):**
    - Pytest построен на плагинах, которые могут перехватывать и изменять его поведение через **хуки**. Хуки определены
      в `pytest.hookspec` и реализуются плагинами.
    - Примеры хуков: `pytest_collection_modifyitems` (для изменения списка тестов), `pytest_runtest_setup` (выполняется
      перед каждым тестом).
    - Эта система позволяет глубоко кастомизировать процесс тестирования.

4. **Маркеры (Markers):**
    - Декоратор `@pytest.mark` позволяет помечать тесты для выборочного запуска (например, `@pytest.mark.slow`).
    - Можно создавать собственные маркеры и использовать их для фильтрации или передачи метаданных.

5. **Параметризация:**
    - `@pytest.mark.parametrize` создает несколько тестовых случаев из одной функции. Каждый набор параметров становится
      отдельным тестовым случаем в отчете.

6. **Для AQA:**
    - **Интеграция с Allure:** Генерация детальных отчетов через `pytest-allure`.
    - **Параллельный запуск:** Использование `pytest-xdist` для ускорения выполнения тестов.
    - **Мокирование:** Хотя pytest сам не предоставляет моки, он хорошо интегрируется с `unittest.mock`.

# ## **Senior Level**

Глубокое погружение во внутреннюю механику pytest, расширяемость и интеграция с экосистемой Python.

1. **Архитектура на основе плагинов:**
    - Сам pytest реализован как набор **встроенных плагинов**. Даже базовые функции, такие как сбор тестов и выполнение,
      являются плагинами.
    - **Загрузка плагинов:** Плагины могут быть зарегистрированы через `setup.cfg`, `pyproject.toml`, аргументы
      командной строки или автоматически обнаружены через setuptools entry points.
    - **Конфликты плагинов:** При наличии нескольких плагинов, переопределяющих одни и те же хуки, порядок их выполнения
      определяется порядком загрузки и может быть критичным.

2. **Механизм фикстур: как работает внедрение зависимостей:**
    - **Разрешение зависимостей:** При обнаружении тестовой функции pytest анализирует ее сигнатуру и ищет имена
      параметров среди зарегистрированных фикстур. Затем он строит **граф зависимостей** и определяет порядок создания
      фикстур, учитывая их области видимости.
    - **Кэширование фикстур:** Фикстуры с областью видимости выше, чем `function`, кэшируются. Например, фикстура
      `session` создается один раз и переиспользуется. Это реализовано через объект `FixtureCache`.
    - **Время жизни фикстур и очистка:** При использовании `yield` в фикстуре, код после `yield` выполняется в момент
      окончания области видимости. Это не просто генератор, а специальная обработка внутри pytest, которая гарантирует
      выполнение финализатора даже при падении теста.

3. **Сбор тестов и интроспекция:**
    - **Объекты Node:** Pytest представляет тестовую сессию как дерево объектов: `Session` -> `Collection` -> `Item` (
      тест). Каждый узел соответствует каталогу, файлу, классу или функции.
    - **Плагин `pytest_pycollect`:** Отвечает за сбор тестов из Python-модулей. Он использует интроспекцию для поиска
      функций и классов, соответствующих шаблонам.
    - **Кастомизация сбора:** Можно написать плагин, который изменяет процесс сбора, например, добавляет тесты из
      не-Python файлов или динамически генерирует тесты на основе внешних данных.

4. **Хуковая система и взаимодействие с event loop:**
    - **Асинхронная поддержка:** Pytest поддерживает асинхронные тесты через плагин `pytest-asyncio`. Он предоставляет
      фикстуру `event_loop` и маркер `@pytest.mark.asyncio`.
    - **Внутренности:** Асинхронные фикстуры и тесты требуют особой обработки, так как хуки pytest сами не являются
      асинхронными. Плагин `pytest-asyncio` использует свой собственный цикл событий и оборачивает асинхронные функции.

5. **Для AQA:**
    - **Динамическое создание фикстур:** В продвинутых сценариях можно создавать фикстуры программно во время
      выполнения, используя `pytest.fixture` как обычную функцию и регистрируя ее через `request.addfixturedef`. Это
      позволяет создавать фикстуры на основе конфигурации или внешних данных.
    - **Кастомизация отчетов через хуки:** Хуки `pytest_runtest_makereport` и `pytest_terminal_summary` позволяют
      перехватывать результаты тестов и формировать кастомные отчеты, интегрируясь с внешними системами (например,
      TestRail, JIRA).
    - **Распределенное тестирование:** Используя `pytest-xdist`, можно распределять тесты по нескольким worker'ам. При
      этом каждая worker-нода имеет свою копию фикстур сессии, что может привести к избыточному созданию ресурсов. Для
      оптимизации можно использовать фикстуры с областью `"worker"` (только в xdist).
    - **Интеграция с контейнеризацией:** Запуск тестов в Docker-контейнерах с помощью кастомного плагина, который через
      хук `pytest_configure` поднимает инфраструктуру, а через `pytest_unconfigure` — останавливает.
    - **Мокирование на уровне импорта:** Плагины могут использовать `importlib` и `sys.modules` для подмены модулей до
      начала сбора тестов, что полезно для изоляции тестов от внешних зависимостей.
    - **Профилирование тестов:** Написание плагина, который с помощью `cProfile` или `py-spy` собирает метрики
      производительности для каждого теста и выводит топ медленных тестов.
    - **Валидация тестовых данных:** Использование хука `pytest_collection_modifyitems` для проверки, что все тесты
      имеют определенные маркеры или что параметризованные тесты используют валидные данные.
    - **Селективный запуск на основе изменений:** Плагин, который интегрируется с Git и анализирует diff, чтобы
      запускать только тесты, связанные с измененными файлами.
    - **Работа с базами данных:** Создание фикстур, которые управляют транзакциями БД, откатывают изменения после
      каждого теста (используя `rollback`) или применяют миграции для тестовой базы.

6. **Ограничения и граничные случаи:**
    - **Циклические зависимости фикстур:** Pytest обнаружит цикл и выдаст ошибку. Нужно перепроектировать фикстуры,
      введя третью, или использовать ленивую инициализацию.
    - **Параллелизм и состояние:** При использовании `pytest-xdist` нужно избегать разделяемого изменяемого состояния
      между worker'ами. Фикстуры `session` создаются в каждом worker отдельно.
    - **Моники-патчинг и фикстуры:** Если фикстура делает monkeypatch, то важно убедиться, что патч применяется и
      откатывается в правильной области видимости. Использование встроенной фикстуры `monkeypatch` решает эту проблему.
    - **Наследование фикстур в классах:** Фикстуры, определенные в родительском классе, не автоматически доступны в
      дочерних. Нужно явно объявить их или использовать миксины.

- [Содержание](#содержание)

---

# **Pytest hooks**

## **Junior Level*

Pytest hooks (хуки) — это специальные функции, которые позволяют расширять и кастомизировать поведение pytest на разных
этапах выполнения тестов. Если представить pytest как кинотеатр, то хуки — это моменты, когда можно вставить свою
рекламу или изменить сценарий: перед началом сеанса, во время показа или после его завершения.

Хуки позволяют плагинам (включая ваши собственные) вмешиваться в процесс тестирования: изменять список тестов, добавлять
дополнительную обработку перед или после каждого теста, модифицировать отчеты, интегрироваться с внешними системами. Для
QA инженера понимание хуков открывает возможность создания кастомных плагинов для специфичных нужд проекта: интеграция с
системой отчетности, подготовка тестового окружения, сбор дополнительных метрик.

# ## **Middle Level**

Технически, хуки — это часть архитектуры pytest, построенной на библиотеке `pluggy`. Это система точек расширения, где
каждая точка соответствует определенному этапу жизненного цикла тестов.

1. **Система плагинов и pluggy:**
    - Pytest сам является набором встроенных плагинов, которые регистрируют и используют хуки.
    - `pluggy` — это отдельная библиотека, реализующая механизм «хук-спецификаций» и «хук-имплементаций». Она управляет
      обнаружением, регистрацией и вызовом хуков.

2. **Типы хуков:**
    - **Хуки настройки/завершения:** `pytest_configure`, `pytest_unconfigure`. Вызываются при инициализации и завершении
      сессии.
    - **Хуки сбора тестов:** `pytest_collection_modifyitems`, `pytest_collection_finish`. Позволяют фильтровать,
      переупорядочивать или модифицировать собранные тесты.
    - **Хуки выполнения тестов:** `pytest_runtest_setup`, `pytest_runtest_call`, `pytest_runtest_teardown`. Вызываются
      соответственно перед тестом, во время выполнения теста и после.
    - **Хуки отчетов:** `pytest_runtest_makereport`, `pytest_terminal_summary`. Позволяют создавать кастомные отчеты и
      выводить информацию в терминал.
    - **Хуки вызова:** `pytest_internalerror`, `pytest_keyboard_interrupt`. Обработка внутренних ошибок и прерываний.

3. **Реализация хуков:**
    - Хуки реализуются в плагинах (отдельных модулях или классах) как функции с именами, соответствующими спецификациям.
    - Плагин регистрирует свои хуки автоматически при загрузке (через entry points) или вручную через `pytest.addhooks`.
    - Хуки могут иметь параметры, которые pytest передает в них (например, `session`, `item`, `report`).

4. **Примеры использования для AQA:**
    - **Автоматическая маркировка тестов:** Хук `pytest_collection_modifyitems` может анализировать имена тестов и
      автоматически помечать их как `@pytest.mark.slow` или `@pytest.mark.integration`.
    - **Динамическое добавление тестов:** Хук `pytest_generate_tests` позволяет генерировать параметризованные тесты на
      основе внешних данных.
    - **Кастомная отчетность:** Хук `pytest_runtest_makereport` позволяет добавлять в отчет дополнительную информацию (
      скриншоты, логи, метрики производительности).
    - **Интеграция с внешними системами:** Хуки `pytest_sessionstart` и `pytest_sessionfinish` могут отправлять
      уведомления в Slack, JIRA или обновлять дашборды.

# ## **Senior Level**

Глубокий анализ системы хуков, их взаимодействия с внутренней архитектурой pytest и продвинутые сценарии использования.

1. **Архитектура pluggy и порядок вызова:**
    - **HookspecMarker и HookimplMarker:** `pluggy` использует декораторы `@hookspec` для объявления спецификации хука (
      что он делает, какие параметры принимает) и `@hookimpl` для реализации.
    - **Приоритеты и ordering:** Реализации хуков могут иметь модификаторы `tryfirst=True` или `trylast=True`, чтобы
      управлять порядком вызова среди нескольких плагинов.
    - **Hook wrappers:** Особый тип реализации, оборачивающий вызов других реализаций. Хук-обертка получает генератор,
      который `yield`'ит результат остальных реализаций. Это позволяет выполнить код до и после основного вызова хука (
      аналогично `around` advice в AOP).
    - **Пример:** Хук `pytest_runtest_makereport` является hook wrapper, позволяя плагинам модифицировать отчет до и
      после его создания.

2. **Внутренний объект `pytest` и `config`:**
    - Объект `pytest` (или `config` в хуках) — это центральный реестр, содержащий все зарегистрированные плагины, хуки и
      состояние сессии.
    - Плагины могут добавлять свои атрибуты в `config`, чтобы делиться данными между хуками (например,
      `config.myplugin_data = {}`).
    - `config.hook` — это объект `pluggy.PluginManager`, через который происходит вызов всех хуков.

3. **Хуки и жизненный цикл тестового элемента (Item):**
    - Каждый тест (функция, метод) представляется объектом `Item`. Хуки `pytest_runtest_*` получают этот объект.
    - Внутри `Item` есть `_request` — объект, содержащий контекст выполнения, включая фикстуры.
    - Хуки могут модифицировать `Item` (например, добавлять пользовательские атрибуты через `item.user_properties`),
      которые затем могут быть использованы в отчетах.

4. **Динамическая регистрация хуков:**
    - Плагины могут регистрировать новые хуки во время выполнения через `pluggy.PluginManager.add_hookspecs`. Это
      позволяет создавать плагины, которые расширяют не только поведение, но и интерфейс pytest.
    - Однако, такой подход требует глубокого понимания архитектуры и может привести к конфликтам.

5. **Для AQA:**
    - **Создание DSL через хуки:** Плагин может через хук `pytest_pycollect_makeitem` перехватывать сбор тестов и
      преобразовывать функции с определенными декораторами в кастомные тестовые объекты. Например, можно создать
      синтаксис `@scenario("ID-123")`, который будет превращать функцию в тест-кейс с привязанным ID из TestRail.
    - **Распределенное выполнение тестов:** Плагин, использующий `pytest_collection_modifyitems` для разделения тестов
      на группы (sharding) и `pytest_sessionstart` для координации между несколькими инстансами runner'ов через
      распределенную очередь (Redis, RabbitMQ). Хук `pytest_runtest_protocol` может быть полностью переопределен для
      отправки теста на выполнение в удаленный worker.
    - **Адаптивное тестирование:** Плагин, который в `pytest_runtest_makereport` анализирует результаты и на лету
      изменяет план тестирования (например, при падении smoke-теста, пропускает dependent тесты). Это требует сохранения
      состояния между вызовами хуков через `config` или внешнее хранилище.
    - **Глубокая интеграция с мониторингом:** Хук `pytest_runtest_call` может оборачивать выполнение теста в span
      трассировки (OpenTelemetry), отправляя метрики длительности, а `pytest_terminal_summary` — агрегировать и выводить
      перформанс-дайджест.
    - **Валидация тестового кода:** Плагин, использующий хук `pytest_collect_file` для анализа не только Python-файлов,
      но и конфигурационных (YAML, JSON) на соответствие схеме, и падения сборки тестов при нарушении.
    - **Кастомные фикстуры на уровне плагина:** Хук `pytest_fixture_setup` позволяет перехватывать создание фикстур и
      модифицировать их поведение глобально (например, добавлять автоматическое логирование для всех фикстур с
      определенным маркером).
    - **Интроспекция и отладка:** Плагин для отладки самого pytest, который регистрирует все хуки и логирует их вызовы с
      параметрами. Это помогает понять порядок выполнения и взаимодействие плагинов в сложных конфигурациях.
    - **Динамическое управление ресурсами:** Плагин, который в `pytest_sessionstart` резервирует виртуальную машину в
      облаке, в `pytest_collection_modifyitems` фильтрует тесты, которые могут на ней выполняться, а в
      `pytest_sessionfinish` — освобождает ее. Состояние VM хранится в `config` и доступно в фикстурах через
      `request.config`.
    - **Поддержка новых языков/форматов:** Через хук `pytest_pycollect_makeitem` можно научить pytest собирать тесты из
      файлов, написанных на другом языке (например, Gherkin для BDD), преобразуя их в Python-объекты на лету.

6. **Опасности и тонкости:**
    - **Циклические зависимости и порядок:** Неправильный порядок вызова хуков может привести к неожиданному поведению.
      Например, если плагин A зависит от данных, которые подготавливает плагин B, нужно обеспечить правильный порядок
      через `tryfirst`/`trylast` или явные зависимости плагинов.
    - **Производительность:** Слишком много хуков, особенно тех, которые выполняют тяжелые операции (сетевые запросы,
      анализ файлов), могут сильно замедлить выполнение тестов.
    - **Совместимость:** При обновлении pytest сигнатуры хуков могут меняться. Плагины должны быть готовы к обратной
      совместимости или явно указывать версию pytest, которую они поддерживают.
    - **Отладка:** Ошибки внутри хуков могут быть трудноотлаживаемыми, так как они происходят глубоко внутри механизма
      pytest. Необходимо тщательное логирование и обработка исключений.

7. **Расширенные объекты в хуках:**
    - **`PytestPluginManager`:** Позволяет программно управлять плагинами (включать/выключать).
    - **`Node`:** Базовый класс для `Item`, `Collector`. Хуки часто получают узлы дерева тестов и могут перемещаться по
      нему (родитель, дети).
    - **`CallInfo`:** Объект, содержащий информацию о вызове теста: результат, исключение, длительность. Доступен в
      `pytest_runtest_makereport`.

- [Содержание](#содержание)

---

# **Kubernetes**

## **Junior Level*

Kubernetes (K8s) — это система для автоматизации развертывания, масштабирования и управления контейнеризированными
приложениями. Представьте, что у вас есть много контейнеров (как изолированных пакетов с вашим приложением), и вам нужно
управлять ими на множестве серверов. Kubernetes берет на себя эту задачу: он сам решает, где запускать контейнеры, как
распределять между ними нагрузку, как перезапускать их при сбоях и как обновлять без простоев.

Для QA инженера Kubernetes важен по нескольким причинам:

1. **Тестовые окружения:** Можно быстро создавать изолированные окружения для тестирования, которые точно повторяют
   продакшен.
2. **Масштабирование тестов:** Запускать тысячи тестов параллельно, используя возможности Kubernetes по управлению
   ресурсами.
3. **Инфраструктура для тестов:** Сами тестовые фреймворки и системы отчетности можно развертывать в Kubernetes как
   микросервисы.
4. **Тестирование в реалистичных условиях:** Тестировать приложение в той же среде, где оно будет работать.

# ## **Middle Level**

С технической точки зрения, Kubernetes состоит из нескольких ключевых компонентов, которые взаимодействуют через API.

1. **Архитектура кластера:**
    - **Control Plane (Master):** Управляющая нода, содержащая API Server, Scheduler, Controller Manager, etcd (
      хранилище конфигурации).
    - **Worker Nodes:** Ноды, на которых запускаются контейнеры. Каждая содержит kubelet (агент), kube-proxy (сетевой
      прокси) и container runtime (например, Docker).

2. **Основные объекты Kubernetes:**
    - **Pod:** Минимальная единица развертывания. Это один или несколько контейнеров, которые разделяют сеть и
      хранилище.
    - **Deployment:** Описывает желаемое состояние приложения и управляет обновлением и откатом версий.
    - **Service:** Абстракция для доступа к группе подов (обычно через балансировку нагрузки).
    - **ConfigMap и Secret:** Для управления конфигурацией и секретами.
    - **Namespace:** Виртуальный кластер внутри физического, для изоляции ресурсов.

3. **Для AQA:**
    - **Тестовые среды:** Использование Namespaces для изоляции тестовых окружений. Можно создать namespace для каждого
      тестового прогона.
    - **Запуск тестов в Pod'ах:** Тесты могут запускаться в отдельных Pod'ах как Job или CronJob. Это позволяет легко
      масштабировать и управлять выполнением тестов.
    - **Доступ к приложению:** Использование Services для доступа к тестируемому приложению, развернутому в кластере.
    - **Конфигурация тестов:** Использование ConfigMaps для передачи конфигурации тестов (например, URL приложения,
      учетные данные).

4. **Инструменты:**
    - **kubectl:** CLI для управления кластером.
    - **Helm:** Менеджер пакетов для Kubernetes, упрощающий развертывание сложных приложений.
    - **Minikube и Kind:** Инструменты для запуска локального кластера Kubernetes на машине разработчика.

# ## **Senior Level**

На этом уровне мы рассматриваем Kubernetes как сложную распределенную систему, понимаем ее внутренние механизмы и
используем для построения продвинутых тестовых инфраструктур.

1. **Control Plane и распределенный консенсус:**
    - **etcd:** Распределенное key-value хранилище, основанное на Raft consensus алгоритме. Вся конфигурация и состояние
      кластера хранятся здесь. Для тестовых кластеров важно понимать, что etcd может стать узким местом при интенсивной
      нагрузке (много изменений конфигурации, например, частые создания/удаления Pod'ов для тестов).
    - **API Server:** Центральный управляющий компонент, который валидирует и обрабатывает запросы. Все взаимодействие с
      кластером происходит через его REST API. Можно интегрировать тестовые фреймворки напрямую с API Server для
      мониторинга состояния развертываний.

2. **Scheduler и распределение нагрузки:**
    - Scheduler решает, на какой ноде запустить Pod, на основе политик, ресурсов и ограничений. Для тестовых нагрузок
      важно правильно настраивать ресурсы (requests/limits) для Pod'ов с тестами, чтобы не перегружать ноды и не влиять
      на другие приложения.
    - Можно создавать custom schedulers для специализированных сценариев, например, для запуска тестов на нодах с
      определенными характеристиками (GPU, SSD).

3. **Сеть в Kubernetes:**
    - **CNI (Container Network Interface):** Плагины, обеспечивающие сетевое взаимодействие между Pod'ами. Для
      тестирования сетевых политик (Network Policies) важно понимать, какой CNI используется в кластере.
    - **Service Mesh (Istio, Linkerd):** Надстройка над сетью Kubernetes, предоставляющая продвинутые возможности:
      трафик, наблюдение, безопасность. В тестировании можно использовать Service Mesh для управления трафиком во время
      тестов (например, перенаправление части запросов на канареечную версию).

4. **Хранилище:**
    - **Persistent Volumes (PV) и Persistent Volume Claims (PVC):** Механизм предоставления постоянного хранилища
      Pod'ам. Для тестов, которые требуют сохранения состояния (например, тесты базы данных), важно правильно
      настраивать PVC.
    - **Storage Classes:** Позволяют динамически предоставлять хранилище. Можно создать отдельный storage class для
      тестов, который использует быстрые, но недолговечные диски.

5. **Для AQA:**
    - **Динамическое создание тестовых окружений:** Использование операторов Kubernetes (Kubernetes Operators) для
      автоматического развертывания и управления состоянием тестовых окружений. Оператор может отслеживать custom
      resources (CRD) и создавать namespace, deployment, service и другие ресурсы для каждого тестового прогона.
    - **Распределенное выполнение тестов:** Запуск тестов как Job с параллельными Pod'ами. Использование очередей (
      например, RabbitMQ, Kafka) для координации между Pod'ами. Каждый Pod берет задание из очереди, выполняет тест и
      отправляет результат обратно.
    - **Нагрузочное тестирование в K8s:** Развертывание системы нагрузочного тестирования (например, Apache JMeter в
      кластере) как Deployment. Управление количеством Pod'ов-генераторов нагрузки в зависимости от требований теста.
      Использование Horizontal Pod Autoscaler для автоматического масштабирования генераторов нагрузки.
    - **Мониторинг и observability:** Интеграция тестовых прогонов с системой мониторинга кластера (Prometheus,
      Grafana). Сбор метрик не только с тестируемого приложения, но и с самого кластера (использование ресурсов, ошибки
      kubelet). Настройка алертинга на аномалии во время тестов.
    - **Тестирование отказоустойчивости:** Использование chaos engineering инструментов (например, Chaos Mesh, Litmus)
      для внесения сбоев в кластер во время выполнения тестов. Проверка, как приложение и тесты себя ведут при отказе
      ноды, сети или хранилища.
    - **Безопасность (Security Testing):** Использование возможностей Kubernetes для тестирования безопасности: Security
      Context, Pod Security Policies (устарели), Pod Security Standards. Запуск тестов на уязвимости контейнеров (Trivy,
      Clair) непосредственно в CI/CD пайплайне, интегрированном с Kubernetes.
    - **Миграция состояния тестов:** При тестировании stateful приложений (базы данных) использование снапшотов (
      snapshots) Persistent Volumes для быстрого восстановления состояния перед каждым тестом. Настройка Init Containers
      в Pod'ах тестов для подготовки данных.
    - **Интеграция с внешними системами:** Использование Kubernetes API для автоматического создания тестовых данных во
      внешних системах (например, в облачных сервисах AWS, GCP) через механизм External Secrets и External DNS.
    - **Оптимизация затрат:** Настройка node autoscaling (Cluster Autoscaler) для автоматического добавления и удаления
      нод в зависимости от нагрузки тестов. Использование spot/preemptible инстансов для тестовых нод, чтобы снизить
      стоимость. Гибкое управление ресурсами через ResourceQuotas и LimitRanges в namespace тестов.

6. **Проблемы и решения:**
    - **Сетевые задержки:** В распределенной среде сетевые задержки могут влиять на результаты тестов, особенно на
      производительность. Необходимо учитывать расположение Pod'ов с тестами и тестируемым приложением (желательно в
      одной зоне доступности).
    - **Очистка ресурсов:** После тестового прогона важно удалить все созданные ресурсы (namespace, persistent volumes)
      чтобы избежать утечек. Использование механизма ttl для Job или написание собственных cleanup скриптов, которые
      вызываются через хуки (pre-stop, post-start).
    - **Доступ к результатам тестов:** Сбор артефактов тестов (логи, скриншоты, отчеты) из Pod'ов. Использование sidecar
      контейнеров для отправки артефактов в объектное хранилище (S3, GCS) или в специализированную систему (
      Elasticsearch).
    - **Воспроизводимость тестовых сред:** Использование Infrastructure as Code (IaC) инструментов (Terraform,
      Crossplane) для описания кластера и GitOps (ArgoCD, Flux) для управления конфигурацией приложений. Это позволяет
      воспроизводить окружения для отладки проблем.

7. **Тренды и будущее:**
    - **Serverless поверх Kubernetes (Knative):** Запуск тестов как serverless функций, что позволяет еще более
      эффективно использовать ресурсы.
    - **eBPF:** Использование расширенных возможностей ядра Linux для наблюдения и безопасности без модификации
      приложений. Инструменты типа Cilium предоставляют мощные возможности для тестирования сетевых взаимодействий.
    - **WebAssembly (Wasm) в Kubernetes:** Возможность запуска тестов, скомпилированных в Wasm, что обеспечивает лучшую
      изоляцию и производительность.

- [Содержание](#содержание)

---

# **Пирамида тестирования**

## **Junior Level*

Пирамида тестирования — это концепция, которая визуализирует оптимальное соотношение различных типов автоматизированных
тестов в проекте. Она состоит из трех основных уровней:

1. **Unit-тесты (нижний уровень, основание пирамиды):** Тестируют отдельные компоненты системы (функции, классы) в
   полной изоляции. Их должно быть больше всего — они быстрые, дешевые в поддержке и дают мгновенную обратную связь.

2. **Интеграционные тесты (средний уровень):** Проверяют взаимодействие нескольких компонентов (модулей, сервисов, баз
   данных). Их меньше, чем unit-тестов — они медленнее, сложнее в поддержке, но проверяют критически важные
   взаимодействия.

3. **UI/E2E-тесты (верхний уровень, вершина пирамиды):** Тестируют систему с точки зрения конечного пользователя,
   проверяя полные сценарии работы. Их должно быть меньше всего — они самые медленные, хрупкие и дорогие в поддержке, но
   дают уверенность в работе системы в целом.

Цель пирамиды — создать сбалансированную стратегию тестирования: много быстрых и стабильных тестов внизу, меньше
медленных и комплексных наверху. Для QA инженера понимание этой концепции помогает планировать усилия по автоматизации,
распределять ресурсы и строить эффективный процесс тестирования.

# ## **Middle Level**

С технической точки зрения реализация каждого уровня пирамиды в Python-экосистеме имеет свои особенности:

1. **Unit-тестирование:**
    - **Инструменты:** `pytest`, `unittest`, `nose2`. Pytest стал де-факто стандартом благодаря гибкости и богатой
      экосистеме.
    - **Изоляция:** Использование моков (`unittest.mock`) для замены зависимостей. Ключевые техники: патчинг (`patch`),
      подмены (`MagicMock`, `AsyncMock`).
    - **Покрытие кода:** Инструменты `coverage.py` и `pytest-cov` для измерения покрытия.
    - **Параметризация:** Декоратор `@pytest.mark.parametrize` для запуска одного теста с разными входными данными.
    - **Важно:** Хороший unit-тест не зависит от внешних систем (БД, файловая система, сеть).

2. **Интеграционное тестирование:**
    - **Тестирование API:** Библиотеки `requests` + `pytest` для HTTP-API. Для асинхронных API — `aiohttp` или `httpx`.
    - **Тестирование БД:** Использование тестовых баз данных (например, SQLite in-memory) или механизмов транзакций с
      откатом после каждого теста. Инструменты: `pytest-django`, `factory_boy` для генерации данных.
    - **Тестирование микросервисов:** Использование тестовых дублей (test doubles) — заглушек (stubs) и моков для
      зависимых сервисов. Контейнеризация зависимостей (Docker) для запуска реальных сервисов в тестовом окружении.
    - **Фикстуры с областью видимости:** В pytest использование `@pytest.fixture(scope="module")` или
      `@pytest.fixture(scope="session")` для создания дорогих ресурсов (например, соединение с БД), которые
      переиспользуются между тестами.

3. **UI/E2E-тестирование:**
    - **Инструменты:** `Selenium WebDriver`, `Playwright`, `Cypress` (через `pytest-playwright`).
    - **Page Object Pattern:** Организация тестового кода через абстракции страниц/компонентов для уменьшения хрупкости
      и повышения переиспользуемости.
    - **Управление состоянием:** Создание и очистка тестовых данных перед/после тестов. Использование API для
      предварительной настройки состояния системы.
    - **Параллельный запуск:** Инструменты `pytest-xdist` для параллельного выполнения тестов. Для UI-тестов важно
      изолировать сессии браузера.

4. **Для AQA:**
    - **Баланс уровней:** Практическое правило: 70% unit-тестов, 20% интеграционных, 10% E2E. Но пропорции зависят от
      проекта.
    - **CI/CD интеграция:** Размещение разных уровней тестов в разных стадиях пайплайна: unit-тесты запускаются на
      каждом коммите, интеграционные — на пулл-реквестах, E2E — на релизных кандидатах.
    - **Флаки-тесты:** UI-тесты часто нестабильны. Необходимы стратегии борьбы: retry механизмы, стабилизация ожиданий (
      explicit waits), изоляция окружения.

# ## **Senior Level**

Глубокий анализ пирамиды тестирования как архитектурного паттерна, его эволюции, ограничений и интеграции с современными
практиками разработки.

1. **Эволюция и критика классической пирамиды:**
    - **"Песочные часы" или "Ромб":** Современные подходы предлагают увеличивать средний уровень (
      интеграционные/сервисные тесты) для микросервисных архитектур. Вместо пирамиды — песочные часы: много unit-тестов,
      много E2E, но акцент на контрактных тестах между сервисами.
    - **Пирамида Майка Кона:** Дополнение пирамиды ручным тестированием (исследовательское, usability) и тестами
      производительности/безопасности.
    - **Критика:** В микросервисной архитектуре unit-тесты часто дают ложное чувство безопасности, так как не проверяют
      взаимодействие сервисов. Акцент смещается на контрактное тестирование (Pact) и тестирование потребителя (
      consumer-driven contracts).

2. **Архитектурные аспекты реализации каждого уровня:**
    - **Unit-тесты и чистая архитектура:** Unit-тесты должны тестировать бизнес-логику в изоляции от инфраструктуры.
      Достигается через Dependency Injection и следование принципам SOLID. Использование `Protocol` для абстракций
      позволяет создавать моки без наследования.
    - **Интеграционные тесты и транзакции:** Для тестов БД важно использовать механизмы отката транзакций. В Django —
      `@pytest.mark.django_db(transaction=True)`. В SQLAlchemy — `session.begin_nested()` для nested transactions. Для
      NoSQL БД — создание отдельной тестовой базы на каждый тестовый прогон.
    - **E2E тесты и идемпотентность:** Каждый E2E тест должен быть идемпотентным — его повторный запуск не должен
      зависеть от предыдущих запусков. Достигается через:
        - Глобальную уникальность тестовых данных (UUID, временные метки).
        - Паттерн Test Data Builder.
        - Автоматическую очистку через хуки (например, `pytest.fixture` с `autouse=True` и `yield`).

3. **Пирамида и CI/CD:**
    - **Стратификация выполнения:** Разделение тестов на "быстрые" и "медленные". Быстрые тесты запускаются на каждом
      коммите, медленные — по расписанию или по мере необходимости. В GitLab CI/CD — `rules: changes`, в GitHub
      Actions — `paths`.
    - **Канареечный деплоймент и тестирование:** E2E-тесты выполняются на канареечном окружении перед выкатом в прод.
      Использование feature flags для управления доступностью функциональности.
    - **Тестирование в продакшене:** Практики progressive delivery: A/B тестирование, мониторинг ошибок, трассировка
      запросов. Тесты в проде — это следующий уровень после пирамиды.

4. **Для AQA:**
    - **Динамическое определение уровня тестов:** Плагин для pytest, который анализирует зависимости теста (через
      интроспекцию фикстур) и автоматически определяет его уровень: если тест использует моки — unit; если реальную БД —
      интеграционный; если Selenium — E2E. Это позволяет автоматически распределять тесты по разным стадиям CI/CD.
    - **Автоматический баланс пирамиды:** Скрипт, который анализирует историю выполнения тестов (длительность,
      стабильность, покрытие кода) и рекомендует, какие тесты нужно перевести на другой уровень или переписать.
      Использование machine learning для предсказания хрупкости тестов.
    - **Контрактное тестирование для микросервисов:** Внедрение Pact или аналогичных инструментов. Проблема: поддержание
      актуальности контрактов. Решение: автоматическая генерация контрактов из аннотаций типов (TypeScript/OpenAPI для
      фронтенда, `pydantic` для бэкенда) и их валидация в CI.
    - **Тестирование event-driven архитектур:** Для систем на базе Kafka/RabbitMQ unit-тесты бессмысленны. Акцент на
      интеграционное тестирование с тестовым брокером (например, `testcontainers` для запуска Kafka в Docker) и проверка
      корректности обработки событий в различных сценариях.
    - **Перформанс-тестирование как часть пирамиды:** Интеграция нагрузочных тестов (Locust, k6) в CI/CD. Запуск базовых
      нагрузочных тестов на каждую сборку, расширенных — перед релизом. Анализ degradation по метрикам (памяти, CPU,
      latency).
    - **Security-тестирование:** Статический анализ (SAST) — на уровне unit-тестов (каждый коммит). Динамический
      анализ (DAST) — на уровне интеграционных тестов. Penetration testing — на уровне E2E.
    - **Метрики качества тестов:**
        - **Unit-уровень:** Code coverage (branch coverage > 80%), цикломатическая сложность, мутационное тестирование (
          mutmut).
        - **Интеграционный уровень:** Time to failure (как быстро тест обнаруживает регрессию), стабильность (percentage
          of flaky tests).
        - **E2E-уровень:** Business risk coverage (какие бизнес-сценарии покрыты), cost per test (время выполнения *
          стоимость инфраструктуры).
    - **Пирамида для ML-систем:** Особый вызов. Unit-тесты для функций предобработки данных, интеграционные для проверки
      цепочек пайплайнов, E2E для проверки качества модели на тестовых датасетах. Добавляется специфичный уровень —
      тестирование данных (data validation, drift detection).

5. **Антипаттерны и темная сторона:**
    - **Перевернутая пирамида:** Много медленных E2E-тестов, мало unit-тестов. Признак: CI/CD пайплайны выполняются
      часами, разработчики не запускают тесты локально.
    - **Ложные unit-тесты:** Тесты, которые используют реальную БД или сетевые вызовы, но называются unit-тестами.
      Следствие: медленные "unit-тесты", которые падают при отсутствии сети.
    - **Хрупкие интеграционные тесты:** Тесты, которые зависят от специфичного состояния данных. Падают при запуске в
      неправильном порядке или на неподготовленном окружении.
    - **Over-mocking:** Избыточное использование моков, когда тест проверяет не поведение системы, а то, как вызываются
      моки. Тест становится хрупким к рефакторингу.

6. **Будущее пирамиды:**
    - **Shift-left и shift-right:** Тестирование смещается влево (в процесс разработки) и вправо (в прод). Пирамида
      становится объемной фигурой.
    - **AI-assisted testing:** Использование LLM для генерации тестовых случаев, выявления паттернов в падающих тестах,
      предложения оптимизаций.
    - **Serverless и FaaS:** Для бессерверных архитектур классическая пирамида не подходит. Акцент на тестировании
      функций в изоляции (unit) и тестировании оркестрации (интеграционное).

- [Содержание](#содержание)

---

# **Виды тестирования**

## **Junior Level*

Виды тестирования — это различные подходы и методы проверки программного обеспечения, каждый из которых решает
конкретные задачи и имеет свою область применения. Основные виды:

1. **Функциональное тестирование** — проверяет, что система работает в соответствии с требованиями (что она делает).
2. **Нефункциональное тестирование** — проверяет, как система работает (производительность, безопасность, надежность).
3. **Модульное тестирование (Unit)** — тестирование отдельных компонентов кода (функций, классов) в изоляции.
4. **Интеграционное тестирование** — проверка взаимодействия между компонентами, модулями или системами.
5. **Системное тестирование (End-to-End)** — тестирование полного рабочего потока приложения от начала до конца.
6. **Регрессионное тестирование** — проверка, что новые изменения не сломали существующую функциональность.
7. **Дымовое тестирование (Smoke)** — быстрая проверка основных функций системы после сборки.
8. **Приемочное тестирование (Acceptance)** — проверка соответствия системы бизнес-требованиям.

Для QA инженера понимание этих видов помогает выбирать правильные подходы для разных ситуаций: что тестировать
автоматически, а что вручную, как распределять ресурсы и строить стратегию тестирования.

# ## **Middle Level**

С технической точки зрения каждый вид тестирования в Python-экосистеме реализуется через конкретные инструменты и
практики:

1. **Функциональное тестирование:**
    - **API-тестирование:** Использование `requests`, `httpx`, `aiohttp` для HTTP-запросов. Фреймворки: `pytest` с
      плагинами `pytest-httpx`, `pytest-asyncio`.
    - **UI-тестирование:** `Selenium WebDriver`, `Playwright`, `Cypress` через Python-биндинги. Паттерн Page Object для
      структурирования кода.
    - **Тестирование бизнес-логики:** Модульные и интеграционные тесты с использованием моков (`unittest.mock`) и
      стабов.

2. **Нефункциональное тестирование:**
    - **Нагрузочное тестирование:** `locust` (кодовая нагрузка), `k6` (через subprocess), `JMeter` (через
      `jmeter-python`).
    - **Тестирование безопасности:** Статические анализаторы (`bandit`, `safety`), динамические (`OWASP ZAP` API),
      проверка зависимостей (`dependabot`, `renovate`).
    - **Тестирование доступности (a11y):** `axe-core` через `selenium` или `playwright`.

3. **Модульное тестирование (Unit):**
    - **Изоляция:** Использование `unittest.mock.patch`, `MagicMock`, `AsyncMock` для подмены зависимостей.
    - **Параметризация:** `@pytest.mark.parametrize` для тестирования с разными входными данными.
    - **Property-based тестирование:** `hypothesis` для генерации тестовых данных и проверки инвариантов.

4. **Интеграционное тестирование:**
    - **Тестирование с БД:** Использование тестовых БД (SQLite in-memory), транзакций с откатом, фикстур для данных.
    - **Тестирование микросервисов:** `docker-compose` для поднятия зависимостей, `testcontainers` для управления
      контейнерами из кода.
    - **Контрактное тестирование:** `pact-python` для проверки совместимости между потребителем и поставщиком API.

5. **Регрессионное тестирование:**
    - **Тест-сьюты:** Организация тестов по тегам (`@pytest.mark.regression`) для выборочного запуска.
    - **Анализ покрытия:** `pytest-cov` для отслеживания покрытия измененного кода.

6. **Приемочное тестирование:**
    - **BDD-подход:** `behave`, `pytest-bdd` для тестирования на основе пользовательских сценариев (Gherkin).
    - **Автоматизация сценариев:** Комбинация API и UI-тестов для проверки полных пользовательских сценариев.

7. **Тестирование в CI/CD:**
    - **Стратификация тестов:** Разделение на быстрые (unit) и медленные (UI, нагрузочные) с разными триггерами запуска.
    - **Параллельный запуск:** `pytest-xdist` для ускорения выполнения.

# ## **Senior Level**

На этом уровне мы рассматриваем виды тестирования как систему взаимосвязанных практик, интегрированных в процесс
разработки и архитектуру приложения.

1. **Архитектурные аспекты тестирования:**
    - **Тестируемость как свойство архитектуры:** Принципы SOLID, Dependency Injection, использование абстракций (ABC,
      Protocol) для упрощения тестирования. Хорошо спроектированная система допускает легкую изоляцию компонентов для
      модульного тестирования и четкие контракты для интеграционного.
    - **Гексагональная архитектура (Ports & Adapters):** Позволяет тестировать бизнес-логику без инфраструктуры через
      in-memory адаптеры.
    - **CQRS и Event Sourcing:** Требует специализированных подходов к тестированию: проверка корректности обработки
      команд, проекций и событий.

2. **Продвинутые техники тестирования:**
    - **Мутационное тестирование:** `mutmut` для оценки качества unit-тестов. Мутанты (малые изменения в коде) должны
      обнаруживаться тестами.
    - **Фаззинг (Fuzz testing):** `atheris` (на основе libFuzzer) для поиска уязвимостей путем генерации случайных
      входных данных.
    - **Дифференциальное тестирование:** Сравнение поведения двух реализаций (старой и новой) на одном наборе данных для
      обнаружения регрессий.
    - **Тестирование вероятностных систем:** Системы с рандомным поведением или машинным обучением требуют
      статистических методов проверки (доверительные интервалы, p-value).

3. **Тестирование распределенных систем:**
    - **Тестирование в условиях сетевых проблем:** Использование инструментов chaos engineering (`chaostoolkit`,
      `pytest-chaos`) для внесения сбоев (задержки, обрывы соединений).
    - **Проверка идемпотентности и консистентности:** В системах с eventual consistency тесты должны учитывать временные
      задержки и возможные конфликты.
    - **Трассировка запросов:** Интеграция с OpenTelemetry для отслеживания выполнения запроса через несколько сервисов
      и проверки корректности работы цепочек.

4. **Мета-тестирование (тестирование тестов):**
    - **Валидация тестового кода:** Статический анализ тестов с помощью `pylint`, `flake8`, `mypy` для поддержания
      качества.
    - **Тестирование фикстур:** Проверка, что фикстуры корректно создают и очищают ресурсы, не оставляют побочных
      эффектов.
    - **Измерение эффективности тестов:** Метрики: процент обнаружения дефектов, время до обнаружения, стабильность
      тестов (flakiness score).

5. **Для AQA:**
    - **Динамическая генерация тестов:** Использование метаклассов и декораторов для создания тестов на лету на основе:
        - Конфигурационных файлов (YAML/JSON описывающих тест-кейсы).
        - Моделей данных (`pydantic` схемы для генерации тестовых данных).
        - Контрактов API (OpenAPI/Swagger спецификации для генерации тестов валидации запросов/ответов).
    - **Адаптивное тестирование:** Система, которая анализирует результаты предыдущих запусков и:
        - Повышает приоритет тестов, которые чаще ломались.
        - Динамически формирует регрессионный набор на основе анализа изменений кода (например, через `git diff` и
          анализ зависимостей).
    - **Тестирование времени выполнения (Runtime testing):** Использование декораторов, которые проверяют инварианты во
      время выполнения продакшен-кода (аналогично `assert`, но с сбором статистики и без прерывания работы). Например,
      проверка типов в рантайме с помощью `typeguard`, валидация бизнес-правил.
    - **Интеграция с мониторингом:** Тесты, которые проверяют не только функциональность, но и метрики мониторинга (
      Prometheus, Grafana). Например, после деплоя запускается тест, который создает нагрузку и проверяет, что latency
      не выросла, а ошибок нет.
    - **Тестирование миграций:** Автоматическая проверка миграций БД:
        - Тест на откат (rollback) миграции.
        - Проверка, что миграция не блокирует таблицы на продолжительное время в продакшене (через анализ плана
          выполнения).
        - Генерация тестовых данных, которые покрывают edge cases миграции.
    - **Тестирование resilience:** Автоматизированные сценарии восстановления после сбоев:
        - Симуляция отказов диска, сети, OOM killer.
        - Проверка, что система переключается на резервные сервера, перезапускает упавшие процессы, сохраняет данные.
    - **ML-тестирование:** Для систем с машинным обучением:
        - Тестирование качества моделей на валидационных датасетах.
        - Проверка на смещение (bias) и дрейф данных (data drift).
        - Интеграционные тесты для полного пайплайна ML: от данных до предсказания.

6. **Организационные и процессные аспекты:**
    - **Тестирование в условиях непрерывной поставки:** При частых деплоях (десятки в день) полный регресс невозможен.
      Стратегии:
        - Канареечный деплоймент с автоматическим откатом при падении ключевых тестов.
        - Feature flags для изоляции новой функциональности и её постепенного включения.
    - **Тестирование legacy систем:** Подходы для работы с кодом без тестов:
        - Characterization tests (тесты, которые описывают текущее поведение системы).
        - Golden master testing (сохранение выходных данных системы для будущего сравнения).
    - **Экономика тестирования:** Расчет ROI автоматизации, оптимизация стоимости тестов (инфраструктура, время
      выполнения, поддержка).

7. **Эволюция видов тестирования:**
    - **Shift-left:** Перенос тестирования на ранние этапы (статический анализ, проверка типов, линтинг в pre-commit).
    - **Shift-right:** Тестирование в продакшене (A/B тесты, мониторинг, canary releases).
    - **Тестирование как код (Testing as Code):** Инфраструктура тестирования управляется через код (тестовые сценарии,
      конфигурации, окружения), что позволяет применять практики разработки: ревью, версионирование, модульность.

- [Содержание](#содержание)

---

# **Техники тест дизайна**

## **Junior Level*

Техники проектирования тестов (Test Design Techniques) — это структурированные методы создания тестовых случаев, которые
помогают эффективно и полно проверить систему. Они отвечают на вопрос: "Как придумать хорошие тесты?" Вместо случайного
перебора тестовых данных эти техники предлагают системный подход.

Основные техники:

1. **Эквивалентное разделение (Equivalence Partitioning):** Разделение входных данных на группы (классы
   эквивалентности), в которых система должна вести себя одинаково. Например, для поля "возраст" можно выделить группы:
   отрицательные числа (невалидные), 0-17 (несовершеннолетние), 18-65 (взрослые), больше 65 (пенсионеры). Достаточно
   протестировать по одному значению из каждой группы.

2. **Анализ граничных значений (Boundary Value Analysis):** Тестирование на границах разделов. Ошибки часто возникают на
   краях допустимых диапазонов. Для возраста 18-65 граничные значения: 17, 18, 19 и 64, 65, 66.

3. **Таблица принятия решений (Decision Table Testing):** Используется, когда логика системы зависит от комбинации
   условий. Создается таблица, где перечислены все условия и соответствующие им действия. Каждая строка — это тестовый
   сценарий.

4. **Тестирование состояний и переходов (State Transition Testing):** Применяется для систем, которые имеют конечное
   число состояний и переходов между ними (например, банкомат: ввод карты -> ввод PIN -> выбор операции). Тестируются
   валидные и невалидные переходы.

5. **Тестирование сценариев использования (Use Case Testing):** Тестирование на основе пользовательских сценариев,
   описывающих взаимодействие пользователя с системой для достижения цели.

Для QA инженера владение этими техниками позволяет создавать тесты, которые с большей вероятностью найдут дефекты,
избегая избыточного тестирования.

# ## **Middle Level**

С технической точки зрения, применение этих техник в автоматизированном тестировании на Python имеет свои особенности.

1. **Эквивалентное разделение и анализ граничных значений:**
    - **Параметризация тестов:** В pytest с помощью `@pytest.mark.parametrize` легко реализовать проверку нескольких
      значений из одного класса эквивалентности.
    - **Генерация данных:** Для проверки граничных значений можно использовать `itertools.chain` для объединения списков
      значений или создавать фикстуры, возвращающие наборы данных.
    - **Пример:** Тестирование валидатора возраста:
      ```python
      @pytest.mark.parametrize('age, expected', [
          (-5, False),  # невалидный класс
          (0, False),   # граница
          (10, False),  # внутри класса
          (17, False),  # граница
          (18, True),   # граница
          (30, True),   # внутри класса
          (65, True),   # граница
          (66, False)   # граница
      ])
      def test_age_validation(age, expected):
          assert validate_age(age) == expected
      ```

2. **Таблица принятия решений:**
    - **Реализация через data-driven тесты:** Таблицу можно хранить в CSV, JSON или прямо в коде как список словарей.
      Затем итерироваться по строкам и запускать тест для каждой комбинации.
    - **Использование `pytest` и `pandas`:** Загрузить таблицу из Excel/CSV, преобразовать в список параметров.
    - **Пример:** Тестирование логики скидок, зависящей от статуса клиента и суммы покупки.

3. **Тестирование состояний и переходов:**
    - **Моделирование состояний:** Можно использовать библиотеку `state_machine` или реализовать простой конечный
      автомат своими силами.
    - **Проверка переходов:** Создавать тесты, которые имитируют последовательность событий и проверяют текущее
      состояние системы.
    - **Инструменты:** `pytest` с фикстурами для управления состоянием между шагами.

4. **Тестирование сценариев использования:**
    - **BDD-подход:** Инструменты `behave` или `pytest-bdd` позволяют описывать сценарии на языке, близком к
      естественному (Gherkin), и привязывать их к коду.
    - **Page Object Pattern:** Для UI-тестов Page Object хорошо ложится на сценарии использования, инкапсулируя
      взаимодействие с элементами страницы.

5. **Дополнительные техники:**
    - **Попарное тестирование (Pairwise Testing):** Использование инструментов `allpairspy` или `pict` для генерации
      комбинаций параметров, которые покрывают все пары значений. Это сильно сокращает количество тестов.
    - **Предугадывание ошибок (Error Guessing):** Опытный тестировщик на основе знаний о системе и типичных ошибках
      создает тесты. Автоматизировать сложно, но можно накапливать "шаблоны ошибок" и проверять их в регрессионных
      тестах.

6. **Для AQA:**
    - **Автоматизация техник:** Создание утилит, которые генерируют тестовые данные на основе техник эквивалентного
      разделения и граничных значений.
    - **Интеграция в CI/CD:** Параметризованные тесты могут запускаться на каждый коммит, обеспечивая быстрое получение
      обратной связи.
    - **Отчетность:** При использовании параметризации pytest генерирует отдельные записи для каждого набора данных, что
      упрощает анализ падений.

# ## **Senior Level**

На этом уровне техники проектирования тестов рассматриваются как фундаментальные принципы, интегрированные в процесс
разработки и архитектуру тестовой системы.

1. **Формальные основы и связь с теорией:**
    - **Теория множеств и логика:** Эквивалентное разделение и анализ граничных значений основаны на разбиении множества
      входных данных. Таблицы решений — это представление булевой логики.
    - **Конечные автоматы:** Тестирование переходов между состояниями основано на теории автоматов и может быть
      формально верифицировано с помощью model checking.
    - **Комбинаторика:** Попарное тестирование использует ортогональные массивы и покрывающие массивы (covering arrays)
      для минимизации количества комбинаций.

2. **Автоматическая генерация тестовых случаев:**
    - **Property-based testing:** Библиотека `hypothesis` не просто генерирует случайные данные, а использует техники
      эквивалентного разделения и граничных значений "под капотом". Она строит стратегии генерации данных (strategies),
      которые можно кастомизировать.
    - **Модельное тестирование (Model-based testing):** Создание формальной модели системы (например, на языке Python) и
      автоматическая генерация тестовых последовательностей, покрывающих все состояния и переходы. Инструменты:
      `GraphWalker` (можно интегрировать через API).
    - **Символьное выполнение (Symbolic execution):** Хотя в Python это сложно из-за динамической типизации, существуют
      инструменты, которые пытаются анализировать код и генерировать тесты, покрывающие все пути выполнения (например,
      `pyexz3` на основе Z3).

3. **Динамическое применение техник в зависимости от контекста:**
    - **Адаптивное тестирование:** Система, которая анализирует покрытие кода и результаты предыдущих тестов, чтобы
      решить, какие техники применять дальше. Например, если обнаруживается много ошибок на границах, автоматически
      увеличивается количество тестов на граничные значения.
    - **Машинное обучение для дизайна тестов:** Использование ML для предсказания, какие комбинации параметров с
      наибольшей вероятностью приведут к ошибке, на основе истории дефектов. Обучение модели на исторических данных и
      генерация тестовых случаев, которые "похожи" на те, что находили ошибки ранее.

4. **Интеграция техник в тестовые фреймворки:**
    - **Кастомные маркеры и хуки в pytest:** Создание плагина, который добавляет маркеры для различных техник (например,
      `@pytest.mark.boundary_values`). Хуки могут использоваться для сбора метрик: сколько тестов используют каждую
      технику, насколько они эффективны (количество найденных багов).
    - **Расширение pytest для поддержки таблиц решений:** Плагин, который позволяет загружать таблицы решений из файлов
      и преобразовывать их в параметризованные тесты автоматически.
    - **Генерация тестов из спецификаций:** Интеграция с инструментами спецификации (OpenAPI, Swagger) для
      автоматического создания тестов, использующих техники эквивалентного разделения (валидные/невалидные данные) для
      каждого поля.

5. **Для AQA:**
    - **Мета-тестирование техник:** Написание тестов, которые проверяют, что техники проектирования тестов применяются
      корректно. Например, тест, который проверяет, что для каждого параметризованного теста есть хотя бы один случай из
      каждого класса эквивалентности. Это можно сделать через анализ AST тестовых функций.
    - **Динамический анализ кода для определения границ:** Инструмент, который анализирует исходный код тестируемого
      приложения (через `ast` или `inspect`), находит условия (if-elif-else, сравнения) и автоматически определяет
      граничные значения для параметров. Затем генерирует тесты для этих границ.
    - **Фаззинг с обратной связью (coverage-guided fuzzing):** Инструменты типа `atheris` (на основе libFuzzer)
      используют информацию о покрытии кода для динамической корректировки входных данных, чтобы достичь новых ветвей
      кода. Это автоматическое применение анализа граничных значений и эквивалентного разделения в реальном времени.
    - **Тестирование вероятностных и недетерминированных систем:** Применение статистических методов для определения,
      достаточно ли тестовых случаев для обнаружения ошибок с заданной вероятностью. Использование техник, таких как
      Monte Carlo тестирование.
    - **Интеграция с формальной верификацией:** Для критичных систем (авионика, медицинское ПО) техники дизайна тестов
      могут комбинироваться с формальными методами. Например, использование контрактов (assertions, pre/post-conditions)
      и автоматической генерации тестов из формальных спецификаций с помощью инструментов, подобных `Daikon` (для Python
      можно использовать `icontract`).

6. **Проблемы и ограничения:**
    - **Проклятие размерности:** При большом количестве параметров даже попарное тестирование может генерировать много
      комбинаций. Необходимо использовать эвристики для приоритизации.
    - **Динамическая природа Python:** Статический анализ кода для определения границ сложен из-за динамической
      типизации и метапрограммирования. Часто приходится полагаться на аннотации типов и документацию.
    - **Сложность тестирования состояний:** В системах с большим количеством состояний и переходов полное покрытие может
      быть недостижимо. Приходится выбирать наиболее важные пути (например, на основе использования в продакшене).

7. **Будущее техник проектирования тестов:**
    - **AI-assisted test design:** Использование LLM (например, GPT) для предложения тестовых случаев на основе описания
      функциональности. Обучение моделей на больших наборах кода и тестов.
    - **Тестирование на основе моделей (MBT) в CI/CD:** Интеграция MBT в конвейер непрерывной интеграции, где модель
      системы постоянно обновляется и тесты генерируются автоматически при каждом изменении.
    - **Самоадаптирующиеся тестовые системы:** Фреймворки, которые обучаются на ходу, корректируя применяемые техники в
      зависимости от обнаруженных дефектов и изменений в системе.

- [Содержание](#содержание)

---

# **Метрики тестирования**

## **Junior Level*

Метрики тестирования — это количественные показатели, которые помогают измерить и оценить различные аспекты процесса
тестирования и качества продукта. Они отвечают на вопросы: "Насколько хорошо мы тестируем?", "Каково качество нашего
кода?", "Эффективны ли наши тесты?".

Основные метрики:

- **Покрытие кода (Code Coverage):** Какой процент кода выполняется во время тестов. Измеряется в процентах по строкам,
  ветвям, функциям.
- **Количество дефектов:** Сколько багов найдено, сколько исправлено, скорость их закрытия.
- **Время выполнения тестов:** Как долго работает тестовый набор.
- **Стабильность тестов (Flakiness):** Как часто тесты падают не из-за багов в коде, а по случайным причинам (например,
  проблемы с сетью).
- **Стоимость дефекта:** Сколько стоит найти и исправить баг на разных этапах (чем раньше, тем дешевле).

Метрики помогают принимать обоснованные решения: куда направить усилия по тестированию, когда можно выпускать релиз,
какие тесты нужно улучшить.

# ## **Middle Level**

С технической точки зрения метрики в Python-экосистеме тестирования собираются и анализируются с помощью конкретных
инструментов и практик.

1. **Метрики покрытия кода:**
    - **Инструменты:** `coverage.py` — стандартный инструмент для измерения покрытия. Интегрируется с pytest через
      `pytest-cov`.
    - **Типы покрытия:**
        - **Line coverage:** Процент выполненных строк.
        - **Branch coverage:** Процент пройденных ветвей в условиях (if/else).
        - **Function coverage:** Процент вызванных функций.
        - **Condition coverage:** Процент комбинаций условий в сложных булевых выражениях.
    - **Интеграция в CI/CD:** Генерация отчетов в формате XML/HTML, интеграция с сервисами (Codecov, Coveralls).

2. **Метрики качества тестов:**
    - **Mutation score (Мутационное тестирование):** `mutmut` внедряет мелкие изменения (мутации) в код и проверяет,
      обнаружат ли их тесты. Процент убитых мутаций — показатель эффективности тестов.
    - **Стабильность тестов (Flakiness):** Анализ истории запусков тестов. Если тест иногда проходит, иногда падает при
      тех же условиях — он нестабилен. Инструменты: `pytest-flakefinder`, кастомные скрипты анализа Jenkins/Allure
      отчетов.
    - **Время выполнения:** `pytest` с флагом `--durations` показывает самые медленные тесты. `pytest-xdist` для
      параллельного запуска, но нужно учитывать накладные расходы.

3. **Метрики дефектов:**
    - **Плотность дефектов (Defect Density):** Количество багов на тысячу строк кода (KLOC).
    - **Эффективность тестирования (Test Effectiveness):** Процент дефектов, найденных тестами, от общего числа
      дефектов (включая найденные пользователями).
    - **Время жизни дефекта (Defect Age):** Среднее время от создания бага до его закрытия.

4. **Метрики процесса:**
    - **Скорость выполнения тестов:** Сколько тестов выполняется в минуту/час.
    - **Автоматизация:** Процент автоматизированных тестов от общего числа.
    - **Стоимость:** Затраты на инфраструктуру тестирования (вычислительные ресурсы, лицензии инструментов).

5. **Инструменты для сбора метрик:**
    - **Allure TestOps / ReportPortal:** Системы для хранения результатов тестов, анализа метрик.
    - **Prometheus + Grafana:** Для мониторинга производительности тестовой инфраструктуры и самого приложения во время
      тестов.
    - **Кастомные скрипты на Python:** Анализ логов, парсинг отчетов, вычисление метрик.

# ## **Senior Level**

На этом уровне метрики рассматриваются как часть системы измерения качества (Quality Measurement System), которая
интегрирована в процесс разработки, архитектуру и бизнес-цели.

1. **Архитектурные метрики и тестируемость:**
    - **Связность (Cohesion) и связанность (Coupling):** Можно измерять через статические анализаторы (`radon`,
      `mccabe`). Высокая связность и низкая связанность упрощают тестирование.
    - **Цикломатическая сложность:** Количество независимых путей в коде. Высокая сложность (>10) указывает на код,
      трудный для тестирования. Инструменты: `mccabe`, `pylint`.
    - **Индекс поддерживаемости (Maintainability Index):** Комбинированная метрика, учитывающая сложность, объем кода,
      комментарии. Позволяет прогнозировать, насколько сложно будет изменять и тестировать код.

2. **Продвинутые метрики покрытия:**
    - **Semantic Coverage:** Покрытие не только синтаксических конструкций, но и семантики (например, все возможные
      состояния объекта). Частично достигается property-based тестированием (`hypothesis`).
    - **Data Coverage:** Покрытие различных комбинаций входных данных. Использование техник попарного тестирования (
      `allpairspy`) для оценки полноты тестовых данных.
    - **Temporal Coverage:** Для асинхронных и параллельных систем — покрытие различных порядков выполнения и состояний
      гонки. Инструменты: `pytest-asyncio` с детерминированным планировщиком.

3. **Мета-метрики (метрики метрик):**
    - **Полезность метрики:** Оценивается через корреляцию с бизнес-показателями (например, влияет ли покрытие кода на
      количество инцидентов в проде).
    - **Стоимость сбора:** Сколько ресурсов (время, вычисления) уходит на сбор и анализ метрики.
    - **Стабильность метрики:** Насколько метрика подвержена случайным колебаниям. Для важных метрик нужно вычислять
      доверительные интервалы.

4. **Прогнозные метрики и ML:**
    - **Предсказание дефектов:** Модели машинного обучения, которые на основе исторических данных (изменения кода,
      сложность, покрытие тестами) предсказывают, где могут появиться баги. Использование `scikit-learn` для построения
      моделей.
    - **Рекомендательные системы для тестов:** Анализ истории падений тестов и изменений кода для предложения, какие
      тесты нужно запустить при конкретном изменении (test impact analysis).
    - **Аномалии в метриках:** Автоматическое обнаружение аномалий в метриках (например, внезапное падение покрытия,
      рост времени выполнения тестов). Инструменты: `Prophet` от Facebook для временных рядов.

5. **Для AQA:**
    - **Динамическое вычисление метрик во время выполнения тестов:** Плагин для pytest, который с помощью хуков (
      `pytest_runtest_protocol`) собирает метрики в реальном времени: использование памяти, CPU, количество исключений,
      даже покрытие кода на лету (через `sys.settrace` или инструментирование байткода).
    - **Метрики для распределенных тестов:** При использовании `pytest-xdist` или запуске тестов в Kubernetes:
        - **Распределение тестов по нодам:** Равномерность загрузки.
        - **Сетевые задержки:** Время обмена данными между master и worker.
        - **Стоимость инфраструктуры:** Затраты на облачные инстансы во время тестового прогона.
    - **Метрики тестовых данных:**
        - **Разнообразие данных:** Энтропия тестовых данных (сколько различных сценариев покрывают).
        - **Реалистичность:** Насколько тестовые данные близки к продакшен-данным (можно измерять через статистические
          тесты — распределения, корреляции).
    - **Интеграция с бизнес-метриками:**
        - **Влияние тестирования на бизнес-показатели:** Корреляция между метриками тестирования (покрытие, количество
          найденных багов) и бизнес-метриками (коэффициент оттока пользователей, средний чек).
        - **ROI автоматизации:** Расчет возврата инвестиций: (экономия на ручном тестировании - затраты на разработку и
          поддержку автотестов) / затраты.
    - **Метрики для тестирования AI/ML систем:**
        - **Coverage для моделей:** Покрытие пространства признаков (feature space) тестовыми данными.
        - **Смещение (Bias) тестовых данных:** Насколько тестовые данные репрезентативны для всех групп пользователей.
        - **Дрейф данных (Data Drift):** Изменение распределения входных данных со временем и его влияние на качество
          модели.

6. **Визуализация и дашборды:**
    - **Grafana дашборды:** Отображение метрик в реальном времени: покрытие кода, скорость тестов, стабильность.
    - **Интерактивные отчеты:** `plotly` или `bokeh` для создания интерактивных графиков, позволяющих "копать" в данные.
    - **Heatmaps:** Визуализация покрытия кода на карте исходного кода — какие файлы/функции плохо покрыты.

7. **Ограничения и опасности метрик:**
    - **Охватка метрик (Goodhart's law):** "Когда метрика становится целью, она перестает быть хорошей метрикой".
      Пример: разработчики начинают писать бессмысленные тесты только чтобы повысить покрытие.
    - **Ложные корреляции:** Две метрики могут коррелировать, но не иметь причинно-следственной связи.
    - **Контекстная зависимость:** Одинаковые значения метрик могут означать разное в разных проектах (например, 80%
      покрытия для CRUD API и для ядра СУБД).
    - **Перегруженность метриками:** Слишком много метрик приводит к "аналитическому параличу". Нужно выбирать ключевые
      индикаторы (KPIs).

8. **Будущее метрик:**
    - **AI-ассистированный анализ:** LLM для генерации инсайтов из метрик, автоматического написания пояснений к
      изменениям.
    - **Прогнозное тестирование:** Системы, которые на основе метрик предсказывают, какие области кода станут
      проблемными в будущем, и предлагают превентивные тесты.
    - **Интеграция с DORA-метриками:** Связь метрик тестирования с DevOps Research and Assessment метриками (частота
      деплоев, время восстановления и т.д.).

- [Содержание](#содержание)

---

# **Техники тест-дизайна**

## **Junior Level**

Техники тест-дизайна — это систематические методы создания тестовых случаев, которые помогают эффективно проверять
программное обеспечение, находя максимальное количество дефектов при минимальном количестве тестов. Они основаны на
логике, математике и понимании работы системы.

Основные техники:

1. **Эквивалентное разбиение (Equivalence Partitioning):** Разделение входных данных на группы, которые система должна
   обрабатывать одинаково. Достаточно протестировать по одному значению из каждой группы.
2. **Анализ граничных значений (Boundary Value Analysis):** Тестирование значений на границах этих групп, где чаще всего
   возникают ошибки.
3. **Причина-Следствие (Cause-Effect Graphing):** Создание графа, связывающего входные условия (причины) с выходными
   действиями (следствиями), затем преобразование в таблицу решений.
4. **Таблица решений (Decision Table):** Систематическое представление комбинаций условий и соответствующих действий.
5. **Диаграмма переходов состояний (State Transition):** Тестирование систем, которые могут находиться в разных
   состояниях и переходить между ними.
6. **Попарное тестирование (Pairwise Testing):** Проверка всех возможных пар значений параметров вместо полного перебора
   комбинаций.

Для QA инженера владение этими техниками позволяет создавать осмысленные, полные и эффективные тесты вместо случайного
подбора проверок.

## **Middle Level**

С технической точки зрения применение этих техник в автоматизированном тестировании на Python требует специфических
подходов и инструментов:

1. **Эквивалентное разбиение и граничные значения:**
    - **Реализация в pytest:** Параметризация через `@pytest.mark.parametrize` с явным перечислением классов
      эквивалентности и граничных значений.
    - **Генерация данных:** Использование `itertools` или кастомных генераторов для создания тестовых данных.
    - **Пример:** Для функции, принимающей возраст от 18 до 65, тестируем значения: 17 (ниже границы), 18 (нижняя
      граница), 30 (внутри), 65 (верхняя граница), 66 (выше границы).

2. **Таблицы решений:**
    - **Data-driven подход:** Хранение таблиц в CSV, JSON, YAML или Excel. Использование `pytest` с фикстурой, которая
      загружает таблицу и преобразует в параметры.
    - **Библиотеки:** `pandas` для работы со сложными таблицами.
    - **Пример:** Таблица с условиями "статус пользователя" и "сумма заказа" и соответствующими скидками.

3. **Диаграмма переходов состояний:**
    - **Моделирование состояний:** Использование конечных автоматов (`transitions`, `automaton` библиотеки) или
      собственной реализации.
    - **Тестирование последовательностей:** Создание тестов, которые проверяют валидные и невалидные переходы между
      состояниями.
    - **Пример:** Тестирование жизненного цикла заказа: создан -> оплачен -> отправлен -> доставлен.

4. **Попарное тестирование:**
    - **Инструменты:** `allpairspy`, `pairwise` для генерации комбинаций.
    - **Интеграция с pytest:** Генерация параметров для тестов на лету.
    - **Пример:** Система с 10 параметрами, каждый из которых может принимать 10 значений. Полный перебор — 10^10
      комбинаций, попарное тестирование — порядка 100-200 комбинаций.

5. **Причина-Следствие:**
    - **Преобразование в таблицу решений:** После построения графа преобразование в таблицу и реализация как data-driven
      тесты.
    - **Инструменты:** Часто делается вручную или с помощью специализированных инструментов.

6. **Дополнительные техники:**
    - **Use Case Testing:** Реализация через BDD (`behave`, `pytest-bdd`).
    - **Error Guessing:** Накопление шаблонов ошибок и создание тестов на их основе.
    - **Exploratory Testing:** Хотя это ручная техника, можно автоматизировать некоторые аспекты через скрипты, которые
      выполняют случайные действия.

## **Senior Level**

На этом уровне техники тест-дизайна рассматриваются как формальные методы, интегрированные в процесс разработки, с
применением метапрограммирования, статического анализа и машинного обучения.

1. **Формальные методы и автоматическая дедукция:**
    - **Symbolic Execution:** Хотя в Python сложно реализовать полное символьное выполнение, инструменты вроде `pyexz3`
      пытаются анализировать пути выполнения и генерировать тестовые данные для покрытия всех ветвей.
    - **Теоретико-множественная основа:** Эквивалентное разбиение — это разбиение множества входных данных на классы
      эквивалентности (отношение эквивалентности). Граничные значения — это элементы на границах этих классов.
    - **Логическая полнота:** Таблицы решений обеспечивают проверку всех комбинаций условий, что соответствует полному
      покрытию по условиям (condition coverage).

2. **Автоматическая генерация тестов на основе техник:**
    - **Статический анализ кода для определения границ:** Инструмент, который анализирует AST кода, находит сравнения (
      `<`, `>`, `<=`, `>=`, `==`) и автоматически определяет граничные значения для параметров.
    - **Property-based тестирование как обобщение:** Библиотека `hypothesis` использует техники эквивалентного разбиения
      и граничных значений внутри стратегий генерации данных. Можно создавать кастомные стратегии, отражающие
      специфичные для домена разбиения.
    - **Модель-ориентированное тестирование (MBT):** Создание формальной модели системы (например, на языке `pydantic`
      или с помощью специализированных инструментов) и автоматическая генерация тестовых последовательностей,
      покрывающих все состояния и переходы.

3. **Динамическое применение техник в зависимости от контекста:**
    - **Адаптивный тест-дизайн:** Система, которая анализирует результаты предыдущих тестов (покрытие, найденные
      дефекты) и выбирает, какие техники применять дальше. Например, если в модуле много граничных ошибок — увеличивает
      количество тестов на граничные значения.
    - **Machine Learning для оптимизации комбинаций:** Использование ML для предсказания, какие комбинации параметров в
      попарном тестировании наиболее критичны, на основе исторических данных о дефектах.
    - **Генерация тестов на основе мутаций:** Инструменты мутационного тестирования (`mutmut`) могут быть расширены для
      генерации тестов, убивающих конкретные мутации, что косвенно применяет технику "предугадывания ошибок".

4. **Интеграция в CI/CD и DevOps:**
    - **Динамическое определение тестового набора:** На основе изменений кода (`git diff`) автоматическое определение,
      какие техники тест-дизайна применить к измененным модулям. Например, если изменено условие — генерировать тесты на
      граничные значения.
    - **Приоритизация тестов на основе риска:** Комбинирование техник тест-дизайна с анализом рисков. Более рискованные
      модули тестируются с использованием большего количества техник и более тщательно.
    - **Тестирование в продакшене (Production):** Применение техник тест-дизайна к данным из продакшена (
      анонимизированным) для создания более репрезентативных тестовых сценариев.

5. **Для AQA:**
    - **Мета-тестирование техник тест-дизайна:** Написание тестов, которые проверяют, что техники тест-дизайна
      применяются корректно. Например, тест, который анализирует параметризованные тесты и проверяет, что в них
      присутствуют значения из всех классов эквивалентности. Это можно сделать через анализ AST и декораторов pytest.
    - **Автоматическое построение диаграмм переходов состояний:** Инструмент, который путем статического анализа кода (
      поиск паттернов состояний) или динамического анализа (трассировка выполнения) строит диаграмму переходов, а затем
      генерирует тесты для покрытия всех переходов.
    - **Комбинаторные взрывы и их обход:** Для систем с огромным количеством параметров (например, конфигурация облачной
      инфраструктуры) использование продвинутых комбинаторных техник: ортогональные массивы, покрывающие массивы (
      covering arrays), t-wise тестирование (где t > 2). Интеграция с инструментами вроде `ACTS` (NIST).
    - **Тест-дизайн для вероятностных систем:** Для систем с недетерминированным поведением (например, рекомендательные
      системы, AI) использование статистических техник: проверка распределений выходных данных, доверительные интервалы,
      A/B тестирование в автоматическом режиме.
    - **Фаззинг с интеллектуальной генерацией данных:** Инструменты вроде `atheris` (на основе libFuzzer) могут быть
      дополнены знаниями о домене: подсказки о граничных значениях и классах эквивалентности для более эффективного
      поиска краевых случаев.
    - **Тест-дизайн для безопасности:** Применение техник тест-дизайна для поиска уязвимостей. Например, анализ
      граничных значений для буферов (переполнение), таблицы решений для контроля доступа.
    - **Визуализация и анализ покрытия техник:** Дашборды, которые показывают, какие техники тест-дизайна применялись к
      каждому модулю, и как это коррелирует с количеством найденных дефектов. Использование графовых баз данных для
      хранения отношений между тестами, техниками и дефектами.

6. **Ограничения и эволюция:**
    - **Контекстная зависимость:** Эффективность техник зависит от типа системы (например, для ML-моделей техники должны
      быть другими).
    - **Human-in-the-loop:** Некоторые техники (как Error Guessing) требуют экспертизы, которую сложно формализовать.
    - **Эволюция с развитием AI:** Возможность использования LLM для предложения тестовых случаев на основе
      естественного описания функциональности.

- [Содержание](#содержание)

---

# **Автоматизация тестирования**

## **Junior Level*

Автоматизация тестирования — это процесс использования специальных инструментов и скриптов для выполнения тестов,
проверки результатов и сравнения фактического поведения системы с ожидаемым без непосредственного участия человека.
Вместо того чтобы вручную кликать по интерфейсу или проверять API, мы пишем код, который делает это за нас.

Основные преимущества:

- **Скорость:** Автоматические тесты выполняются гораздо быстрее ручных.
- **Повторяемость:** Тесты можно запускать сколько угодно раз с одинаковой точностью.
- **Раннее обнаружение ошибок:** Автотесты можно запускать при каждом изменении кода, быстро находя регрессии.
- **Освобождение времени тестировщиков:** Позволяет сосредоточиться на сложных, исследовательских и нефункциональных
  тестах.

Для QA инженера автоматизация — это ключевой навык, который позволяет масштабировать тестирование, интегрировать его в
процесс разработки (CI/CD) и повышать общее качество продукта.

# ## **Middle Level**

С технической точки зрения автоматизация тестирования в Python-экосистеме охватывает несколько уровней и требует знания
конкретных инструментов и подходов.

1. **Уровни автоматизации (по пирамиде тестирования):**
    - **Unit-тесты:** Автоматизация с помощью `pytest`, `unittest`. Использование моков (`unittest.mock`) для изоляции.
    - **Интеграционные тесты:** Автоматизация проверки API (`requests`, `httpx`), взаимодействия с БД (транзакции,
      фикстуры), внешними сервисами.
    - **UI-тесты:** Автоматизация веб-интерфейсов через `Selenium WebDriver`, `Playwright`, `Cypress`. Использование
      паттерна Page Object.
    - **Нагрузочные тесты:** Автоматизация с помощью `locust`, `k6`, `JMeter`.

2. **Ключевые принципы автоматизации:**
    - **Поддерживаемость:** Код автотестов должен быть чистым, хорошо организованным и легко изменяемым.
    - **Стабильность:** Тесты должны быть надежными, не должны давать ложные срабатывания (flaky tests).
    - **Изоляция:** Тесты не должны зависеть друг от друга и от внешнего состояния.
    - **Информативность:** При падении тест должен давать четкое сообщение о том, что пошло не так.

3. **Инструменты и фреймворки:**
    - **Основной фреймворк:** `pytest` — де-факто стандарт для написания тестов в Python благодаря простоте, фикстурам и
      плагинам.
    - **Управление зависимостями:** `pip`, `poetry`, `pipenv`.
    - **CI/CD интеграция:** `Jenkins`, `GitLab CI`, `GitHub Actions`, `CircleCI` для автоматического запуска тестов.
    - **Управление тестовыми данными:** `factory_boy`, `Faker` для генерации данных.
    - **Отчетность:** `Allure`, `pytest-html`, `ReportPortal`.

4. **Паттерны автоматизации:**
    - **Page Object:** Для UI-тестов, инкапсулирует работу с элементами страницы.
    - **Screenplay:** Более современная альтернатива Page Object, фокусируется на взаимодействии пользователя с
      системой.
    - **Data-Driven Testing:** Отделение тестовых данных от логики теста (например, хранение данных в JSON, CSV).
    - **Keyword-Driven Testing:** Использование ключевых слов для описания тестовых шагов (часто используется в Robot
      Framework).

5. **Для AQA:**
    - **Выбор правильного уровня автоматизации:** Не все нужно автоматизировать. Критерии: частота выполнения, важность,
      стабильность функционала, сложность ручного тестирования.
    - **Баланс автоматизации:** Соотношение времени на написание автотестов и время на ручное тестирование. Обычно
      20-30% времени на автоматизацию, остальное — на исследовательское тестирование и анализ.
    - **Интеграция в процесс разработки:** Автотесты должны запускаться при каждом коммите (unit-тесты) и
      пулл-реквесте (интеграционные), а также ночью (полный регресс).

# ## **Senior Level**

На этом уровне автоматизация рассматривается как стратегическая дисциплина, интегрированная в архитектуру, процессы и
культуру разработки.

1. **Архитектура автоматизированной тестовой системы:**
    - **Многослойная архитектура:** Разделение на уровни: слой тестовых сценариев (тест-кейсы), слой бизнес-логики (Page
      Objects, API клиенты), слой утилит (хелперы, генераторы данных), слой драйверов (Selenium, requests).
    - **Принципы проектирования:** Применение SOLID, DRY, KISS к коду автотестов. Использование паттернов
      проектирования (Фабрика, Стратегия, Команда) для повышения гибкости и поддерживаемости.
    - **Микросервисная архитектура для тестов:** В больших распределенных системах сама тестовая система может быть
      построена как набор микросервисов: оркестратор тестов, генератор данных, сборщик отчетов, система нотификаций.

2. **Динамическая и адаптивная автоматизация:**
    - **Генерация тестов на лету:** Использование метапрограммирования (`type`, декораторы, метаклассы) для создания
      тестов в runtime на основе конфигурации или данных.
    - **Самоадаптирующиеся тесты:** Тесты, которые анализируют окружение и подстраиваются под него (например,
      определяют, какая версия API доступна, и выбирают соответствующую стратегию).
    - **Интеллектуальные тестовые данные:** Система, которая анализирует продакшен-данные (с соблюдением анонимности) и
      генерирует репрезентативные тестовые данные, сохраняя распределения и корреляции.

3. **Автоматизация в контексте DevOps и SRE:**
    - **Тестирование как часть инфраструктуры:** Инфраструктурные тесты (например, с помощью `terraform test` или
      `inspec`) для проверки конфигурации окружений.
    - **Тестирование надежности (Reliability Testing):** Автоматизация тестов на отказоустойчивость (chaos engineering)
      с помощью инструментов `chaostoolkit`, `Litmus`.
    - **Тестирование в продакшене:** Автоматизация canary-релизов, A/B тестов, проверки метрик мониторинга (Prometheus,
      Grafana) после деплоя.

4. **AI и ML в автоматизации тестирования:**
    - **Генерация тестовых сценариев:** Использование LLM (например, GPT) для создания тест-кейсов на основе
      пользовательских историй или документации.
    - **Визуальная валидация:** Компьютерное зрение (`opencv`, `pytesseract`) для сравнения скриншотов, поиска элементов
      на основе изображений.
    - **Предсказание падений тестов:** ML-модели, которые на основе истории выполнения предсказывают, какие тесты
      вероятнее упадут при данном изменении кода.
    - **Автоматический анализ root cause:** Система, которая анализирует падение теста, логи и изменения кода, и
      предлагает вероятную причину.

5. **Для AQA:**
    - **Мета-автоматизация:** Автоматизация процесса создания автотестов. Например, инструмент, который по OpenAPI
      спецификации генерирует базовые API-тесты, или по HTML-странице — скелет Page Object.
    - **Распределенное выполнение тестов в облаке:** Система, которая динамически запускает тесты в облачных средах (AWS
      Lambda, Google Cloud Functions) для экономии ресурсов и ускорения выполнения. Использование Kubernetes для
      оркестрации тысяч тестовых подов.
    - **Тестирование времени выполнения (Runtime Verification):** Внедрение агентов в продакшен-код, которые в реальном
      времени проверяют инварианты, контракты и бизнес-правила, отправляя алерты при нарушениях. Это форма "
      перманентного" автоматизированного тестирования.
    - **Эмуляция пользовательского поведения:** Создание "виртуальных пользователей", которые не просто следуют жестким
      сценариям, а используют reinforcement learning для изучения интерфейса и нахождения путей к ошибкам.
    - **Автоматизация исследования (Exploratory Testing Automation):** Инструменты, которые комбинируют случайные
      действия с эвристиками для исследования приложения и обнаружения неочевидных дефектов. Например, комбинация
      фаззинга для UI.
    - **Тестирование безопасности (Security Testing Automation):** Интеграция статических (SAST) и динамических (DAST)
      анализаторов в пайплайн, автоматическое сканирование зависимостей на уязвимости, автоматическое тестирование на
      инъекции и другие атаки.
    - **Перформанс-тестирование как код:** Описание нагрузочных тестов в виде кода (например, на Python с `locust`),
      который версионируется, ревьюится и запускается автоматически. Автоматический анализ результатов и сравнение с
      baseline.

6. **Экономика и ROI автоматизации:**
    - **Расчет ROI:** Формулы для оценки возврата инвестиций в автоматизацию. Учет факторов: время на разработку и
      поддержку, сокращение времени ручного тестирования, уменьшение стоимости дефектов, найденных позже.
    - **Оптимизация стоимости выполнения:** Использование spot-инстансов в облаке, отключение тестовых окружений когда
      они не используются, кэширование зависимостей.
    - **Приоритизация автоматизации:** Матрица принятия решений: что автоматизировать в первую очередь на основе частоты
      изменений, бизнес-критичности, сложности ручного тестирования.

7. **Культурные и организационные аспекты:**
    - **Shift-Left и Shift-Right:** Автоматизация тестирования на всех этапах: от статического анализа кода до
      мониторинга в проде.
    - **Collaboration между разработчиками и QA:** Разработчики пишут unit-тесты, QA фокусируется на интеграционных и
      E2E. Использование pull request ревью для кода автотестов.
    - **Обучение и менторинг:** Создание внутренних библиотек, шаблонов, проведение воркшопов для распространения лучших
      практик автоматизации.

8. **Будущее автоматизации:**
    - **Автономные тестовые системы:** Системы, которые сами определяют, что тестировать, генерируют тесты, выполняют
      их, анализируют результаты и вносят изменения в код (самоисцеляющиеся тесты).
    - **Тестирование в метавселенных и AR/VR:** Новые инструменты для автоматизации тестирования 3D-интерфейсов и
      иммерсивных сред.
    - **Квантовое тестирование:** С появлением квантовых компьютеров потребуются новые подходы к автоматизации
      тестирования квантовых алгоритмов и программ.

- [Содержание](#содержание)

---

Лягушка
