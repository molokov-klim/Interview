# Собеседование Python AQA

# Содержание

## Базовые знания

1. [Типы данных](#типы-данных)
2. [*args и **kwargs](#args-и-kwargs)
3. [Хеш-таблица](#хеш-таблица)
4. [Встроенные функции](#встроенные-функции)
5. [Контекстные менеджеры (with)](#контекстные-менеджеры-with)
6. [Генераторы и итераторы](#генераторы-и-итераторы)
7. [Декораторы и замыкания](#декораторы-и-замыкания)
8. [GIL (Global Interpreter Lock)](#gil-global-interpreter-lock)
9. [Изменение списка во время итерации](#изменение-списка-во-время-итерации)
10. [Области видимости (scope)](#области-видимости-scope)
11. [Lambda-функции](#lambda-функции)
12. [Comprehensions и генераторные выражения](#comprehensions-и-генераторные-выражения)
13. [copy() и deepcopy()](#copy-и-deepcopy)
14. [Асинхронность (Async/Await)](#асинхронность-asyncawait)
15. [Dataclass](#dataclass)
16. [Enum](#enum)
17. [Garbage Collector (сборщик мусора)](#garbage-collector-сборщик-мусора)
18. [Виды сложности кода](#виды-сложности-кода)

## ООП

1. [Парадигмы ООП](#парадигмы-ооп)
2. [Абстракция (ООП)](#абстракция-ооп)
3. [Инкапсуляция (ООП)](#инкапсуляция-ооп)
4. [Наследование (ООП)](#наследование-ооп)
5. [Полиморфизм (ООП)](#полиморфизм-ооп)
6. [Магические методы](#магические-методы)
7. [Инвариантность и ковариантность](#инвариантность-и-ковариантность)
8. [Декораторы классов и методов](#декораторы-классов-и-методов)
9. [Множественное наследование и MRO](#множественное-наследование-и-mro)
10. [ABC (Abstract Base Classes)](#abc-abstract-base-classes)
11. [Протокол (Protocol)](#протокол-protocol)
12. [Паттерны проектирования](#паттерны-проектирования)
13. [Композиция и агрегация](#композиция-и-агрегация)
14. [Связность и связанность](#связность-и-связанность)
15. [SOLID](#solid)
16. [Специфика ООП в Python](#специфика-ооп-в-python)
17. [Наследование и композиция](#наследование-и-композиция)
18. [Метапрограммирование](#метапрограммирование)
19. [Подводные камни ООП в Python](#подводные-камни-ооп-в-python)

## Типизация

1. [typing: Optional, Union, TypeVar, Generic](#typing-optional-union-typevar-generic)
2. [Literal, TypedDict, Protocol](#literal-typeddict-protocol)
3. [Covariance/Contravariance](#covariancecontravariance)

## Инструменты

1. [pytest](#pytest)
2. [pytest hooks](#pytest-hooks)
3. [Kubernetes](#kubernetes)

## Теория тестирования

1. [Пирамида тестирования](#пирамида-тестирования)
2. [Виды тестирования](#виды-тестирования)
3. [Метрики](#метрики)
4. [Test Design Techniques](#test-design-techniques)
5. [Автоматизация](#автоматизация)

---

# Типы данных

Отлично, подготовлю ответ на вопрос о типах данных, соответствующий уровню позиции Senior AQA. Понимание внутреннего
устройства языка — это ключ к написанию стабильных, эффективных тестов и анализу сложных дефектов.

### **Типы данных в Python**

**Junior Level**

В Python, как и в любом языке программирования, данные бывают разных видов или «типов». Это базовые кирпичики, с
которыми работает программа. Их можно разделить на две большие категории: изменяемые и неизменяемые.

Простые, или «скалярные» типы — это числа (целые, вещественные, комплексные), строки текста и логические значения (
`True`/`False`). Важное свойство строк и чисел — их **нельзя изменить после создания**. Если ты пишешь `x = "hello"`, а
потом `x = "world"`, ты не меняешь строку "hello", а создаешь новую и даешь ей то же имя `x`.

Более сложные, «коллекционные» типы — это списки, кортежи, словари и множества. Они нужны для хранения набора других
значений. Здесь ключевое различие: **список можно изменить** (добавить или удалить элемент), а **кортеж — нет**. Словарь
хранит данные в парах «ключ-значение», и его тоже можно изменять.

Понимание, какой тип перед тобой и можно ли его изменить, критически важно для избегания ошибок в коде и тестах.

**Middle Level**

На этом уровне мы рассматриваем типы не просто как «коробки для данных», а как объекты со строгой классификацией,
определенным поведением и моделью в памяти.

1. **Классификация по мутабельности (изменяемости):**

* **Неизменяемые (immutable):** `int`, `float`, `complex`, `str`, `bytes`, `tuple`, `frozenset`, `bool`, `NoneType`.
  Экземпляр такого типа после создания не может быть изменен. Любая операция, выглядящая как изменение, на деле
  создает новый объект. Это имеет глубокие последствия для хэшируемости (объекты этих типов обычно хэшируемы),
  использования в качестве ключей словаря и поведения при передаче в функции (передается ссылка на объект, но так
  как объект нельзя изменить, создается иллюзия передачи «по значению»).
* **Изменяемые (mutable):** `list`, `dict`, `set`, `bytearray`, пользовательские классы (по умолчанию). Содержимое
  таких объектов можно менять. При передаче в функцию передается ссылка на тот же самый объект, поэтому изменения,
  сделанные внутри функции, видны снаружи. Это частая причина коварных багов в тестах, связанных с непреднамеренным
  изменением фикстур или конфигураций.

2. **Устройство объектов:** В Python всё является объектом. Каждый объект имеет три обязательных атрибута: *
   *идентификатор** (уникальный числовой адрес в памяти, возвращаемый `id()`), **тип** (определяющий возможные операции,
   возвращаемый `type()`) и **значение**. Для неизменяемых типов идентификатор и значение жестко связаны. Для
   изменяемых — объект может менять значение, сохраняя идентификатор.

3. **Специфика типов:**

* `None` — синглтон, объект, обозначающий отсутствие значения. `id(None)` всегда одинаков.
* Булевы значения `True` и `False` — также синглтоны и являются подклассами `int`.
* `tuple` — неизменяем, но если содержит изменяемые элементы (например, списки), то эти внутренние элементы менять
  можно. Это делает кортеж «условно хэшируемым».
* `dict` начиная с Python 3.7 гарантирует сохранение порядка вставки, а с 3.6 это было особенностью реализации
  CPython. Для AQA это важно при тестировании API, возвращающих JSON.
* `set` и `frozenset` хранят только уникальные, хэшируемые элементы. Их внутренняя реализация близка к словарю, где
  есть только ключи.

Для AQA middle-уровня важно использовать это понимание для проектирования тестовых данных, предсказания побочных
эффектов в тестируемом коде и грамотного сравнения ожидаемого и фактического результатов (например, понимая, что
сравнение `[1,2,3] == [1,2,3]` дает `True`, но `id()` у этих списков разный).

**Senior Level (Кровь, кишки и CPython)**

Здесь мы спускаемся на уровень байткода, структуры `PyObject` и менеджмента памяти. Senior AQA должен понимать это,
чтобы отлаживать сложнейшие race conditions, утечки памяти в тестируемом приложении или анализировать аномалии
производительности.

1. **Всё есть `PyObject`:** В исходном коде CPython (`Include/object.h`) лежит фундамент — структура `PyObject`. Это
   базовый «контейнер» для любого типа данных.

```c
typedef struct _object {
Py_ssize_t ob_refcnt;  // Счетчик ссылок — основа механизма GC
PyTypeObject *ob_type; // Указатель на структуру типа
} PyObject;
```

Каждый объект в Python начинается с этих двух полей. `ob_refcnt` — счетчик ссылок для сборщика мусора. `ob_type` —
указатель на другой объект — его тип, который сам является объектом (`PyTypeObject`). Добавление новых полей
происходит путем «расширения» этой структуры. Например, `PyLongObject` (целое число) добавляет поле для хранения
цифр.

2. **Неизменяемость и интернирование (interning):** Это не просто договоренность, а оптимизация на уровне
   интерпретатора.

* **Маленькие целые числа:** Диапазон обычно от -5 до 256. Эти объекты создаются при запуске интерпретатора и
  хранятся в специальном массиве. Операция `a = 10; b = 10` приведет к тому, что `a` и `b` будут указывать на **один
  и тот же** объект в памяти (`id(a) == id(b)`). Проверка делается макросом `PyLong_FromLong`.
* **Строки (interned strings):** Если строка состоит только из символов ASCII, букв, цифр и подчеркивания, и не
  выглядит как число, она также может быть интернирована. Это особенно важно для имен переменных, атрибутов. Python
  делает это автоматически, но можно принудительно интернировать строку через `sys.intern()`. Это ускоряет сравнение
  строк (достаточно сравнить указатели, `a is b`) и экономит память в случае множества одинаковых строк (например,
  при парсинге больших XML/JSON в тестах).

3. **Мутабельность и байткод:** Рассмотрим операцию `my_list.append(x)`. Байткод инструкция `LIST_APPEND` работает
   непосредственно с внутренним C-массивом структуры `PyListObject`. Объект списка (`list`) хранит указатель (
   `**ob_item`) на этот массив указателей на `PyObject` и его текущую длину (`ob_size`). `LIST_APPEND` увеличивает
   `ob_size`, при необходимости перераспределяет память для `ob_item` (сложность amortized O(1)) и помещает в новый слот
   ссылку на `x`, увеличивая `ob_refcnt` у объекта `x`. Никакого нового `list` не создается, `id(my_list)` остается
   прежним.

4. **Словарь: краеугольный камень языка.** `dict` — не просто тип, это фундаментальная структура, используемая
   повсеместно: пространства имен модулей, атрибуты объектов, передача аргументов в функции (`**kwargs`) реализованы
   через словари. Его внутренности — это хэш-таблица (массив `PyDictKeyEntry`). Ключевая хитрость в том, как разрешаются
   коллизии (метод открытой адресации). При удалении многих элементов словарь может оставаться разреженным, что ведет к
   утечкам памяти. Для AQA это означает, что долгоживущие объекты с большими изменяющимися словарями (например, кэши в
   тестируемом приложении) требуют мониторинга памяти. Словари также резко замедляются при атаках хэш-коллизиями, что
   может быть вектором DoS-атаки — это важно для security-тестирования.

5. **`tuple` vs `list`: Не просто мутабельность.** `tuple` из-за своей неизменности аллоцируется одной непрерывной
   областью памяти. Его `ob_item` — это встроенный массив указателей фиксированного размера. `list` же имеет буфер с
   «запасом» (`allocated`), чтобы не переаллоцировать память при каждом `append`. Сравнение `is` для кортежей,
   содержащих только неизменяемые элементы, может давать `True` благодаря механизму кэширования (`tupleobject.c`):
   Python может переиспользовать только что созданный кортеж, если он пуст или состоит из одного элемента.

6. **Типизация для AQA:** На этом уровне мы понимаем, что система типов Python — динамическая, но сильная (strong).
   «Утиная типизация» реализуется через механизм поиска атрибутов в `__dict__` объекта и далее по MRO (Method Resolution
   Order). Паттерн `isinstance(obj, abc.ABC)` или `hasattr(obj, '__len__')` на байткод-уровне сводится к проверкам
   `PyObject_IsInstance` и `PyObject_HasAttr`, которые проходят по цепочке классов. Для Senior AQA критично понимать эти
   механизмы при тестировании полиморфных компонентов, мокинге и создании сложных фикстур, имитирующих определенные
   интерфейсы.

- [Содержание](#содержание)

---

# *args и **kwargs

### ***args и **kwargs**

**Junior Level**

`*args` и `**kwargs` — это специальные синтаксические конструкции в Python, позволяющие функциям принимать произвольное
количество аргументов.

`*args` (от слова "arguments") собирает все **позиционные аргументы**, переданные функции сверх явно объявленных, в *
*кортеж**. Это полезно, когда вы не знаете заранее, сколько аргументов может понадобиться передать.

`**kwargs` (от "keyword arguments") собирает все **именованные аргументы** (ключ=значение), которые не были явно
перечислены в параметрах функции, в **словарь**. Это часто используется для передачи конфигурационных параметров или для
создания функций-оберток.

Также символы `*` и `**` используются при **вызове** функции для распаковки коллекций в отдельные аргументы. `*`
распаковывает итерируемый объект (список, кортеж) в позиционные аргументы, а `**` распаковывает словарь в именованные
аргументы.

Это фундаментальный механизм для создания гибких API и декораторов.

**Middle Level**

На этом уровне важно понимать **строгие правила порядка параметров** и внутреннее представление:

1. **Строгий порядок в определении функции**:

```
def f(a, b, *args, c=None, d=None, **kwargs)
```

Порядок следования:

- Позиционные параметры (a, b)
- `*args` — собирает избыточные позиционные аргументы
- Keyword-only аргументы (c, d) — могут быть переданы только по имени
- `**kwargs` — собирает избыточные именованные аргументы

После `*args` все последующие параметры становятся **keyword-only** (требуют явного указания имени). Это важная фича
для создания чистого API.

2. **Распаковка на уровне байткода**:
   Когда вы вызываете `func(*[1, 2, 3])`, происходит:

- Байткод `BUILD_LIST` создает список
- Байткод `CALL_FUNCTION_EX` с флагом `0x01` (HAVE_ARG_FLAGS) распаковывает итерируемый объект в аргументы
- Внутри функции эти аргументы доступны через `sys._getframe().f_locals`

3. **`*` и `**` — не магия, а синтаксический сахар**:
   Конструкция `def foo(*args, **kwargs)` компилируется в функцию с двумя специальными параметрами. При компиляции в
   байткод для них создаются отдельные инструкции для упаковки аргументов в кортеж и словарь.

4. **Важные нюансы для AQA**:

- При передаче словаря в `**kwargs` ключи **должны быть строками**
- Дублирование имен аргументов при распаковке приводит к `TypeError`
- `**kwargs` сохраняет порядок аргументов начиная с Python 3.6 (благодаря сохранению порядка вставки в словарях)
- Метод `__getitem__` объекта используется при распаковке через `**`, что позволяет распаковывать не только словари,
  но и любые mapping-объекты

**Senior Level (Байткод, CPython и грабли производительности)**

1. **Уровень байткода: как работает упаковка аргументов**:

Рассмотрим функцию:

```python
def func(*args, **kwargs):
    pass
```

Байткод для вызова `func(1, 2, a=3, b=4)`:

```
LOAD_CONST               1 (1)
LOAD_CONST               2 (2)
LOAD_CONST               3 (3)
LOAD_CONST               4 (4)
LOAD_CONST               5 (('a', 'b'))  # Имена ключей
CALL_FUNCTION_KW         2           # 2 позиционных + keyword args
```

Байткод внутри `func` для доступа к аргументам:

```
# Для *args
BUILD_TUPLE              # Собирает позиционные аргументы в кортеж
STORE_FAST             0 (args)

# Для **kwargs  
BUILD_MAP                # Собирает именованные аргументы в словарь
STORE_FAST             1 (kwargs)
```

В CPython 3.10+ используется `CALL` с флагами вместо отдельных `CALL_FUNCTION*` инструкций.

2. **CPython: механизм вызова функций**:

В `Include/cpython/abstract.h` функция `PyObject_Call` принимает `PyObject *args` (кортеж) и `PyObject *kwargs` (
словарь). Вся система вызова построена вокруг этих двух структур.

Когда интерпретатор видит `*args` в вызове, он выполняет:

- `PySequence_Tuple` для преобразования итерируемого объекта в кортеж
- `PyTuple_New` для создания нового кортежа аргументов
- Конкатенацию с существующими позиционными аргументами

3. **Критические оптимизации CPython 3.11+**:

В Python 3.11 появилась **специализация байткода для вызовов функций** (PEP 659). Интерпретатор создает "
адаптивные" (adaptive) инструкции вызова, которые кэшируют:

- Форму вызова (позиционные vs именованные аргументы)
- Типы передаваемых аргументов
- Количество аргументов

Например, вызов `func(*args)` без именованных аргументов компилируется в специализированную версию `CALL`, которая
пропускает проверки на наличие `kwargs`. Это дает до 50% ускорения для частых вызовов.

**Но!** Если функция определена как `def f(**kwargs)`, а вызывается как `f(*args)`, происходит деоптимизация — сброс
кэша и возврат к обобщенному медленному пути.

4. **Frame object и доступ к аргументам**:

Локальные переменные функции (включая `args` и `kwargs`) хранятся в `frame->f_localsplus` — массиве указателей
`PyObject*`. `*args` занимает один слот (указатель на кортеж), `**kwargs` — один слот (указатель на словарь).

Доступ через `sys._getframe()` позволяет инспектировать это в runtime, что используется в продвинутых тестовых
фреймворках для анализа вызовов.

5. **`inspect.Signature` и валидация аргументов**:

Модуль `inspect` использует `__code__.co_varnames`, `__code__.co_argcount`, `__code__.co_kwonlyargcount` для
реконструкции сигнатуры. `*args` соответствует `__code__.co_flags & 0x04` (CO_VARARGS), `**kwargs` — `0x08` (
CO_VARKEYWORDS).

Для AQA: это позволяет создавать динамические валидаторы аргументов в тестовых фреймворках.

6. **Производительность и антипаттерны**:

- **Двойная упаковка/распаковка**: `func(*tuple(args), **dict(kwargs))` создает **новые** кортеж и словарь, копируя
  все элементы. Вместо этого нужно использовать прямое присваивание.

- **Рекурсивная распаковка в циклах**:

```python
for item in items:
    process(**item)  # Создание нового словаря для каждого вызова!
```

Лучше: `process(key1=item['key1'], key2=item['key2'])` если сигнатура известна.

- **Большие `*args`**: передача огромного списка через `*` приводит к созданию кортежа из всех элементов на стеке
  вызовов, что может вызвать `RecursionError` при глубокой рекурсии.

7. **Специфика для декораторов (как для AQA)**:

При написании декораторов для тестов:

```python
def retry(max_attempts):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)  # Внимание!

                except AssertionError:
                    if attempt == max_attempts - 1:
                        raise

        return wrapper

    return decorator
```

**Проблема**: каждый вызов `func(*args, **kwargs)` внутри `wrapper` создает **новые** кортеж и словарь. Для high-load
тестов это может стать бутылочным горлышком. Решение — использовать `functools.wraps` который кэширует signature.

8. **Интроспекция и мокирование**:

Для создания умных моков в тестах нужно понимать, как `unittest.mock` работает с `*args`/`**kwargs`:

- `mock.call_args` хранит `args` как кортеж и `kwargs` как словарь
- `mock.assert_called_with(*args, **kwargs)` использует ту же семантику распаковки
- При `side_effect = lambda *a, **k: ...` сигнатура должна точно соответствовать

9. **C-расширения и `PyArg_ParseTupleAndKeywords`**:

В нативных модулях функция `PyArg_ParseTupleAndKeywords` принимает:

```c
static PyObject* func(PyObject *self, PyObject *args, PyObject *kwargs)
{
char *keywords[] = {"param1", "param2", NULL};
// Парсинг args и kwargs
}
```

Здесь `args` и `kwargs` — те же PyObject*, что и в Python-функциях. Это знание критично для тестирования нативных
расширений.

- [Содержание](#содержание)

---

# Хеш-таблица

### **Хеш-таблица в Python**

**Junior Level**

Хеш-таблица — это структура данных, которая позволяет очень быстро находить, добавлять и удалять элементы. В Python
хеш-таблицы лежат в основе двух ключевых типов: **словарей (dict)** и **множеств (set)**.

Представьте себе библиотеку с книгами. Вместо того чтобы искать книгу по названию, перебирая все полки, вы вычисляете
номер полки по названию книги по определенному правилу (например, первая буква). Это правило — **хеш-функция**. Она
преобразует ключ (название книги) в число (номер полки). В идеальном случае вы сразу идете к нужной полке и находите
книгу за O(1) время.

Коллизии (когда две разные книги должны лежать на одной полке) решаются разными способами — например, на полке может
быть несколько книг, и вы тогда ищете среди них уже по полному названию.

В Python словарь — это коллекция пар «ключ-значение», где ключ должен быть **хешируемым** (неизменяемым) объектом.
Множество — это коллекция уникальных хешируемых элементов.

**Middle Level**

В Python хеш-таблицы реализованы через открытую адресацию (open addressing) с двойным хешированием для разрешения
коллизий.

**Ключевые аспекты:**

1. **Хешируемость:** Объект хешируем, если:

- Имеет метод `__hash__`, возвращающий целое число
- Имеет метод `__eq__` для сравнения
- Удовлетворяет условию: `a == b` ⇒ `hash(a) == hash(b)`

Неизменяемые типы (int, str, tuple, frozenset) хешируемы. Изменяемые (list, dict, set) — нет, но могут стать
хешируемыми, если реализовать неизменяемую версию.

2. **Размер таблицы:** Всегда является степенью двойки. Это позволяет использовать быструю битовую маску для вычисления
   индекса: `index = hash(key) & (table_size - 1)`.

3. **Коэффициент загрузки (load factor):** При достижении ~2/3 заполнения таблица увеличивается вдвое, происходит *
   *rehashing** — пересчет позиций всех элементов. Это амортизированная операция O(n).

4. **Разрешение коллизий:** Используется **квадратичное зондирование (quadratic probing)** вида
   `index = (5*index + 1 + perturb) & mask`, где `perturb` изначально равен хешу, а затем сдвигается. Это обеспечивает
   хорошее распределение.

5. **Удаление элементов:** При удалении элемент не удаляется физически, а помечается как **dummy** (удаленный слот). Это
   необходимо для сохранения цепочек зондирования.

6. **Порядок элементов:** Начиная с Python 3.7/3.6 (как деталь реализации CPython) порядок вставки сохраняется. Это
   достигается тем, что хеш-таблица хранит индексы в отдельном массиве записей (ключ-значение), который сохраняет
   порядок вставки.

**Senior Level (CPython, память, байткод и темные углы)**

1. **Структура PyDictObject:**

В `Include/cpython/dictobject.h`:

```c
typedef struct {
PyObject_HEAD
Py_ssize_t ma_used;      // Количество активных элементов
Py_ssize_t ma_version_tag // Уникальная версия для обнаружения изменений
PyDictKeysObject *ma_keys; // Указатель на ключи
PyObject **ma_values;      // Указатель на значения (для split-table)
} PyDictObject;
```

**Эволюция структур:**

- До Python 3.6: единая хеш-таблица размером 8 строк (indices + entries)
- Python 3.6+: **split-table layout**: отдельно массив индексов (`dk_indices`) и массив записей (`dk_entries`)
- Python 3.11+: **компактная модель** с кэшированием хешей

2. **Детали split-table layout:**

Массив `dk_indices` хранит не записи, а индексы в `dk_entries`:

- Для таблицы размером 8: `dk_indices[hash & 7] = i` где `i` — индекс в `dk_entries`
- `dk_entries` — массив структур `PyDictKeyEntry`, хранящих хеш, ключ, значение

Это дает:

- Сохранение порядка вставки (массив `dk_entries` заполняется последовательно)
- Улучшенную локальность памяти при итерации
- Меньшую фрагментацию памяти

3. **Хеш-атаки и SipHash:**

До Python 3.4 использовался простой хеш (FNV для строк), что позволяло проводить DoS-атаки через искусственное
создание коллизий. С Python 3.4 для строк, bytes и datetime используется **SipHash24** — криптографически стойкая
хеш-функция с ключом, рандомизируемым при запуске интерпретатора.

Ключ для SipHash хранится в `_Py_HashSecret` (глобальная переменная). Для тестирования можно установить
`PYTHONHASHSEED=0` для детерминированного поведения.

4. **Байткод операций с dict:**

- `LOAD_GLOBAL` → `PyDict_GetItem` по ключу-строке
- `STORE_SUBSCR` для dict → `PyObject_SetItem`
- `BUILD_MAP` → `_PyDict_NewPresized`

При компиляции словарных литералов `{k: v}` Python 3.9+ использует `BUILD_MAP` с предвычисленными хешами для
константных ключей.

5. **Оптимизации CPython 3.10+:**

- **PEP 603**: Добавлен `dict.__getitem__` с быстрым путем для строковых ключей
- **PEP 659**: Специализация байткода для операций со словарями:

```python
# Адаптивный (adaptive) байткод для dict[key]
# Первые несколько выполнений собирают статистику
# Если ключ всегда строка, генерируется специализированный байткод
# Который использует быстрый путь поиска в хеш-таблице
```

6. **Скрытые структуры для оптимизации:**

- **Keys-sharing (dict splitting)**: Когда создается много объектов с одинаковыми атрибутами (например, экземпляры
  класса), их `__dict__` могут разделять таблицу ключей (`ma_keys`), храня только значения в `ma_values`. Это
  экономит память.

- **Compact dict (Python 3.11)**: Запись `PyDictKeyEntry` уменьшена с 24 до 8 байт за счет выноса хеша в отдельный
  массив.

7. **Сборка мусора и weakref:**

Словари участвуют в циклическом GC через `Py_TPFLAGS_HAVE_GC`. Weakref-словари (`weakref.WeakKeyDictionary`,
`WeakValueDictionary`) используют специальные прокси-объекты, которые не увеличивают счетчик ссылок.

Важно: обычные словари хранят **сильные ссылки** на ключи и значения, что может приводить к утечкам памяти в циклах.

8. **Производительность и антипаттерны:**

- **Изменение ключа-объекта после вставки:** Если объект-ключ изменяется так, что меняется его хеш, он становится *
  *невозможным для нахождения**. Это коварный баг.

- **Частые resizing:** При добавлении N элементов в пустой словарь происходит ~log₂(N) ресайзов. Решение:
  `dict.fromkeys()` или предвыделение через `dict(initial_size)`.

- **Итерация с изменением:** Изменение размера словаря во время итерации вызывает `RuntimeError`. Но изменение
  значений (не ключей) безопасно.

- **Memory overhead:** Пустой словарь занимает ~72 байта (Python 3.11), каждая запись — дополнительно 8-32 байта в
  зависимости от размера.

9. **Интроспекция через CPython API:**

Для тестирования можно использовать:

```python
import sys

d = {}
sys.getsizeof(d)  # Размер всей структуры
d.__sizeof__()  # Тоже

# Просмотр внутренних структур (CPython specific)
d.__dictoffset__  # Смещение для __dict__ в объектах
```

10. **Для множеств (set):**

Используется та же хеш-таблица, но без значений (только ключи). Особенности:

- `set` хранит только хеш и ключ
- `frozenset` — неизменяемая версия, кэширует хеш самого множества
- Операции вроде `union`, `intersection` используют оптимизированные C-реализации

11. **Специфика тестирования хеш-таблиц:**

- **Тестирование коллизий:** Создание объектов с одинаковым хешем для проверки деградации производительности
- **Тестирование rehashing:** Измерение времени вставки при достижении порогов заполнения
- **Проверка сохранения порядка:** Гарантия, что `list(dict.keys())` соответствует порядку вставки
- **Тестирование memory leaks:** Убедиться, что удаление элементов освобождает память (но помнить про dummy-слоты)
- **Конкурентность:** Dict не потокобезопасен. Тестирование race conditions при одновременном чтении/записи.

---

# Встроенные функции

### **Встроенные функции Python (Built-in Functions)**

**Junior Level**

Встроенные функции — это функции, которые доступны в Python по умолчанию, без необходимости импорта каких-либо модулей.
Они представляют собой базовый инструментарий языка и всегда находятся в глобальной области видимости.

Эти функции охватывают основные операции: работу с типами данных (`str()`, `int()`, `list()`), математические
вычисления (`abs()`, `round()`, `sum()`), преобразования (`len()`, `sorted()`, `reversed()`), ввод-вывод (`print()`,
`input()`), итерации (`range()`, `enumerate()`, `zip()`), проверки (`isinstance()`, `hasattr()`), и другие
фундаментальные операции.

Важно понимать, что это не просто функции, а часть ядра языка. Они реализованы максимально эффективно и их поведение
стандартизировано.

**Middle Level**

1. **Пространство имен `builtins`**: Все встроенные функции находятся в модуле `builtins`, который автоматически
   импортируется при запуске интерпретатора. Можно получить прямой доступ через `import builtins`. Переопределение
   функций в этом модуле (что крайне не рекомендуется) повлияет на всю программу.

2. **Категории встроенных функций**:

- **Конструкторы типов**: `int()`, `str()`, `list()`, `dict()`, `set()`, `tuple()`, `bytes()`, `bytearray()`,
  `memoryview()`, `frozenset()`
- **Математические**: `abs()`, `divmod()`, `pow()`, `round()`, `sum()`, `min()`, `max()`
- **Преобразования и проверки**: `bool()`, `complex()`, `float()`, `hash()`, `id()`, `isinstance()`, `issubclass()`,
  `callable()`
- **Работа с последовательностями**: `len()`, `sorted()`, `reversed()`, `enumerate()`, `zip()`, `filter()`, `map()`,
  `all()`, `any()`, `slice()`
- **Итераторы и генераторы**: `iter()`, `next()`, `range()`
- **Ввод-вывод**: `print()`, `input()`, `open()`
- **Компиляция и выполнение**: `eval()`, `exec()`, `compile()`
- **Отражение (introspection)**: `dir()`, `globals()`, `locals()`, `vars()`, `getattr()`, `setattr()`, `delattr()`,
  `hasattr()`, `property()`, `classmethod()`, `staticmethod()`, `super()`
- **Разное**: `breakpoint()`, `__import__()`, `format()`, `repr()`, `ascii()`, `chr()`, `ord()`, `bin()`, `oct()`,
  `hex()`

3. **Особенности поведения**:

- `sorted()` всегда возвращает новый список, тогда как метод `list.sort()` изменяет список на месте
- `reversed()` возвращает итератор, а не список
- `map()` и `filter()` в Python 3 возвращают итераторы, а не списки (как было в Python 2)
- `range()` тоже возвращает специальный объект, а не список
- `open()` является фабрикой, возвращающей файловый объект с разным поведением в зависимости от режима

4. **Функции высшего порядка**: `map()`, `filter()`, `sorted()` принимают функции в качестве аргументов. Это делает их
   мощным инструментом для функционального программирования.

**Senior Level (CPython, байткод и системные вызовы)**

1. **Реализация в CPython**:

Встроенные функции реализованы в C в файлах CPython:

- `Python/bltinmodule.c` — основные встроенные функции
- `Objects/` — конструкторы типов (`listobject.c`, `dictobject.c` и т.д.)
- `Python/` — специализированные функции (`pythonrun.c` для `exec()`)

Каждая функция представлена структурой `PyMethodDef`:

```c
static PyMethodDef builtin_methods[] = {
{"abs",       builtin_abs,       METH_O,  abs_doc},
{"all",       builtin_all,       METH_O,  all_doc},
// ...
{NULL,        NULL}  /* Sentinel */
};
```

Флаг `METH_O` означает, что функция принимает один объект (позиционный аргумент). Есть также `METH_VARARGS`,
`METH_KEYWORDS` и их комбинации.

2. **Байткод и вызов встроенных функций**:

При вызове `len(obj)` генерируется байткод:

```
LOAD_NAME                0 (len)
LOAD_NAME                1 (obj)
CALL_FUNCTION            1
```

Но для некоторых часто используемых функций есть специализированные инструкции:

- `BUILD_LIST`, `BUILD_TUPLE`, `BUILD_SET`, `BUILD_MAP` — вместо вызова конструкторов
- `UNPACK_SEQUENCE`, `UNPACK_EX` — для распаковки
- `COMPARE_OP` — вместо вызова `cmp()` (удалена в Python 3)

3. **Оптимизации CPython 3.11+**:

**PEP 659 (специализация байткода)** добавила адаптивные инструкции для встроенных функций:

- `CALL` с кэшированием типа результата и побочных эффектов
- Для `len()`, `sum()`, `range()` создаются специализированные быстрые пути
- При частом вызове `len(list)` байткод заменяется на инструкцию, которая напрямую обращается к `PyList_GET_SIZE`

4. **`__builtins__` vs `builtins`**:

- `__builtins__` — это псевдомодуль, который есть в каждом модуле. В `__main__` это ссылка на модуль `builtins`, а в
  импортированных модулях — на его словарь `builtins.__dict__`
- `builtins` — реальный модуль, который можно импортировать
- Это различие важно для тестирования, так как переопределение в `__builtins__` влияет только на текущий модуль

5. **Опасные функции: `eval()`, `exec()`, `compile()`**:

- `eval()` принимает выражение и возвращает его значение. Работает в текущем пространстве имен
- `exec()` выполняет код (может быть многострочным). Возвращает `None`
- `compile()` преобразует строку в объект кода, который потом можно выполнить

**Безопасность**: Эти функции выполняют произвольный код. В production-коде нужно:

- Ограничивать глобальные и локальные пространства имен
- Использовать `ast.literal_eval()` для безопасного вычисления литералов
- В тестах — быть осторожным при тестировании кода, использующего эти функции

6. **`property()`, `classmethod()`, `staticmethod()` как дескрипторы**:

Эти функции не просто возвращают декорированные методы — они создают объекты-дескрипторы:

```c
// property() в CPython
property_new(PyTypeObject *type, PyObject *args, PyObject *kwds) {
// Создает property object с слотами для getter, setter, deleter, doc
}
```

При доступе к свойству через экземпляр класса срабатывает протокол дескриптора (`__get__`, `__set__`, `__delete__`).

7. **`super()` — магия на уровне C**:

`super()` не просто возвращает родительский класс. Она:

- Динамически вычисляет MRO (Method Resolution Order)
- Использует `__class__` и `self` из фрейма вызова
- В CPython: `super_new()` в `Objects/typeobject.c` анализирует стек вызовов через `PyThreadState_GET()->frame`

8. **`range()` — не просто функция, а фабрика объектов**:

В Python 3 `range()` возвращает объект типа `range`, который:

- Реализует `__len__`, `__getitem__`, `__contains__`
- Поддерживает слайсинг: `range(10)[2:5]` возвращает новый `range`
- Имеет постоянную память O(1) независимо от размера
- В CPython вычисляет элементы на лету через формулу: `start + i*step`

9. **`print()` и системные вызовы**:

Реализация `print()` в `builtin_print()`:

- Парсит аргументы: `sep`, `end`, `file`, `flush`
- По умолчанию `file=sys.stdout` (объект `PyTextIOWrapper`)
- Вызывает `PyFile_WriteObject()` для каждого аргумента
- При `flush=True` вызывает `PyObject_CallMethod(file, "flush", NULL)`
- В тестах можно перехватывать вывод через `io.StringIO` или мокать `sys.stdout`

10. **`open()` и файловые дескрипторы**:

`open()` — это фабрика, которая возвращает разные типы в зависимости от режима:

- Текстовый режим: `_io.TextIOWrapper`
- Бинарный: `_io.BufferedReader` или `_io.BufferedWriter`
- Режим 'x' (эксклюзивное создание): проверка через `os.O_EXCL`

На уровне системы вызывает `open()` из libc с флагами `O_RDONLY`, `O_WRONLY`, `O_CREAT` и т.д.

11. **`__import__()` — основа импорта**:

Эта функция:

- Вызывается оператором `import`
- Проходит через `importlib` и sys.meta_path
- Кэширует загруженные модули в `sys.modules`
- В тестах можно мокать для изоляции модулей

12. **Производительность и микрооптимизации**:

- `len()` для встроенных типов — O(1), так как обращается к полю `ob_size` в `PyObject`
- `sum()` использует быстрый путь для чисел, но медленный для других типов (из-за создания промежуточных объектов)
- `min()`/`max()` для отсортированных данных могут быть оптимизированы, но в общем случае — O(n)
- `sorted()` использует Timsort (гибрид сортировки слиянием и вставками)

13. **Для AQA: тестирование встроенных функций**:

- **Мокирование**: `unittest.mock.patch('builtins.open')` для тестирования работы с файлами
- **Перехват ввода-вывода**: `io.StringIO` для `input()`/`print()`
- **Изоляция**: временное изменение `sys.path` для тестирования импорта
- **Безопасность**: тестирование `eval()`/`exec()` на уязвимости инъекции кода
- **Производительность**: бенчмаркинг встроенных функций vs кастомных реализаций
- **Поведение при ошибках**: как функции реагируют на некорректные аргументы

14. **Диагностика через байткод**:

Можно анализировать, как используются встроенные функции в тестируемом коде:

```python
import dis

dis.dis(some_function)  # Показывает CALL_FUNCTION для встроенных функций
```

---

### Контекстные менеджеры

### **Контекстные менеджеры и оператор `with`**

**Junior Level**

Контекстные менеджеры в Python — это специальные объекты, которые позволяют управлять ресурсами и выполнять
настройку/очистку до и после выполнения блока кода. Они используются с оператором `with`, который обеспечивает
правильное приобретение и освобождение ресурсов, даже если в блоке кода произошла ошибка.

Представьте, что вы открываете файл для чтения. Вам нужно гарантировать, что файл будет закрыт после работы, независимо
от того, успешно ли вы прочитали данные или произошла ошибка. Контекстный менеджер решает именно эту задачу:

```python
with open('file.txt') as f:
    data = f.read()
# Здесь файл уже гарантированно закрыт
```

Это работает не только для файлов, но и для любых ресурсов: сетевых соединений, транзакций баз данных, блокировок
потоков и т.д.

**Middle Level**

1. **Протокол контекстного менеджера**: Любой объект может стать контекстным менеджером, если реализует два специальных
   метода:

- `__enter__(self)` — вызывается при входе в блок `with`. Возвращаемое значение присваивается переменной после `as`.
- `__exit__(self, exc_type, exc_value, traceback)` — вызывается при выходе из блока `with`. Получает информацию об
  исключении (если оно произошло). Если метод возвращает `True`, исключение считается обработанным.

2. **Классические способы создания**:

- Реализация класса с `__enter__` и `__exit__`
- Использование `contextlib.contextmanager` декоратора для генераторной функции
- Готовые менеджеры из `contextlib`: `closing()`, `suppress()`, `nullcontext()`

3. **Вложенность**: Можно использовать несколько контекстных менеджеров в одном операторе:

```python
with open('a.txt') as f1, open('b.txt') as f2:
# работа с двумя файлами
```

Это эквивалентно вложенным блокам `with`. Порядок выхода обратный порядку входа (LIFO).

4. **Особенности обработки исключений**:

- Если в `__exit__` не перехватить исключение, оно пробрасывается дальше
- Исключение в `__enter__` приводит к тому, что `__exit__` не вызывается
- Исключение в `__exit__` заменяет исходное исключение (если было)

5. **Применение в тестировании**:

- `unittest.mock.patch` является контекстным менеджером
- `pytest.raises` для проверки исключений
- Временное изменение настроек или переменных окружения

**Senior Level (Байткод, CPython и системные вызовы)**

1. **Байткодовая реализация `with`**:

Рассмотрим байткод для простого `with`:

```python
with manager as var:
    body
```

Генерируется следующий байткод (упрощенно):

```
SETUP_WITH           L1     # Настраивает блок with, L1 - адрес для завершения
LOAD_FAST            manager
CALL_FUNCTION        0      # Вызов __enter__
STORE_FAST           var
...                   body  # Код тела
POP_BLOCK
LOAD_CONST           None
L1: WITH_CLEANUP_START       # Начало очистки
LOAD_FAST            manager
CALL_FUNCTION        3      # Вызов __exit__ с None, None, None
WITH_CLEANUP_FINISH
```

Для Python 3.9+ используется байткод `WITH_EXCEPT_START` для обработки исключений.

2. **CPython: макросы и фреймы**:

В `Python/ceval.c` операция `SETUP_WITH`:

- Сохраняет текущий контекст в стеке фрейма
- Устанавливает обработчик исключений (`f->f_exc_info`)
- Вызывает `PyObject_CallNoArgs` для `__enter__`

При возникновении исключения в теле `with`:

- Интерпретатор переходит в `WITH_EXCEPT_START`
- Извлекает `exc_type`, `exc_value`, `traceback` из `f->f_exc_info`
- Вызывает `__exit__(exc_type, exc_value, traceback)`

3. **Асинхронные контекстные менеджеры**:

Для `async with` используется протокол `__aenter__`/`__aexit__`. Байткод:

```
BEFORE_ASYNC_WITH
GET_AWAITABLE
LOAD_FAST
CALL_FUNCTION
YIELD_FROM          # Ожидание __aenter__
SETUP_ASYNC_WITH
...
```

Важно: асинхронные менеджеры приостанавливают выполнение корутины в точках `__aenter__` и `__aexit__`.

4. **`contextlib` и генераторы**:

Декоратор `@contextmanager` превращает генератор в контекстный менеджер:

```python
@contextmanager
def manager():
    setup()


try:
    yield resource
finally:
    cleanup()
```

Под капотом:

- Генератор оборачивается в класс `_GeneratorContextManager`
- `__enter__` вызывает `next(gen)` до `yield`
- `__exit__` вызывает `next(gen)` для завершения или `gen.throw()` при исключении
- Критически важно: если между `yield` и `finally` возникает исключение, оно преобразуется в `RuntimeError`

5. **Системные ресурсы и файловые дескрипторы**:

Для файлового менеджера `open()`:

- `__enter__` возвращает файловый объект (уже открытый)
- `__exit__` вызывает `file.close()`, который:
- Сбрасывает буферы (`fflush()` для текстовых режимов)
- Вызывает `close()` системного файлового дескриптора через `os.close(fd)`
- Устанавливает `file.closed = True`

Особенность: файловый объект имеет `__del__`, который тоже вызывает `close()`, но полагаться на это нельзя — сборщик
мусора может сработать поздно.

6. **Транзакции и атомарность**:

В базах данных контекстные менеджеры обеспечивают атомарность:

```python
with connection.transaction():
    cursor.execute("UPDATE ...")
```

Реализация:

- `__enter__`: `BEGIN TRANSACTION`
- `__exit__`: если исключение — `ROLLBACK`, иначе `COMMIT`

Нюанс: если в `__exit__` происходит исключение во время `ROLLBACK`, исходное исключение теряется. Правильная
реализация должна это учитывать.

7. **Блокировки и многопоточность**:

`threading.Lock` — классический контекстный менеджер:

- `__enter__`: `self.acquire()` (блокирующий вызов)
- `__exit__`: `self.release()`

Важно для тестирования:

- `__enter__` может принимать `timeout` параметр
- RLock (рекурсивная блокировка) считает количество входов
- При использовании в `with` гарантируется release даже при исключении

8. **Производительность и оптимизации CPython 3.11+**:

Python 3.11 добавляет специализацию байткода для контекстных менеджеров:

- Кэширование `__enter__` и `__exit__` методов
- Инлайн-кэширование для частых менеджеров
- Ускорение на 10-20% для вложенных `with`

Однако, для менеджеров на основе `@contextmanager` (генераторы) оптимизаций меньше — дополнительный накладные расходы
на фрейм генератора.

9. **Диагностика и отладка**:

Для AQA важно уметь отлаживать контекстные менеджеры:

- `sys.settrace` может отслеживать вход/выход из `with`
- Можно модифицировать байткод через `bytecode` или `code` модули
- `inspect.currentframe()` внутри `__enter__`/`__exit__` показывает стек вызовов

10. **Кастомные обработчики исключений**:

Продвинутый паттерн — менеджер, который преобразует исключения:

```python
class ConvertException:

    def __exit__(self, exc_type, exc_val, tb):
        if exc_type == ValueError:
            raise TypeError from exc_val
```

Важно: цепочка исключений (`__cause__`, `__context__`) сохраняется.

11. **Менеджеры для тестирования**:

В тестовых фреймворках контекстные менеджеры используются для:

- `pytest.raises(Exception)` — проверяет исключение
- `unittest.mock.patch` — мокает объекты
- Создания временных файлов/директорий (`tempfile.TemporaryDirectory`)
- Изменения окружения (`os.environ`)

Особенность: эти менеджеры часто используют `yield` в `@contextmanager`, что добавляет один фрейм в стек вызовов.

12. **Рекурсивные и композитные менеджеры**:

Можно создавать менеджеры, которые комбинируют другие:

```python
class CompositeManager:

    def __init__(self, *managers):
        self.managers = managers


def __enter__(self):
    return tuple(m.__enter__() for m in self.managers)


def __exit__(self, *args):
    for m in reversed(self.managers):
        m.__exit__(*args)
```

Проблема: если `__exit__` одного менеджера вернет `True`, исключение будет подавлено для остальных. Нужно аккуратно
обрабатывать это.

13. **Ресурсы и сборка мусора**:

Контекстные менеджеры не защищают от утечек ресурсов при циклических ссысках:

```python
class Resource:

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.cleanup()


res = Resource()
with res:
    res.self_ref = res  # Циклическая ссылка!
```

`__exit__` вызовется, но объект не удалится сборщиком мусора. Решение: weakref или явный вызов `del`.

14. **Сигналы и прерывания**:

В `__exit__` может прийти сигнал `KeyboardInterrupt`. Рекомендация:

- Выполнять минимально необходимую очистку
- Не блокировать надолго
- Возвращать `False`, чтобы прерывание всплыло

---

# Генераторы и итераторы

### **Генераторы и итераторы в Python**

**Junior Level**

Итератор — это объект, который позволяет последовательно перебирать элементы коллекции. У каждого итератора есть метод
`__next__()`, который возвращает следующий элемент или вызывает исключение `StopIteration`, когда элементы закончились.
Итерируемый объект — это любой объект, который можно перебирать в цикле `for`. У него есть метод `__iter__()`, который
возвращает итератор.

Генератор — это особый вид итератора, который создается с помощью функции, содержащей ключевое слово `yield`. Когда
функция встречает `yield`, она приостанавливает выполнение и возвращает значение. При следующем вызове `next()`
выполнение продолжается с того же места. Генераторы позволяют создавать последовательности "лениво" (lazy evaluation),
не загружая все элементы в память сразу.

**Middle Level**

1. **Протокол итератора**:

- `__iter__()` — должен возвращать объект-итератор
- `__next__()` — возвращает следующий элемент или вызывает `StopIteration`
- Встроенная функция `iter()` вызывает `__iter__()`, а `next()` — `__next__()`

2. **Протокол итерируемого объекта**:

- Любой объект с методом `__iter__()` является итерируемым
- Объекты с методом `__getitem__()` также считаются итерируемыми (для обратной совместимости)
- Цикл `for` сначала вызывает `iter()`, затем в цикле вызывает `next()`

3. **Генераторные функции**:

- Функция с `yield` при вызове возвращает объект-генератор
- Генератор сохраняет свое состояние (локальные переменные, точку выполнения)
- Можно использовать `return` в генераторе — при достижении `return` выбрасывается `StopIteration` с переданным
  значением как `value`

4. **Генераторные выражения**:

- `(x**2 for x in range(10))` — создает генератор
- Аналогично list comprehension, но не создает список сразу
- Может быть бесконечным: `(i for i in itertools.count())`

5. **Методы генераторов**:

- `.send(value)` — отправляет значение в генератор (становится результатом `yield`)
- `.throw(exc)` — бросает исключение в точке `yield`
- `.close()` — останавливает генератор (вызывает `GeneratorExit` внутри)

6. **`yield from`** (Python 3.3+):

- Позволяет делегировать выполнение другому генератору
- Упрощает композицию генераторов
- Также передает `send()`, `throw()` и `close()` вложенному генератору

**Senior Level (CPython, байткод и системные структуры)**

1. **Структура PyGenObject**:

В `Include/genobject.h`:

```c
typedef struct {
PyObject_HEAD
PyFrameObject *gi_frame;      // Фрейм выполнения
PyObject *gi_code;            // Код объекта
PyObject *gi_weakreflist;     // Список weakref
int gi_running;               // Флаг выполнения (0/1)
PyObject *gi_name;            // Имя генератора
PyObject *gi_qualname;        // Полное имя
_PyErr_StackItem *gi_exc_state; // Информация об исключениях
} PyGenObject;
```

2. **Байткод генераторной функции**:

Функция с `yield` компилируется с флагом `CO_GENERATOR` в `code.co_flags`. Байткод:

```
def gen():
yield 1
yield 2

# Байткод:
0 LOAD_CONST               1 (1)
2 YIELD_VALUE
4 POP_TOP
6 LOAD_CONST               2 (2)
8 YIELD_VALUE
10 POP_TOP
12 LOAD_CONST               0 (None)
14 RETURN_VALUE
```

Инструкция `YIELD_VALUE`:

- Сохраняет текущее состояние фрейма
- Возвращает значение вызывающей стороне
- Устанавливает `f->f_lasti` (последнюю выполненную инструкцию)
- При следующем `next()` выполнение продолжается со следующей инструкции

3. **Фреймы и состояние генератора**:

Генератор хранит свой фрейм (`gi_frame`), который содержит:

- `f_code` — код объекта
- `f_lasti` — индекс последней выполненной инструкции
- `f_locals` — локальные переменные
- `f_valuestack` — стек значений
- `f_blockstack` — стек блоков (for/with/try)

Размер фрейма зависит от количества локальных переменных и глубины стека. Фреймы обычно размещаются в куче, но Python
3.11+ оптимизирует это.

4. **PyFrameObject и оптимизации CPython 3.11**:

Python 3.11 представляет **фреймы в куче (heap-allocated frames)**:

- Фреймы создаются только при необходимости (для генераторов, трассировки, исключений)
- Уменьшает использование памяти и улучшает производительность
- Генераторы всегда создают фреймы, так как они нужны для сохранения состояния

5. **`yield from` на уровне байткода**:

```python
def delegator():
    yield from subgenerator()
```

Байткод:

```
0 LOAD_GLOBAL              0 (subgenerator)
2 CALL_FUNCTION            0
4 GET_YIELD_FROM_ITER
6 LOAD_CONST               0 (None)
8 YIELD_FROM
```

`GET_YIELD_FROM_ITER` проверяет, является ли объект генератором. `YIELD_FROM`:

- Делегирует выполнение субгенератору
- Передает значения из `send()` и исключения из `throw()`
- Обрабатывает `StopIteration` и извлекает `value`

6. **Асинхронные генераторы (Python 3.6+)**:

Имеют отдельную структуру `PyAsyncGenObject`:

```c
typedef struct {
PyGenObject ag_gen;
PyObject *ag_finalizer;    // Финалайзер
PyObject *ag_hooks_inited; // Флаги хуков
} PyAsyncGenObject;
```

Асинхронные методы: `__anext__()`, `__aiter__()`. Используют `yield` в `async def`.

7. **Сборка мусора и циклические ссылки**:

Генераторы могут создавать циклические ссычки:

```python
def gen():
    while True:
        yield


g = gen()
g.gi_frame.f_locals['self_ref'] = g  # Циклическая ссылка
```

GC отслеживает генераторы через `Py_TPFLAGS_HAVE_GC`. При сборке мусора:

- Вызывается `__del__` у объектов в `gi_frame`
- Если генератор не закрыт, вызывается `gen_close()`

8. **Системные вызовы и производительность**:

Создание генератора:

- `PyGen_New` — выделяет память для `PyGenObject`
- `PyFrame_New` — создает фрейм (если его нет)
- Копирует ссылки на код и глобальные переменные

Вызов `next()`:

- `gen_iternext()` в `Objects/genobject.c`
- Восстанавливает состояние фрейма
- Выполняет байткод до следующего `YIELD_VALUE`

9. **Интроспекция и отладка**:

- `inspect.getgeneratorstate(g)` — возвращает `GEN_CREATED`, `GEN_RUNNING`, `GEN_SUSPENDED`, `GEN_CLOSED`
- `inspect.getgeneratorlocals(g)` — локальные переменные генератора
- `g.gi_frame` — прямой доступ к фрейму
- `g.gi_code` — код объекта
- `g.gi_running` — выполняется ли генератор (0/1)

10. **Корoutines и генераторы**:

До Python 3.5 корутины реализовывались через генераторы. Теперь:

- `async def` создает корутину (тип `PyCoroObject`)
- `await` использует механизм, аналогичный `yield from`
- Генераторы и корутины имеют разные типы, но схожую структуру

11. **Производительность и микрооптимизации**:

- Генераторы эффективны по памяти: O(1) вместо O(n) для списков
- Но вызов `next()` имеет накладные расходы: проверка состояния, восстановление фрейма
- Для больших последовательностей выгоднее итерировать по списку
- Python 3.11 ускоряет генераторы на 10-15% через оптимизацию фреймов

12. **Бесконечные генераторы и ресурсы**:

Бесконечные генераторы могут висеть в памяти:

```python
def infinite():
    while True:
        yield 1


g = infinite()
# Генератор живет, пока есть ссылка
```

Решение: использовать `contextlib.closing()` или явно вызывать `close()`.

13. **Тестирование генераторов для AQA**:

Особенности тестирования:

- Проверка состояния генератора после каждого `yield`
- Тестирование `send()`, `throw()`, `close()`
- Проверка обработки `StopIteration`
- Тестирование вложенных генераторов (`yield from`)
- Проверка утечек памяти при прерывании генераторов
- Тестирование асинхронных генераторов

Инструменты:

- `pytest` для тестирования
- `tracemalloc` для отслеживания утечек
- `gc` для проверки циклических ссылок
- `unittest.mock` для мокирования

14. **Байткод для `yield` и исключения**:

При `g.throw(ValueError)`:

```
RAISE_VARARGS           # В фрейме генератора
# Если исключение не поймано:
CLEANUP_THROW           # Очистка
# Возвращает исключение вызывающей стороне
```

Генератор может ловить исключения обычным `try/except`.

15. **Генераторы в контексте тестирования**:

- Создание тестовых данных "на лету" для экономии памяти
- Имитация потоков данных (сетевых, файловых)
- Тестирование обработки больших последовательностей
- Создание моков с состоянием (stateful mocks)

---

# Декораторы и замыкания

### **Декораторы и замыкания**

**Junior Level**

Декораторы в Python — это специальные функции или классы, которые позволяют модифицировать поведение других функций или
методов без изменения их исходного кода. Синтаксически декоратор выглядит как `@deкоратор` перед определением функции.

Представьте, что у вас есть функция, и вы хотите добавить к ней логирование времени выполнения. Вместо того чтобы
изменять код функции, вы можете создать декоратор, который обернет оригинальную функцию и добавит нужную
функциональность.

Замыкания — это функции, которые запоминают окружение, в котором они были созданы. Они сохраняют доступ к переменным из
внешней области видимости, даже когда эта внешняя функция уже завершила работу. Замыкания часто используются для
создания фабрик функций и являются основой для декораторов.

**Middle Level**

1. **Простой декоратор**: Функция, принимающая другую функцию и возвращающая новую функцию-обертку. Когда Python видит
   `@decorator`, он делает примерно следующее: `func = decorator(func)`.

2. **Декораторы с аргументами**: Требуют дополнительного уровня вложенности. Функция с аргументами возвращает декоратор,
   который уже принимает и оборачивает целевую функцию.

3. **Цепочка декораторов**: Декораторы применяются снизу вверх (ближайший к функции применяется первым).
   `@a @b @c def f(): ...` превращается в `f = a(b(c(f)))`.

4. **`functools.wraps`**: Критически важный декоратор, который сохраняет метаданные оригинальной функции (имя,
   документацию, сигнатуру). Без него отладочная информация и интроспекция ломаются.

5. **Классы-декораторы**: Класс с методом `__call__` может быть декоратором. Также можно декорировать методы класса
   через `__get__`.

6. **Замыкания и LEGB правило**: Python ищет переменные в следующем порядке: Local → Enclosing (замыкания) → Global →
   Built-in. Замыкания захватывают переменные из Enclosing scope.

7. **`nonlocal`**: Ключевое слово для изменения переменных из замыкания. Без него попытка изменить захваченную
   переменную создаст новую локальную переменную.

8. **Применение в тестировании**:

- Декораторы для параметризации тестов (`@pytest.mark.parametrize`)
- Декораторы для мокирования (`@unittest.mock.patch`)
- Декораторы для setup/teardown (`@pytest.fixture`)

**Senior Level (Байткод, фреймы и байты до мозга костей)**

1. **Байткод декоратора**:

Рассмотрим:

```python
@deco
def func():
    pass
```

Байткод:

```
LOAD_NAME                0 (deco)
LOAD_CONST               0 (<code object func>)
MAKE_FUNCTION            0
CALL_FUNCTION            1        # Вызов deco с func
STORE_NAME               1 (func)  # Сохранение результата
```

Ключевой момент: **декоратор выполняется во время загрузки модуля**, а не при вызове функции.

2. **Замыкания на уровне байткода**:

```python
def outer(x):
    def inner():
        return x

    return inner
```

Байткод для `inner`:

```
# Для замыкания переменной x
LOAD_DEREF               0 (x)    # Загрузка из cell
RETURN_VALUE
```

`LOAD_DEREF` и `STORE_DEREF` работают с **cell objects**, а не напрямую со стеком.

3. **Cell objects и `__closure__`**:

В CPython замыкания реализованы через объекты ячеек:

```c
typedef struct {
PyObject_HEAD
PyObject *ob_ref;    // Ссылка на значение в ячейке
} PyCellObject;
```

Когда `outer` создает `inner`:

- Создается `PyCellObject` для `x`
- Он помещается в `inner.__closure__` как кортеж ячеек
- При вызове `inner` байткод `LOAD_DEREF` обращается к ячейке

4. **Фреймы и пространства имен**:

Каждый вызов функции создает новый фрейм. Замыкания хранят ссылки на ячейки из фрейма внешней функции. Даже после
завершения `outer`, фрейм может остаться в памяти, если на него есть ссылки через замыкания.

В Python 3.11+ фреймы выделяются в куче только при необходимости (для замыканий, генераторов, исключений).

5. **Дескрипторы и методы**:

Декораторы методов класса — это особый случай. Когда декоратор применяется к методу:

```python
class A:

    @deco
    def method(self):
        pass
```

`deco` получает **обычную функцию**, а не связанный метод. При доступе `a.method` срабатывает протокол дескриптора,
создавая bound method.

Если декоратор не сохраняет `__get__` (через `functools.wraps`), методы ломаются.

6. **`functools.wraps` под микроскопом**:

Реализация в CPython (`_functoolsmodule.c`):

```c
static PyObject *
_functools_wraps(PyObject *wrapper, PyObject *wrapped)
{
// Копирует __module__, __name__, __qualname__, __doc__
// Копирует __annotations__, если есть
// Устанавливает __wrapped__ = wrapped
// Для Python 3.10+: копирует __type_params__
}
```

Важно: `wraps` не копирует `__code__`, поэтому сигнатура функции не сохраняется. Для этого нужен `inspect.signature`
или `functools.update_wrapper` с `assigned`, включающим `__signature__`.

7. **Проблема аргументов в замыканиях**:

Классическая ошибка:

```python
funcs = []
for i in range(3):
    def inner():
        return i
funcs.append(inner)
# Все функции вернут 2!
```

На байткод-уровне: все `inner` ссылаются на **одну и ту же ячейку** `i`, которая меняется в цикле.

Решение: `lambda i=i: i` создает новую ячейку для каждой итерации или использует `functools.partial`.

8. **Производительность декораторов**:

- Каждый декоратор добавляет один уровень вызова
- Множественные декораторы могут создать глубокую цепочку вызовов
- Python 3.11+ оптимизирует вызовы через **специализацию байткода**, но только для часто вызываемых функций

Замеры: декорированная функция вызывается на 10-20% медленнее из-за дополнительного фрейма.

9. **Декораторы в метаклассах и `__init_subclass__`**:

Декораторы класса выполняются **после** создания класса. Но можно влиять на декораторы через метаклассы:

```python
class Meta(type):

    def __new__(cls, name, bases, dct):
        # Здесь можно модифицировать декораторы в dct
        return super().__new__(cls, name, bases, dct)
```

`__init_subclass__` (Python 3.6+) вызывается после применения декораторов класса.

10. **`@property`, `@classmethod`, `@staticmethod` — системные декораторы**:

Реализованы на C в `Objects/descrobject.c`:

- `property` создает объект-дескриптор с `__get__`, `__set__`, `__delete__`
- `classmethod` и `staticmethod` меняют поведение `__get__`:
- `classmethod.__get__` возвращает bound method с классом как первым аргументом
- `staticmethod.__get__` возвращает оригинальную функцию без привязки

11. **Замыкания и сборка мусора**:

Замыкания создают циклические ссылки:

```python
def outer():
    x = []


def inner():
    x.append(1)  # inner ссылается на x

    return inner


f = outer()  # f.__closure__[0] ссылается на ячейку, ячейка ссылается на x
```

GC должен разорвать этот цикл. В CPython ячейки имеют `tp_traverse` и `tp_clear` методы.

12. **`inspect` и интроспекция замыканий**:

```python
import inspect


def outer(x):
    def inner():
        return x

    return inner


f = outer(42)
print(inspect.getclosurevars(f))
# ClosureVars(nonlocals={'x': 42}, globals={}, builtins={}, unbound=set())
```

Можно даже изменить значение в замыкании:

```python
f.__closure__[0].cell_contents = 100
```

13. **Декораторы для AQA: тестирование**:

- **Тестирование декораторов**: проверка, что декоратор сохраняет сигнатуру, документацию, правильно обрабатывает
  исключения
- **Мокирование декораторов**: `unittest.mock.patch.object` для временной замены декоратора
- **Производительность**: замер времени выполнения декорированных функций
- **Память**: проверка утечек через замыкания

14. **Особые виды декораторов**:

- **Ленивые декораторы**: откладывают вычисление до первого вызова
- **Декораторы с кэшированием**: `@lru_cache`, `@cached_property`
- **Декораторы валидации**: проверка аргументов/результатов
- **Асинхронные декораторы**: для `async def` функций

15. **Байткод для `nonlocal`**:

```python
def outer():
    x = 0


def inner():
    nonlocal x

    x += 1
    return inner
```

Байткод для `inner`:

```
LOAD_DEREF               0 (x)
LOAD_CONST               1 (1)
INPLACE_ADD
STORE_DEREF              0 (x)
```

Без `nonlocal` была бы ошибка: `x` рассматривался бы как локальная переменная.

---

# GIL (Global Interpreter Lock)

### **GIL (Global Interpreter Lock)**

**Junior Level**

GIL (Global Interpreter Lock) — это механизм в реализации Python CPython, который позволяет выполнять только одному
потоку Python за раз, даже на многоядерных процессорах. Представьте, что у вас есть кухня с несколькими поварами (
потоками), но только один нож (интерпретатор Python). Хотя поваров много, они вынуждены ждать, пока нож освободится.

GIL необходим потому, что управление памятью в CPython не является потокобезопасным. В частности, счетчик ссылок (
reference counting), который используется для сборки мусора, требует защиты от одновременного доступа из нескольких
потоков. Вместо того чтобы делать каждую структуру данных потокобезопасной (что сильно замедлило бы однопоточное
выполнение), CPython использует одну глобальную блокировку на весь интерпретатор.

Это означает, что для CPU-интенсивных задач многопоточность в Python не даст прироста производительности на многоядерных
системах. Однако для I/O-интенсивных задач (сеть, файлы) многопоточность все равно полезна, потому что потоки
освобождают GIL во время ожидания ввода-вывода.

**Middle Level**

1. **Техническая суть**: GIL — это мьютекс (в CPython реализованный через `PyThread_type_lock`), который защищает доступ
   к объектам Python и их счетчикам ссылок. Только поток, захвативший GIL, может выполнять байткод Python или вызывать
   C-API функции, которые работают с объектами Python.

2. **Переключение потоков**: Потоки не удерживают GIL навсегда. Существует механизм переключения, основанный на:

- **Интервалах переключения (check interval)**: По умолчанию каждые 100 тиков байткода (можно изменить через
  `sys.setswitchinterval()`). Тик — это обычно одна инструкция байткода, но не всегда.
- **I/O операциях**: При выполнении системных вызовов (чтение файла, сокета) поток освобождает GIL, позволяя другим
  потокам работать.
- **Явном освобождении**: В C-расширениях можно временно освободить GIL через `Py_BEGIN_ALLOW_THREADS` и
  `Py_END_ALLOW_THREADS`.

3. **Проблемы**:

- **Холодный старт (cold start)**: При создании нового потока требуется время на его инициализацию и захват GIL.
- **Голодание (starvation)**: Если один поток долго выполняет CPU-интенсивный код без I/O, другие потоки могут не
  получать GIL.
- **Нет true параллелизма**: Нельзя использовать несколько ядер CPU для выполнения кода Python одновременно.

4. **Обходные пути**:

- **Мультипроцессинг (`multiprocessing`)**: Запуск отдельных процессов, каждый со своим интерпретатором и GIL.
- **Асинхронное программирование (`asyncio`)**: Использование корутин в одном потоке.
- **C-расширения**: Вынос CPU-интенсивных задач в расширения, которые могут освобождать GIL.
- **Использование других реализаций Python**: Jython, IronPython (без GIL) или PyPy (с GIL, но с другими
  оптимизациями).

5. **Когда GIL не проблема**:

- I/O-интенсивные приложения (веб-серверы, клиенты)
- Смешанные нагрузки, где есть ожидание
- Использование внешних библиотек, которые освобождают GIL (например, NumPy, SciPy для тяжелых вычислений)

**Senior Level (CPython, байткод и системные вызовы)**

1. **Реализация GIL в CPython**:

В `Python/ceval_gil.h` и `Python/ceval.c`:

```c
static PyThread_type_lock global_interpreter_lock = NULL;
```

GIL реализован через примитивы операционной системы:

- На Unix/Linux: `pthread_mutex_t` с `pthread_cond_t`
- На Windows: `CRITICAL_SECTION` с событием (`CreateEvent`)

В Python 3.9+ используется "новый GIL" (с 2009 года, но все еще актуальный), который учитывает системные часы для
более справедливого планирования.

2. **Механизм переключения (switch mechanism)**:

Каждый поток имеет структуру `_gil_runtime_state`:

```c
struct _gil_runtime_state {
unsigned long interval;  // Интервал переключения в микросекундах
PyThread_type_lock mutex;
PyThread_type_lock cond;
volatile unsigned long locked;
PyThreadState *current_thread;
// ...
};
```

Поток удерживает GIL до:

- Истечения интервала (`gil_interval`)
- Вызова `PyEval_ReleaseThread` (при I/O)
- Принудительного переключения сигналом

3. **Байткод и тики (ticks)**:

В виртуальной машине Python есть счетчик тиков:

```c
/* Python/ceval.c */
#define _PY_CHECK_INTERVAL 100

int _Py_CheckInterval = _PY_CHECK_INTERVAL;

// При выполнении байткода
if (--_Py_Ticker < 0) {
_Py_Ticker = _Py_CheckInterval;
if (interpreter_lock) {
// Проверяем, не нужно ли переключиться
PyThread_release_lock(interpreter_lock);
PyThread_acquire_lock(interpreter_lock, 1);
}
}
```

Тик уменьшается при выполнении многих инструкций байткода, но не всех. Некоторые "дорогие" инструкции (например,
`CALL_FUNCTION`) могут сбрасывать счетчик.

4. **`PyThreadState` и состояние потока**:

Каждый поток имеет свой `PyThreadState`, который хранит:

- Указатель на текущий фрейм выполнения
- Исключения, возникшие в потоке
- Словарь с контекстами (`contextvars`)
- Ссылку на следующее состояние потока в связном списке

GIL защищает доступ к глобальному связному списку всех `PyThreadState`.

5. **Сигналы и GIL**:

Обработка сигналов (например, Ctrl+C) требует особого внимания. Только главный поток (который создал GIL) может
обрабатывать сигналы. При получении сигнала:

- Устанавливается флаг `PyOS_InterruptOccurred()`
- Главный поток проверяет этот флаг при получении GIL
- Если флаг установлен, вызывается обработчик сигнала

6. **Подкапотная работа с I/O**:

Когда поток вызывает блокирующий системный вызов (например, `socket.recv()`):

```c
/* Модуль socket */
Py_BEGIN_ALLOW_THREADS
ret = recv(sock->fd, buf, len, flags);
Py_END_ALLOW_THREADS
```

Макросы `Py_BEGIN_ALLOW_THREADS` и `Py_END_ALLOW_THREADS`:

- Сохраняют состояние GIL
- Освобождают GIL
- Выполняют системный вызов
- Захватывают GIL обратно
- Восстанавливают состояние

7. **Проблема справедливости (fairness issue)**:

Старый GIL (до Python 3.2) страдал от "несчастливого" планирования: поток, освободивший GIL для I/O, мог сразу
захватить его снова, не давая другим потокам шанса. Новый GIL использует условные переменные и таймеры для более
справедливого распределения.

8. **Влияние на сборку мусора**:

Циклический сборщик мусора (GC) должен работать, когда другие потоки приостановлены. Перед началом сборки GC
захватывает GIL и приостанавливает все другие потоки через `PyThreadState_Swap(NULL)`.

Это может вызвать задержки (latency), так как все потоки останавливаются на время работы GC.

9. **Многопоточность vs Мультипроцессинг на уровне памяти**:

С процессами:

- Каждый процесс имеет свою копию данных
- Обмен данными через IPC (память, файлы, сокеты)
- Нагрузка на память и накладные расходы на сериализацию

С потоками:

- Общая память, но защищенная GIL
- Нет накладных расходов на копирование
- Но нет true параллелизма для Python-кода

10. **Асинхронность и GIL**:

Асинхронные фреймворки (`asyncio`) обходят GIL, используя один поток и кооперативную многозадачность. Однако:

- Блокирующий вызов в корутине блокирует весь поток
- Для асинхронных системных вызовов (через `select`, `epoll`, `kqueue`) GIL освобождается на время ожидания

11. **Тестирование многопоточного кода с GIL**:

Для AQA важно понимать, как тестировать в условиях GIL:

- **Race conditions**: Несмотря на GIL, race conditions возможны, потому что поток может быть прерван между
  инструкциями байткода
- **Deadlocks**: GIL не защищает от deadlock. Если поток захватывает пользовательскую блокировку, а затем ждет GIL,
  может возникнуть deadlock
- **Тестирование производительности**: Измерять, как GIL влияет на масштабируемость

12. **Будущее GIL**:

Python 3.13+ планирует экспериментальное отключение GIL (PEP 703). Это потребует:

- Изменения счетчика ссылок на thread-safe реализацию (возможно, с использованием hazard pointers или RCU)
- Внесения изменений во все структуры данных CPython
- Возможного замедления однопоточного выполнения

13. **Инструменты для анализа**:

- `sys._current_frames()`: Показывает текущие фреймы во всех потоках
- `threading.enumerate()`: Список всех активных потоков
- Профилировщики: `cProfile` (но он тоже зависит от GIL)
- Внешние утилиты: `py-spy`, `perf` для анализа на уровне системы

14. **C-расширения и GIL**:

При написании C-расширений для AQA (например, для ускорения тестов):

- Можно освобождать GIL на время длительных вычислений
- Нужно быть осторожным с объектами Python: нельзя удалять или изменять объекты без GIL
- Можно использовать `PyGILState_Ensure()` и `PyGILState_Release()` для рекурсивного захвата

---

### Изменение списка во время итерации

### **Изменение списка во время итерации**

**Junior Level**

Изменение списка во время итерации по нему — это опасная операция, которая может привести к непредсказуемому поведению,
ошибкам или даже падению программы. Когда вы перебираете элементы списка в цикле `for` и одновременно добавляете или
удаляете элементы из этого же списка, вы нарушаете внутреннюю логику работы итератора.

Представьте, что вы читаете книгу и кто-то одновременно вырывает из нее страницы или вклеивает новые. Вы можете
пропустить страницы или прочитать одну страницу дважды. То же самое происходит с итератором списка: он отслеживает
текущую позицию, но если список изменяется, эта позиция может стать невалидной.

На практике это приводит к таким проблемам:

- Пропуск элементов
- Обработка одного элемента дважды
- Бесконечные циклы
- `IndexError` или `RuntimeError`

**Middle Level**

1. **Механизм итерации**: Когда вы создаете цикл `for item in my_list:`, Python вызывает `iter(my_list)`, который
   возвращает объект-итератор. Этот итератор хранит ссылку на список и текущий индекс. При каждой итерации он вызывает
   `__next__()`, который возвращает элемент по текущему индексу и увеличивает индекс.

2. **Что происходит при изменении**:

- **Удаление текущего или предыдущих элементов**: Итератор уже прошел эти индексы, но при удалении все последующие
  элементы сдвигаются. Это может привести к пропуску элемента, который сдвинулся на место удаленного.
- **Удаление последующих элементов**: Менее проблематично, но может вызвать `IndexError`, если итератор попытается
  обратиться к индексу за пределами нового размера списка.
- **Добавление элементов в конец**: Может привести к бесконечному циклу, если условие выхода зависит от размера списка.
- **Вставка элементов перед текущей позицией**: Итератор может обработать новый элемент, который был вставлен перед
  текущей позицией.

3. **Защита в Python**: Начиная с Python 3.7, при обнаружении изменения размера списка во время итерации бросается
   `RuntimeError`:

```
RuntimeError: dictionary changed size during iteration
```

Но для списков такой защиты **нет** (она есть только для словарей и множеств). Со списками Python ведет себя более
снисходительно, что делает ошибки более коварными.

4. **Безопасные альтернативы**:

- Итерация по копии списка: `for item in list(my_list):`
- Создание нового списка через list comprehension
- Использование `while` с ручным управлением индексом
- Сбор элементов для удаления в отдельный список и удаление после итерации

**Senior Level (Байткод, память и грабли компилятора)**

1. **Байткод итерации по списку**:

Рассмотрим простой цикл:

```python
for item in my_list:
    process(item)
```

Байткод:

```
SETUP_LOOP               L3
LOAD_FAST                0 (my_list)
GET_ITER
L1: FOR_ITER              L2
STORE_FAST               1 (item)
LOAD_FAST                2 (process)
LOAD_FAST                1 (item)
CALL_FUNCTION            1
POP_TOP
JUMP_ABSOLUTE           L1
L2: POP_BLOCK
L3: ...
```

Ключевая инструкция — `FOR_ITER`. Она вызывает `listiter_next()` у итератора списка.

2. **Структура list iterator в CPython**:

В `Include/listobject.h`:

```c
typedef struct {
PyObject_HEAD
Py_ssize_t it_index;      // Текущий индекс
PyListObject *it_seq;     // Ссылка на исходный список
} listiterobject;
```

Итератор хранит:

- `it_index`: текущую позицию (от 0 до `ob_size-1`)
- `it_seq`: указатель на `PyListObject` исходного списка

3. **Функция `listiter_next()`** (в `Objects/listobject.c`):

```c
static PyObject *
listiter_next(listiterobject *it)
{
PyListObject *seq = it->it_seq;
PyObject *item;

if (seq == NULL)
   return NULL;

// Проверяем, не вышел ли индекс за границы
if (it->it_index < PyList_GET_SIZE(seq)) {
   item = PyList_GET_ITEM(seq, it->it_index);
   ++it->it_index;  // Увеличиваем индекс ДО возврата элемента
   Py_INCREF(item);
   return item;
}

// Итерация завершена
it->it_seq = NULL;
Py_DECREF(seq);
return NULL;
}
```

4. **Критическая проблема**: Индекс увеличивается (`++it->it_index`) **после** получения элемента, но **до** того как
   элемент будет обработан в теле цикла. Если в теле цикла удалить элемент из списка, структура списка изменится, но
   индекс уже указывает на следующую позицию.

5. **Пример опасного сценария**:

Список: `[A, B, C, D]`

- Итератор на элементе B (индекс=1)
- Удаляем B в теле цикла
- Список становится: `[A, C, D]`
- Индекс увеличивается до 2
- Следующая итерация вернет D, пропустив C!

6. **Динамический массив и сдвиги памяти**:

Список в CPython — это динамический массив `PyObject**`. При удалении элемента:

```c
// Objects/listobject.c: list_ass_slice()
memmove(&item[i+1], &item[i], (n - i) * sizeof(PyObject *));
```

`memmove` сдвигает все элементы после удаленного на одну позицию влево. Итератор продолжает работать со старым индексом,
который теперь указывает на другой элемент.

7. **Глубокий грабель: изменение через `__setitem__`**:

Даже если не менять размер, но заменить элемент:

```python
for i, item in enumerate(my_list):
    my_list[i] = transform(item)  # Может вызвать неожиданное поведение
```

Это безопаснее, но если `transform()` вызывает `__del__` у старого объекта, который имеет сайд-эффекты, могут быть
проблемы.

8. **Параллельная модификация из разных потоков**:

Если один поток итерирует список, а другой его модифицирует, возникает **data race**. Даже с GIL это может привести к
повреждению памяти, потому что операции не атомарны на уровне байткода.

9. **Коллекции с защитой от модификации**:

В отличие от списков, словари и множества с Python 3.7+ имеют детектор модификаций:

```c
// Objects/dictobject.c
if (d->ma_used != ep->me_used) {
PyErr_SetString(PyExc_RuntimeError,
               "dictionary changed size during iteration");
return NULL;
}
```

Список такой защиты не имеет, так как проверка на каждую итерацию замедлила бы самый частый случай.

10. **Оптимизации компилятора Python 3.11+**:

В Python 3.11 появилась специализация байткода для циклов. Цикл `for` может быть скомпилирован в более эффективную
форму, но это не меняет фундаментальной проблемы с итератором.

11. **Наиболее опасный паттерн для AQA**:

```python
# Тест, который иногда проходит, иногда нет
def test_flaky():
    items = []


for i in range(5):
    items.append(i)

removed = []
for item in items:
    if item % 2 == 0:
        items.remove(item)  # Удаление во время итерации!
        removed.append(item)

# Результат зависит от фазы луны
assert removed == [0, 2, 4]
```

Такой тест будет вести себя нестабильно (flaky test).

12. **Правильные паттерны для тестирования**:

- **Копирование**: `for item in list(original):`
- **Обратная итерация**: `for i in range(len(lst)-1, -1, -1):`
- **Filter-подход**: `new_list = [x for x in lst if condition(x)]`
- **While с ручным индексом**:

```python
i = 0
while i < len(lst):
    if condition(lst[i]):
        del lst[i]
    else:
        i += 1
```

13. **Отладка и диагностика**:

Для поиска таких ошибок в тестируемом коде:

- Использовать `sys.settrace()` для отслеживания модификаций списка
- Создать wrapper-класс для списка, который бросает исключение при модификации во время итерации
- Использовать статический анализ (mypy, pylint с соответствующими правилами)

14. **Особый случай: `enumerate()`**:

`enumerate()` создает итератор, который возвращает пары (индекс, элемент). Индекс отражает позицию **на момент создания
итератора**, а не текущее состояние списка.

```python
for i, item in enumerate(lst):
    del lst[i]  # Удалит не тот элемент!
```

После удаления индексы смещаются, но `enumerate` продолжает выдавать старые индексы.

---

# Области видимости (scope)

### **Области видимости (Scope) в Python**

**Junior Level**

Область видимости в Python определяет, где и как можно использовать переменные в программе. Это как разные комнаты в
доме: переменные, созданные в одной комнате, не всегда видны в другой.

Python имеет четыре основные области видимости (в порядке поиска):

1. **Локальная (Local)** — внутри текущей функции
2. **Охватывающая (Enclosing)** — во внешних функциях (при вложенности)
3. **Глобальная (Global)** — на уровне модуля (файла)
4. **Встроенная (Built-in)** — встроенные функции и типы Python

Это правило называется **LEGB** (Local, Enclosing, Global, Built-in). Когда вы обращаетесь к переменной, Python ищет ее
в этом порядке.

Ключевые слова `global` и `nonlocal` позволяют изменять переменные из внешних областей видимости. Без них присвоение
значения переменной внутри функции создает новую локальную переменную, даже если существует внешняя с тем же именем.

**Middle Level**

1. **Пространства имен (Namespaces)**: Каждая область видимости связана с пространством имен — словарем, который
   связывает имена переменных с объектами. В Python есть:
    - Локальное пространство имен (функции)
    - Глобальное пространство имен (модули)
    - Встроенное пространство имен (builtins)
    - Пространство имен классов

2. **Время жизни пространств имен**:
    - Локальное: создается при вызове функции, уничтожается при ее завершении (если нет замыканий)
    - Глобальное: создается при загрузке модуля, существует пока модуль загружен
    - Встроенное: создается при запуске интерпретатора, существует до его завершения

3. **`global` и `nonlocal`**:
    - `global` объявляет, что переменная относится к глобальному пространству имен модуля
    - `nonlocal` (появилось в Python 3) указывает, что переменная принадлежит ближайшей охватывающей области видимости (
      но не глобальной)

4. **Замыкания (Closures)**: Когда вложенная функция использует переменные из внешней функции, эти переменные "
   захватываются" и живут дольше, чем внешняя функция. Это возможно благодаря тому, что внутренняя функция хранит ссылку
   на пространство имен внешней функции.

5. **Comprehensions и области видимости**: Начиная с Python 3, генераторные выражения и comprehensions имеют свою
   собственную область видимости. Переменные, определенные внутри них, не "протекают" наружу.

6. **Классы и области видимости**: Тело класса создает временное пространство имен, которое становится `__dict__`
   класса. Методы класса имеют доступ к атрибутам класса через `self` или имя класса.

7. **Модули и `__main__`**: При запуске скрипта напрямую он получает специальное имя `__main__`. Импортированные модули
   имеют свои собственные глобальные пространства имен.

**Senior Level (CPython, фреймы, байткод)**

1. **Фреймы (Frame objects) и области видимости**:

   Каждый вызов функции в CPython создает объект фрейма (`PyFrameObject`):
   ```c
   typedef struct _frame {
       PyObject_VAR_HEAD
       struct _frame *f_back;      // Ссылка на предыдущий фрейм (вызывающая функция)
       PyCodeObject *f_code;       // Код объекта (байткод)
       PyObject *f_builtins;       // Встроенное пространство имен
       PyObject *f_globals;        // Глобальное пространство имен
       PyObject *f_locals;         // Локальное пространство имен
       PyObject **f_valuestack;    // Стек значений
       PyObject **f_stacktop;      // Верхушка стека
       PyObject *f_trace;          // Функция трассировки
       // ...
   } PyFrameObject;
   ```

   Фреймы образуют стек вызовов. `f_back` создает цепочку, по которой можно подняться к вызывающим функциям.

2. **Байткод для доступа к переменным**:

   Python компилирует доступ к переменным в разные инструкции:
    - `LOAD_FAST` — доступ к локальной переменной (по индексу в массиве)
    - `LOAD_GLOBAL` — доступ к глобальной или встроенной переменной
    - `LOAD_DEREF` — доступ к переменной из замыкания (через cell object)
    - `LOAD_NAME` — универсальный доступ (медленнее, используется в глобальной области)
    - `STORE_*` аналогичные инструкции для присваивания

3. **Cell objects и free variables**:

   Для реализации замыканий Python использует **cell objects**:
   ```c
   typedef struct {
       PyObject_HEAD
       PyObject *ob_ref;    // Ссылка на значение в ячейке
   } PyCellObject;
   ```

   Когда внешняя функция определяет переменную, которая используется во вложенной функции, эта переменная помещается в
   cell object. Вложенная функция хранит ссылки на эти ячейки в `__closure__` (кортеж cell objects).

   Байткод `LOAD_DEREF` и `STORE_DEREF` работают с этими ячейками.

4. **Оптимизации CPython**:

    - **Локальные переменные как массив**: Внутри фрейма локальные переменные хранятся в массиве `f_localsplus`, а не в
      словаре. Доступ по индексу (`LOAD_FAST`) быстрее, чем поиск по имени в словаре.
    - **Глобальные переменные**: Кэширование в байткоде через `LOAD_GLOBAL` с кэшем (Python 3.11+).
    - **Встроенные переменные**: Хранятся в отдельном модуле `builtins`, доступ через `f_builtins`.

5. **`locals()` и `globals()` под капотом**:

    - `locals()` возвращает фактический словарь локальных переменных, но в функциях это может быть **копия**, а не живой
      словарь.
    - `globals()` возвращает `f_globals` текущего фрейма (обычно `__dict__` модуля).
    - В глобальной области видимости `locals() is globals()` возвращает `True`.

6. **Динамическое создание переменных**:

   Инструкции `exec()` и `eval()` могут создавать переменные динамически:
   ```python
   exec("x = 42", globals(), locals())
   ```
   Это создает переменную в указанном пространстве имен. В CPython это вызывает `PyEval_EvalCode` с переданными
   словарями.

7. **Проблема позднего связывания (late binding) в замыканиях**:

   Классическая проблема:
   ```python
   funcs = []
   for i in range(3):
       def inner():
           return i
       funcs.append(inner)
   # Все функции вернут 2!
   ```

   Почему: все `inner` ссылаются на **одну и ту же ячейку** `i`, которая меняется в цикле. Значение берется в момент
   вызова функции, а не создания.

8. **Области видимости в comprehensions и генераторах**:

   Начиная с Python 3, comprehensions выполняются в отдельной области видимости:
   ```python
   x = 10
   result = [x for x in range(5)]  # x внутри comprehension не перезаписывает внешний x
   ```

   Это достигается компиляцией comprehension в отдельную функцию.

9. **Классы и метаклассы**:

   При выполнении тела класса создается временное пространство имен:
    - Имена внутри класса становятся ключами в `__dict__` класса
    - Декораторы методов применяются до создания класса
    - Метаклассы (`__prepare__`) могут создать кастомное пространство имен (например, `collections.OrderedDict`)

10. **Модульная система и `sys.modules`**:

    Каждый модуль имеет свой `__dict__` как глобальное пространство имен. При импорте:
    - Проверяется `sys.modules` (кэш загруженных модулей)
    - Создается новый объект модуля
    - Его `__dict__` становится `f_globals` для кода модуля

11. **Отладка и интроспекция**:

    - `inspect.currentframe()` — текущий фрейм
    - `frame.f_locals`, `frame.f_globals` — пространства имен
    - `frame.f_code.co_names` — имена, используемые в коде
    - `frame.f_code.co_varnames` — локальные переменные

12. **Производительность**:

    - Доступ к локальным переменным (`LOAD_FAST`) в 2-3 раза быстрее, чем к глобальным (`LOAD_GLOBAL`)
    - Замыкания (`LOAD_DEREF`) немного медленнее локальных переменных
    - Python 3.11+ кэширует глобальные и встроенные переменные в байткоде

13. **Особые случаи**:

    - **Параметры функции**: Локальные переменные, инициализированные значениями аргументов
    - `*args` и `**kwargs`: Также локальные переменные
    - **Исключения**: Переменная исключения в `except` блоке — локальная для этого блока
    - **Контекстные менеджеры**: Переменная в `with ... as var` — локальная для блока

14. **Тестирование и области видимости**:

    Для AQA важно понимать области видимости при:
    - **Мокировании**: `unittest.mock.patch` работает с пространствами имен
    - **Изоляции тестов**: Каждый тест должен иметь чистое окружение
    - **Тестировании замыканий**: Проверка захваченных значений
    - **Динамическом коде**: Тестирование `exec()`/`eval()`

---

# Lambda-функции

### **Lambda-функции**

**Junior Level**

Lambda-функции в Python — это анонимные (безымянные) функции, которые могут содержать только одно выражение и возвращают
результат его вычисления. Они создаются с помощью ключевого слова `lambda` и часто используются для простых операций,
когда не хочется определять полноценную функцию через `def`.

Представьте, что вам нужно быстро выполнить простое преобразование данных, например, удвоить каждое число в списке.
Вместо того чтобы писать отдельную функцию, можно использовать lambda: `map(lambda x: x * 2, numbers)`. Lambda-функции
особенно полезны в сочетании с функциями высшего порядка: `sorted()`, `filter()`, `map()` и другими.

Основные ограничения lambda:

- Может содержать только одно выражение
- Не может содержать операторы (только выражения)
- Не может иметь документацию
- Обычно помещается в одну строку

**Middle Level**

1. **Синтаксис и семантика**: `lambda arguments: expression`
    - `arguments` — ноль или более аргументов через запятую
    - `expression` — одно выражение, результат которого возвращается
    - Неявный `return`: результат выражения автоматически возвращается

2. **Замыкания и захват переменных**: Lambda может захватывать переменные из окружающей области видимости (closure). Это
   делает их мощным инструментом для создания фабрик функций на лету.

3. **Сравнение с обычными функциями**:
    - Lambda создает объект функции, но без имени (`__name__ == '<lambda>'`)
    - Не может содержать аннотации типов (до Python 3.12, где появилась поддержка синтаксиса, но с ограничениями)
    - Не может быть декорирована обычным синтаксисом декораторов
    - Всегда возвращает один результат

4. **Использование с функциями высшего порядка**:
    - `sorted(items, key=lambda x: x[1])` — сортировка по второму элементу
    - `filter(lambda x: x > 0, values)` — фильтрация положительных чисел
    - `map(lambda x, y: x + y, list1, list2)` — поэлементное сложение
    - `functools.reduce(lambda acc, x: acc * x, numbers)` — аккумуляция

5. **Пространства имен и аргументы по умолчанию**:
   Lambda может иметь аргументы со значениями по умолчанию: `lambda x, y=10: x + y`
   Важно: значения по умолчанию вычисляются в момент **определения** lambda, а не вызова.

6. **Распространенная ошибка с замыканиями**:
   ```python
   funcs = [lambda: i for i in range(3)]
   # Все функции вернут 2!
   ```
   Причина: все lambda захватывают переменную `i`, а не ее значение на момент создания. Значение берется в момент
   вызова, когда цикл уже завершился.

**Senior Level (Байткод, компиляция и внутреннее устройство)**

1. **Компиляция lambda в байткод**:

   Lambda-функция компилируется почти так же, как обычная функция. Рассмотрим:
   ```python
   f = lambda x: x * 2
   ```

   Байткод для этого выражения:
   ```
   LOAD_CONST               <code object <lambda> at 0x...>
   LOAD_CONST               '<lambda>'
   MAKE_FUNCTION            0
   STORE_FAST               f
   ```

   Ключевые моменты:
    - Объект кода lambda создается на этапе **компиляции** и помещается в константы родительского кода
    - `MAKE_FUNCTION` создает объект функции во время **выполнения**
    - Имя функции всегда `'<lambda>'`

2. **Объект кода lambda**:

   В CPython объект `PyCodeObject` для lambda почти идентичен объекту кода обычной функции:
   ```c
   typedef struct {
       PyObject_HEAD
       // ...
       PyObject *co_code;        // Байткод
       PyObject *co_consts;      // Константы
       PyObject *co_names;       // Имена глобальных переменных
       PyObject *co_varnames;    // Имена локальных переменных
       PyObject *co_freevars;    // Свободные переменные (для замыканий)
       PyObject *co_cellvars;    // Ячейки (cell variables)
       // ...
   } PyCodeObject;
   ```

   Отличие от обычной функции: `co_flags` не имеет флага `CO_NEWLOCALS`? На самом деле, lambda тоже создает локальное
   пространство имен.

3. **Байткод внутри lambda**:

   Для `lambda x: x * 2`:
   ```
   0 LOAD_FAST                0 (x)
   2 LOAD_CONST               1 (2)
   4 BINARY_MULTIPLY
   6 RETURN_VALUE
   ```

   Важно: lambda использует `LOAD_FAST` для доступа к аргументам, как и обычная функция. Это быстрее, чем `LOAD_NAME`
   или `LOAD_GLOBAL`.

4. **Замыкания в lambda**:

   Когда lambda захватывает переменные из внешней области видимости:
   ```python
   def outer(n):
       return lambda x: x * n
   ```

   Байткод для lambda:
   ```
   0 LOAD_DEREF               0 (n)   # Загружает из замыкания
   2 LOAD_FAST                0 (x)
   4 BINARY_MULTIPLY
   6 RETURN_VALUE
   ```

   Lambda хранит ссылки на cell objects в `__closure__`, как и обычная функция.

5. **Создание функции: `MAKE_FUNCTION`**:

   Инструкция `MAKE_FUNCTION` (код 0x84 в Python 3.10+):
    - Берет объект кода из стека
    - Создает объект функции `PyFunctionObject`
    - Устанавливает `__name__`, `__doc__`, `__qualname__`
    - Для lambda `__name__` устанавливается в `'<lambda>'`

   В CPython `PyFunction_New`:
   ```c
   PyObject *
   PyFunction_New(PyObject *code, PyObject *globals)
   {
       PyFunctionObject *op = PyObject_GC_New(PyFunctionObject, 
                                              &PyFunction_Type);
       op->func_code = code;
       op->func_globals = globals;
       op->func_defaults = NULL;
       op->func_kwdefaults = NULL;
       op->func_closure = NULL;
       op->func_doc = NULL;
       op->func_name = ((PyCodeObject *)code)->co_name;
       // ...
   }
   ```

6. **Проблема с аргументами по умолчанию**:

   Рассмотрим:
   ```python
   funcs = [lambda x, i=i: x + i for i in range(3)]
   ```

   Здесь `i=i` создает **локальную** переменную в лямбде, которая инициализируется значением `i` на момент создания. В
   байткоде:
   ```
   # Для каждой lambda в цикле
   LOAD_FAST                0 (i)    # Текущее значение i
   BUILD_TUPLE              1
   LOAD_CONST               <code>
   LOAD_CONST               '<lambda>'
   MAKE_FUNCTION            1        # 1 = есть defaults
   ```

   Аргументы по умолчанию хранятся в `func_defaults` как кортеж.

7. **Производительность lambda vs def**:

    - **Создание**: Lambda создается во время выполнения, как и функция через `def`. Нет разницы в производительности
      создания.
    - **Вызов**: Вызов lambda идентичен вызову обычной функции. Одинаковое количество байткод-инструкций.
    - **Чтение**: Lambda не имеет `__code__.co_firstlineno`, что может затруднить отладку.
    - **Мемоизация**: Сложнее мемоизировать lambda, так как у них нет имени для кэширования.

8. **Специфика Python 3.12+**:

   В Python 3.12 появилась ограниченная поддержка аннотаций типов в lambda:
   ```python
   f: Callable[[int], int] = lambda x: x * 2
   ```

   Но сама lambda не может содержать аннотации в своем синтаксисе. Также улучшены сообщения об ошибках в lambda.

9. **Интроспекция lambda**:

   Lambda имеет те же атрибуты, что и обычная функция, но с особенностями:
    - `lambda x: x.__name__` вернет `'<lambda>'`
    - `lambda x: x.__code__` содержит байткод
    - `lambda x: x.__defaults__` для значений по умолчанию
    - `lambda x: x.__closure__` для замыканий

   Однако `inspect.getsource()` не работает для lambda, так как у них нет исходного кода в отдельной строке.

10. **Рекурсия с lambda**:

    Lambda не может рекурсивно вызывать себя по имени, так как у нее нет имени. Но можно использовать Y-комбинатор:
    ```python
    Y = lambda f: (lambda x: f(lambda y: x(x)(y)))(lambda x: f(lambda y: x(x)(y)))
    factorial = Y(lambda f: lambda n: 1 if n == 0 else n * f(n-1))
    ```

    Практического применения мало, но показывает выразительную мощность lambda.

11. **Оптимизации компилятора**:

    В Python 3.11+ компилятор пытается оптимизировать вызовы lambda:
    - Инлайн-кэширование для часто вызываемых lambda
    - Специализация байткода для lambda с простыми операциями
    - Однако lambda все равно создает полный объект функции и фрейм при вызове

12. **Ограничения и обходные пути**:

    Lambda не может содержать:
    - Присваивания (`=`)
    - Операторы (`if`, `for`, `while`)
    - Исключения (`try/except`)

    Обходные пути:
    - Условное выражение: `lambda x: True if x > 0 else False`
    - Логические операторы: `lambda x: x > 0 and "positive" or "non-positive"`
    - Вызовы других функций: `lambda x: some_func(x)`

13. **Тестирование кода с lambda**:

    Для AQA важно уметь тестировать lambda-функции:
    - **Мокирование**: Lambda может быть заменена mock-объектом
    - **Проверка поведения**: Можно проверить, что lambda вызывается с правильными аргументами
    - **Тестирование замыканий**: Проверка захваченных значений
    - **Производительность**: Замер времени выполнения lambda в цикле

14. **Диагностика проблем**:

    При ошибках в lambda трассировка стека показывает `<lambda>` вместо имени функции. Для отладки можно:
    - Временно заменить lambda на именованную функцию
    - Использовать `inspect.currentframe()` внутри lambda для получения контекста
    - Добавить отладочный вывод через обертку

---

# **Comprehensions и генераторные выражения**

**Junior Level**

Comprehensions и генераторные выражения — это лаконичные конструкции в Python для создания коллекций на основе итераций.
Представьте, что вам нужно преобразовать один список в другой: вместо написания многострочного цикла `for` можно
использовать одну строку с comprehension.

List comprehension (списковое включение) создаёт новый список: `[x*2 for x in range(5)]` даст `[0, 2, 4, 6, 8]`. Set
comprehension создаёт множество, dict comprehension — словарь. Генераторное выражение выглядит похоже, но в круглых
скобках: `(x*2 for x in range(5))`. Разница в том, что генераторное выражение не создаёт коллекцию сразу, а возвращает
итератор, который вычисляет элементы «лениво», по одному, что экономит память при работе с большими объёмами данных.

Их удобно использовать для фильтрации (добавив `if`) и для преобразования элементов. Это делает код чище и часто
быстрее, чем аналогичные циклы.

**Middle Level**

1. **Типы comprehensions**:
    - List comprehension: `[выражение for элемент in итератор]`
    - Set comprehension: `{выражение for элемент in итератор}`
    - Dict comprehension: `{ключ: значение for элемент in итератор}`
    - Generator expression: `(выражение for элемент in итератор)`

2. **Синтаксические возможности**:
    - Могут содержать несколько циклов `for`: `[x+y for x in list1 for y in list2]`
    - Поддерживают условия фильтрации `if`: `[x for x in range(10) if x % 2 == 0]`
    - Условия могут быть вложенными и комбинированными

3. **Область видимости**: Начиная с Python 3, comprehensions и генераторные выражения выполняются в собственной области
   видимости. Переменные, созданные внутри (например, переменная цикла), не «просачиваются» наружу, что предотвращает
   случайные перезаписи.

4. **Производительность**: List comprehensions обычно выполняются быстрее эквивалентных циклов `for`, потому что они
   оптимизированы на уровне байткода и выполняют операции добавления элементов напрямую, минуя вызовы методов.
   Генераторные выражения экономят память, но имеют небольшие накладные расходы на каждый вызов `next()`.

5. **Ленивые вычисления**: Генераторные выражения вычисляют элементы только когда они запрашиваются (например, в цикле
   `for` или при вызове `next()`). Это позволяет работать с бесконечными последовательностями и потоками данных.

6. **Отличия от функций-генераторов**: Генераторные выражения — это синтаксический сахар для создания анонимных
   генераторов. Они не могут содержать сложную логику с несколькими `yield` или `return`, в отличие от
   функций-генераторов.

**Senior Level (Байткод, компиляция и внутренние механизмы)**

1. **Компиляция comprehensions**:

В CPython каждое comprehension компилируется во **временную функцию**. Например, list comprehension
`[x*2 for x in iterable]` преобразуется в скрытую функцию, которая создаёт список, выполняет цикл и возвращает
результат.

Байткод для вызова list comprehension:

```
LOAD_CONST               <code object <listcomp> at 0x...>
LOAD_CONST               '<listcomp>'
MAKE_FUNCTION            0
LOAD_GLOBAL              iterable
GET_ITER
CALL_FUNCTION            1
```

Объект кода `<listcomp>` содержит байткод, реализующий логику comprehension.

2. **Внутренняя функция comprehension**:

Байткод внутренней функции для `[x*2 for x in range(5)]`:

```
0 BUILD_LIST               0       # Создаём пустой список
2 LOAD_FAST                0 (.0)  # Загружаем итератор
4 FOR_ITER                16 (to 22)
6 STORE_FAST               1 (x)   # Сохраняем текущий элемент в x
8 LOAD_FAST                1 (x)
10 LOAD_CONST               0 (2)
12 BINARY_MULTIPLY
14 LIST_APPEND              2       # Добавляем результат в список
16 JUMP_ABSOLUTE            4
18 POP_BLOCK
20 RETURN_VALUE
```

Ключевая инструкция `LIST_APPEND` (код 0x69) добавляет элемент напрямую во внутренний массив списка, что быстрее, чем
вызов метода `append()`.

3. **Генераторные выражения**:

Генераторное выражение `(x*2 for x in range(5))` компилируется в код, создающий объект генератора. Внутренняя функция
`<genexpr>` использует `YIELD_VALUE`:

```
0 LOAD_FAST                0 (.0)
2 FOR_ITER                12 (to 16)
4 STORE_FAST               1 (x)
6 LOAD_FAST                1 (x)
8 LOAD_CONST               0 (2)

10 BINARY_MULTIPLY
12 YIELD_VALUE # Возвращаем значение и приостанавливаемся
14 JUMP_ABSOLUTE 2
16 LOAD_CONST 1 (None)
18 RETURN_VALUE

```

При каждом вызове `next()` выполнение возобновляется с точки после последнего `YIELD_VALUE`.

4. **Специализированные инструкции байткода**:

Для каждого типа comprehension есть своя инструкция добавления элемента:

- `LIST_APPEND` (0x69) для list comprehensions
- `SET_ADD` (0x67) для set comprehensions
- `MAP_ADD` (0x68) для dict comprehensions

Эти инструкции работают напрямую с внутренними структурами данных (`PyListObject`, `PySetObject`, `PyDictObject`), минуя
вызовы методов Python.

5. **Оптимизации CPython**:

- **Предварительное выделение памяти**: Если итерируемый объект имеет метод `__len__`, CPython использует его для
  предварительного выделения памяти под список, уменьшая количество перераспределений.
- **Инлайн-кэширование**: В Python 3.11+ добавлен адаптивный байткод, который кэширует частые операции внутри
  comprehensions.
- **Оптимизация стека**: Внутренние переменные comprehension хранятся в массиве `f_localsplus` фрейма, а не в словаре,
  что ускоряет доступ.

6. **Области видимости и cell variables**:

Если comprehension использует переменные из внешней области видимости, они захватываются через cell objects. Например, в
`[x*y for x in range(3)]`, если `y` — внешняя переменная, она загружается инструкцией `LOAD_DEREF` из cell object.

7. **Производительность: comprehension vs цикл**:

List comprehension быстрее цикла с `append()` по трём причинам:

1. Операция добавления выполняется нативным кодом в `LIST_APPEND`
2. Нет накладных расходов на поиск и вызов метода `append`
3. Весь цикл выполняется в одной области видимости без переключения контекста

8. **Генераторные выражения и память**:

Генераторные выражения создают объект типа `PyGenObject`, который содержит фрейм выполнения. Этот фрейм сохраняет
состояние между вызовами `next()`. Память освобождается только после завершения итерации или явного вызова `close()`.

9. **Вложенные comprehensions**:

Вложенное comprehension `[[i*j for j in range(3)] for i in range(4)]` компилируется в две внутренние функции. Внешняя
функция создаёт внешний список, внутренняя — внутренние списки. Это может создавать дополнительные накладные расходы
из-за создания множества временных объектов.

10. **Словарные comprehension**:

Dict comprehension `{x: x**2 for x in range(5)}` использует инструкцию `MAP_ADD`, которая добавляет пару ключ-значение
напрямую в хеш-таблицу словаря. Это быстрее, чем вызов `dict.__setitem__`.

11. **Особенности множественных comprehension**:

Set comprehension использует `SET_ADD`, который проверяет уникальность элемента через хеш-таблицу множества. При
дублировании элементов выполняется лишняя работа, но результат остаётся корректным.

12. **Потенциальные проблемы**:

- **Утечка памяти в генераторах**: Если генераторное выражение не итерируется до конца (например, из-за `break`), его
  фрейм может остаться в памяти до сборки мусора.
- **Неправильный порядок вложенных циклов**: В comprehension вида `[x+y for x in A for y in B]` сначала фиксируется `x`,
  затем итерируется `y`. Это противоположно вложенным циклам `for x in A: for y in B:`.
- **Оценка условий**: Условия `if` оцениваются для каждого элемента, что может быть дорого, если условие сложное.

13. **Тестирование для AQA**:

При тестировании кода с comprehensions и генераторными выражениями важно:

- Проверять корректность выходных данных для всех типов входных данных (пустые, большие, с дубликатами)
- Измерять потребление памяти при использовании генераторных выражений
- Тестировать производительность на больших наборах данных
- Проверять обработку исключений внутри выражений
- Убеждаться, что генераторные выражения не используются повторно (они одноразовые)
- Проверять корректность работы с замыканиями

---

### **copy() и deepcopy()**

**Junior Level**

`copy()` и `deepcopy()` — это функции из модуля `copy`, которые создают копии объектов Python.

`copy()` делает поверхностную (shallow) копию объекта — создает новый контейнер, но элементы внутри остаются теми же
самыми объектами. Для списка `copy()` создаст новый список, но элементы этого списка будут ссылаться на те же объекты,
что и в оригинале.

`deepcopy()` делает глубокую (deep) копию — рекурсивно копирует все объекты внутри контейнера, создавая полностью
независимую копию всей иерархии объектов. Это гарантирует, что изменения в копии не затронут оригинал, и наоборот.

Пример: если у вас есть список списков, `copy()` скопирует только внешний список, а внутренние списки останутся общими.
`deepcopy()` скопирует и внешний список, и все внутренние списки.

**Middle Level**

1. **Поверхностное копирование (`copy()`)**:
    - Создает новый объект того же типа
    - Копирует ссылки на вложенные объекты, а не сами объекты
    - Для изменяемых объектов (списков, словарей, множеств) изменение вложенных объектов в копии отразится на оригинале
    - Для неизменяемых объектов (числа, строки, кортежи) разницы между поверхностной и глубокой копией нет
    - Использует метод `__copy__()` объекта, если он определен

2. **Глубокое копирование (`deepcopy()`)**:
    - Рекурсивно обходит всю структуру объекта
    - Создает новые экземпляры для всех вложенных объектов
    - Обрабатывает циклические ссылки через словарь `memo` для избежания бесконечной рекурсии
    - Использует метод `__deepcopy__()` объекта, если он определен
    - Может быть очень медленным для больших структур

3. **Когда использовать**:
    - `copy()`: когда объекты неизменяемы или вы уверены, что не будете изменять вложенные объекты
    - `deepcopy()`: когда нужна полная изоляция, особенно для конфигураций, тестовых данных, фикстур

4. **Особые случаи**:
    - Файловые объекты, потоки, сокеты не могут быть скопированы нормально
    - Некоторые объекты (модули, классы, функции) возвращаются как есть
    - Дескрипторы и свойства требуют специальной обработки

**Senior Level (CPython, память и рекурсивные алгоритмы)**

1. **Реализация в CPython**:

   Модуль `copy` реализован на Python (`Lib/copy.py`), но использует низкоуровневые механизмы CPython.

   ```python
   # Упрощенная структура
   def copy(x):
       cls = type(x)
       copier = _copy_dispatch.get(cls)
       if copier:
           return copier(x)
       # Попытка использовать __copy__ метод
       # или создание через конструктор
   ```

   `_copy_dispatch` — это словарь, который сопоставляет типы с функциями-копировщиками.

2. **Алгоритм `deepcopy()`**:

   Основная функция `_deepcopy_atomic()`:
   ```python
   def _deepcopy_atomic(x, memo):
       # Атомарные объекты (неизменяемые) возвращаются как есть
       return x
   
   def _deepcopy_list(x, memo, recursive=0):
       id_x = id(x)
       if id_x in memo:
           return memo[id_x]
       
       memo[id_x] = y = []
       for item in x:
           y.append(deepcopy(item, memo))
       return y
   ```

   Ключевые элементы:
    - `memo` словарь: `{id(original): copy}` для обработки циклических ссылок
    - Рекурсивный обход через `deepcopy(item, memo)`
    - Специальные обработчики для разных типов в `_deepcopy_dispatch`

3. **Обработка циклических ссылок**:

   Для структуры с циклической ссылкой:
   ```python
   a = []
   b = [a]
   a.append(b)
   ```

   `deepcopy()` работает так:
    1. Видит список `a`, создает пустую копию `a'`, сохраняет `{id(a): a'}` в `memo`
    2. Начинает копировать элементы `a` — видит `b`
    3. Создает пустую копию `b'`, сохраняет `{id(b): b'}` в `memo`
    4. Начинает копировать элементы `b` — видит ссылку на `a`
    5. Находит `a'` в `memo`, вставляет ссылку на `a'` в `b'`
    6. Возвращается к `a'`, вставляет `b'` в него

4. **Байткод и производительность**:

   Каждый вызов `deepcopy()`:
    - Создает новый фрейм для рекурсивных вызовов
    - Выполняет множество проверок типа
    - Использует `id()` для каждого объекта (хэш от адреса памяти)
    - Для больших структур может вызвать `RecursionError`

   Оптимизация: `deepcopy()` использует `_deepcopy_dispatch` для быстрого вызова специализированных функций.

5. **Специальные методы `__copy__()` и `__deepcopy__()`**:

   Классы могут определить эти методы для кастомного поведения:
   ```python
   class MyClass:
       def __init__(self, data):
           self.data = data
           self._cache = None
       
       def __deepcopy__(self, memo):
           # Копируем только data, cache не копируем
           new_obj = self.__class__(deepcopy(self.data, memo))
           memo[id(self)] = new_obj
           return new_obj
   ```

   `__deepcopy__()` получает `memo` словарь и должен использовать его для рекурсивных вызовов.

6. **Проблемы с объектами CPython**:

   Некоторые объекты не могут быть скопированы:
    - Модули: возвращаются как есть
    - Классы: копируются только ссылки
    - Файловые дескрипторы: могут быть "скопированы", но это опасно
    - Сокеты, потоки: обычно вызывают исключение
    - Weak references: требуют специальной обработки

7. **Производительность `copy()`**:

   Для встроенных типов `copy()` часто реализована на C:
    - `list.copy()`: `PyList_Copy()` в `Objects/listobject.c`
    - `dict.copy()`: `PyDict_Copy()` в `Objects/dictobject.c`
    - `set.copy()`: `PySet_Copy()` в `Objects/setobject.c`

   Эти функции работают за O(n) времени и создают новые структуры данных.

8. **Memoryview и копирование**:

   Для объектов с буферами памяти (memoryview, array.array, numpy arrays):
    - `copy()` создает новый объект с тем же буфером (shallow)
    - `deepcopy()` обычно делает то же самое, что и `copy()`
    - Для настоящего копирования данных нужны специальные методы (`copy()`, `copy.deepcopy()` с кастомной логикой)

9. **Копирование дескрипторов и property**:

   При копировании класса или экземпляра с дескрипторами:
    - Дескрипторы не копируются (они принадлежат классу)
    - Property объекты копируются как есть
    - Важно: копирование не создает новые функции-геттеры/сеттеры

10. **Тестирование и отладка**:

    Для AQA важно тестировать копирование:
    - Проверять идентичность вложенных объектов после `copy()`
    - Проверять независимость после `deepcopy()`
    - Тестировать циклические ссылки
    - Проверять кастомные `__copy__`/`__deepcopy__` методы
    - Измерять производительность для больших структур

11. **Альтернативные методы копирования**:

    - `pickle.loads(pickle.dumps(obj))`: создает глубокую копию через сериализацию
    - `json.loads(json.dumps(obj))`: для JSON-сериализуемых объектов
    - Специализированные методы: `list()`, `dict()`, `set()` для простых случаев

12. **Копирование в многопоточной среде**:

    При копировании объектов, к которым обращаются несколько потоков:
    - `copy()` может захватить неконсистентное состояние
    - `deepcopy()` рекурсивно захватывает объекты, что может привести к deadlock
    - Нужно использовать блокировки или immutable структуры

13. **Сборка мусора и копирование**:

    При копировании больших объектов:
    - `copy()` увеличивает счетчики ссылок у вложенных объектов
    - `deepcopy()` создает полностью новые объекты, увеличивая нагрузку на GC
    - `memo` словарь в `deepcopy()` предотвращает дублирование, но может удерживать ссылки

---

### **Асинхронность (Async/Await)**

#### **Junior Level**

Асинхронность в Python — это модель программирования, позволяющая выполнять множество операций без блокировки потока
выполнения. Ключевые слова `async` и `await` используются для определения и работы с асинхронным кодом.

`async def` определяет асинхронную функцию (корутину), которая может приостанавливать своё выполнение, не блокируя
другие операции. `await` используется для ожидания результата другой асинхронной операции, освобождая управление, пока
эта операция выполняется. Это особенно полезно для операций ввода-вывода (I/O), таких как сетевые запросы или чтение
файлов, где программа может тратить время в ожидании ответа.

Для запуска асинхронного кода нужен цикл событий (event loop), который управляет выполнением корутин. Модуль `asyncio`
предоставляет инфраструктуру для написания асинхронного кода, включая планирование задач и синхронизацию.

#### **Middle Level**

1. **Корутины (coroutines)**: Функции, определённые с `async def`. При вызове они возвращают объект корутины, который
   необходимо запустить в цикле событий. Корутины могут быть приостановлены (`await`) и возобновлены.

2. **Event Loop**: Центральный механизм, управляющий выполнением асинхронных задач. Он запускает корутины, обрабатывает
   системные события (например, готовность сокета), и переключается между задачами при их приостановке.

3. **Задачи (Tasks)**: Обёртки вокруг корутин, которые планируются в цикле событий. `asyncio.create_task()` запускает
   корутину как конкурентную задачу.

4. **Awaitable объекты**: Любой объект, который можно использовать с `await`. Это включает корутины, задачи и объекты
   `Future` (представляющие результат асинхронной операции).

5. **Принципы работы**: При встрече `await` корутина приостанавливается, управление возвращается в цикл событий, который
   запускает другие задачи. Когда ожидаемая операция завершается, цикл событий возобновляет корутину.

6. **Синхронизация**: `asyncio` предоставляет примитивы для синхронизации задач: `Lock`, `Semaphore`, `Event`,
   `Condition`.

7. **Асинхронные контекстные менеджеры и итераторы**: `async with` и `async for` для работы с асинхронными ресурсами и
   итерируемыми объектами.

#### **Senior Level**

**1. Реализация корутин в CPython**

Корутины строятся на генераторах. Асинхронная функция компилируется с флагом `CO_COROUTINE` (0x80) в `code.co_flags`.
Объект корутины имеет тип `PyCoroObject` (наследуется от `PyGenObject`):

```c
typedef struct {
    PyGenObject gen;
    PyObject *cr_origin;  // Для отладки
} PyCoroObject;
```

При вызове `async def` функции:

- Байткод `MAKE_FUNCTION` создаёт объект функции с флагом `CO_COROUTINE`
- Вызов функции возвращает объект корутины (не выполняя код)
- Для запуска корутины вызывается `coro.send(None)` или `await`

**2. Механизм `await` на уровне байткода**

Байткод для `await` в Python 3.9+ использует инструкции `GET_AWAITABLE` и `YIELD_FROM`:

```
async def foo():
    await bar()

# Байткод foo():
0 LOAD_GLOBAL              0 (bar)
2 CALL_FUNCTION            0
4 GET_AWAITABLE
6 LOAD_CONST               0 (None)
8 YIELD_FROM
10 POP_TOP
12 LOAD_CONST               0 (None)
14 RETURN_VALUE
```

`GET_AWAITABLE` проверяет, является ли объект awaitable (имеет метод `__await__`). `YIELD_FROM` приостанавливает
корутину и передаёт управление.

**3. Цикл событий (Event Loop)**

Реализация цикла событий в `asyncio` использует селекторы (select, epoll, kqueue) для мониторинга файловых дескрипторов.
Основные компоненты:

- **Ready Queue**: Очередь готовых к выполнению задач (корутин)
- **Scheduled Queue**: Очередь отложенных задач (через `asyncio.sleep`)
- **Selector**: Мониторит сокеты и файловые дескрипторы

Работа цикла:

```python
while True:
    # 1. Выполнить готовые задачи
    while ready_queue:
        task = ready_queue.popleft()
        task._step()  # Продвинуть выполнение

    # 2. Ожидать событий ввода-вывода
    timeout = calculate_timeout()
    events = selector.select(timeout)
    for fd, event in events:
        callback = fd_to_callback[fd]
        callback()

    # 3. Проверить отложенные задачи
    current_time = time()
    while scheduled_queue and scheduled_queue[0].when <= current_time:
        task = scheduled_queue.pop(0)
        ready_queue.append(task)
```

**4. Задачи (Tasks) и Future**

`Task` наследуется от `Future`. `Future` представляет результат асинхронной операции. Структура `PyTaskObject` включает:

- `_coro`: ссылка на корутину
- `_state`: состояние (PENDING, CANCELLED, FINISHED)
- `_result`: результат или исключение
- `_callbacks`: список колбэков при завершении

Когда задача завершается, она устанавливает результат во `Future` и запускает прикреплённые колбэки.

**5. Асинхронные генераторы**

Асинхронные генераторы (`async def` с `yield`) используют отдельный тип `PyAsyncGenObject`. Они поддерживают:

- `__anext__()` для асинхронной итерации
- `asend()`, `athrow()`, `aclose()` аналогично обычным генераторам
- Финализацию через `async_gen_finalizer()`

**6. Протокол `__await__`**

Любой объект может стать awaitable, реализовав `__await__()`, который должен возвращать итератор. Цикл событий будет
итерировать по нему до завершения.

**7. Отладка и интроспекция**

- `asyncio.current_task()`: текущая задача
- `asyncio.all_tasks()`: все активные задачи
- `task.get_stack()`: стек вызовов корутины
- `asyncio.get_event_loop_policy()`: политика цикла событий

**8. Производительность и оптимизации**

- **Быстрый путь для локального event loop**: `asyncio.get_running_loop()` кэширует ссылку на текущий цикл
- **Инлайн-кэширование в Python 3.11**: байткод `ASYNC_GEN_WRAP` оптимизирует асинхронные генераторы
- **Протокол буферизации**: асинхронные итераторы могут реализовать `__aiter__`, возвращающий асинхронный итератор, и
  `__anext__` для получения элементов

**9. GIL и асинхронность**

Асинхронный код выполняется в одном потоке, поэтому GIL не препятствует параллелизму. Однако:

- Блокирующие вызовы блокируют весь цикл событий
- Для CPU-интенсивных задач используется `run_in_executor()` (пул потоков/процессов)
- Освобождение GIL в `await` происходит автоматически при вызове системных функций

**10. Системные вызовы и обратные вызовы**

При асинхронном I/O:

- Системный вызов (например, `socket.recv`) регистрируется в селекторе
- Цикл событий добавляет колбэк, который будет вызван при готовности данных
- Колбэк возобновляет корутину через `task.set_result()`

**11. Обработка исключений**

Исключения в корутинах пробрасываются через механизм `throw()`:

- Если корутина не обрабатывает исключение, оно устанавливается в задачу
- `await` пробрасывает исключение вызывающей корутине
- `asyncio.gather()` собирает исключения из нескольких задач

**12. Тестирование асинхронного кода**

Для AQA критически важно:

- **Изоляция тестов**: Каждый тест должен запускаться в новом цикле событий
- **Мокирование**: Подмена асинхронных функций через `unittest.mock.AsyncMock`
- **Таймауты**: Контроль времени выполнения тестов
- **Обработка исключений**: Проверка асинхронных исключений через `pytest.raises()`
- **Нагрузочное тестирование**: Проверка поведения при множестве одновременных корутин

**13. Расширенные паттерны**

- **Шардинг циклов событий**: Несколько циклов в разных потоках
- **Custom Event Loop**: Реализация своего цикла событий для специализированных нужд
- **Протоколы транспорта**: Низкоуровневая работа с сокетами через `asyncio.Protocol`

---

